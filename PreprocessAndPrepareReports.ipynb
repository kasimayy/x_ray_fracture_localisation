{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set local python and nltk paths\n",
    "import sys\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions')\n",
    "sys.path\n",
    "import nltk\n",
    "nltk.data.path = ['/vol/medic02/users/ag6516/nltk_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import randint\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = ('/vol/medic02/users/ag6516/x_ray_fracture_localisation/')\n",
    "# dir = ('/Users/Aydan/PhD/x_ray_fracture_localisation/')\n",
    "df = pd.read_csv(dir + 'cleaned_reports_edited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reports = []\n",
    "reports_tok = []\n",
    "labels = []\n",
    "words = []\n",
    "report_lengths = []\n",
    "num_sentences = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    report = row['Report text'].decode('utf-8').lower()\n",
    "    num_sentences.append(report.count('.'))\n",
    "    # Remove markdown\n",
    "    for char in ['\\n', '\\b']:\n",
    "        report = report.replace(char, '')\n",
    "    \n",
    "    # Replace unnecessary punctuation \n",
    "    for char in ['~', '\"']:\n",
    "        report = report.replace(char, '')\n",
    "        \n",
    "    for char in ['!', '?', ';', ':', '.']:\n",
    "        report = report.replace(char, ' . ')\n",
    "    \n",
    "    for char in ['(', ')', ',', '/', '\\\\']:\n",
    "        report = report.replace(char, ' , ')\n",
    "        \n",
    "    # filter out 'comparison' + dates\n",
    "    #report = re.sub('( compar.*?\\d{4})', '', report)\n",
    "    report = re.sub('(compar.*?\\d{4})', '', report)\n",
    "    report = re.sub('(compar.*?\\d{2})', '', report)\n",
    "    report = re.sub('(xr knee both)', '', report)\n",
    "    report = re.sub('(xr knee)', '', report)\n",
    "    report = re.sub('(previous.*comparison)', '', report)\n",
    "    report = re.sub('( please.*?\\.)', '', report)\n",
    "\n",
    "    # Tokenize\n",
    "    report_tok = word_tokenize(report)\n",
    "    report_length = len(report_tok)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_report_tok = [word for word in report_tok if word not in stopwords.words('english')]\n",
    "    filtered_report = ' '.join(filtered_report_tok)\n",
    "    \n",
    "    reports.append(filtered_report)\n",
    "    reports_tok.append(filtered_report_tok)\n",
    "    labels.append(row['Accession'])\n",
    "    [words.append(word) for word in filtered_report_tok]\n",
    "    report_lengths.append(report_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  1735\n",
      "Avg no. of appearances:  35.4039238315\n",
      "Max:  [(u'.', 12474), (u'joint', 3749)]\n",
      "STD:  170.997692276\n",
      "Number of reports:  3561\n",
      "Average length of report:  30.5237292895\n",
      "STD:  18.2751155872\n",
      "\n",
      "Average number of sentences:  2.70176916596\n",
      "STD:  1.96696094788\n",
      "\n",
      "Percentage of words that appear >=5 times:  34\n",
      "Unnamed: 0          3561\n",
      "Accession           3561\n",
      "Clinical history    2221\n",
      "Comment             2390\n",
      "Report text         3561\n",
      "Cleaned report      3561\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print 'Vocab length: ', len(sorted(set(words)))\n",
    "fdist_all = nltk.FreqDist(words)\n",
    "freqs = [freq for word, freq in fdist_all.most_common() if word not in ['.', ',']]\n",
    "print 'Avg no. of appearances: ', np.mean(freqs)\n",
    "print 'Max: ', fdist_all.most_common(2)\n",
    "print 'STD: ', np.std(freqs)\n",
    "#print fdist_all.hapaxes()\n",
    "print 'Number of reports: ', len(reports)\n",
    "print 'Average length of report: ', np.mean(report_lengths)\n",
    "print 'STD: ', np.std(report_lengths)\n",
    "print ''\n",
    "print 'Average number of sentences: ', np.mean(num_sentences)\n",
    "print 'STD: ', np.std(num_sentences)\n",
    "print ''\n",
    "more_than_5 = [i for i in freqs if i >= 5]\n",
    "print 'Percentage of words that appear >=5 times: ', 100*len(more_than_5)//len(freqs)\n",
    "df['Cleaned report'] = reports\n",
    "print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patient_ids = []\n",
    "for folder in os.listdir(dir+'data/Images'):\n",
    "    #os.path.exists(self.labelfilename)\n",
    "    patient_ids.append(str(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "print len(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total exams:  327\n",
      "\n",
      "Views:\n",
      "{'L_R': 43, 'HBL_L': 7, 'S_R': 20, 'AP_R': 38, 'WBAP_R': 19, 'WBL_R': 9, 'WBL_L': 8, 'AP_L': 39, 'WBAP_L': 22, 'HBL_R': 5, 'S_L': 20, 'L_L': 47, 'WBAP_B': 1}\n",
      "63 36\n"
     ]
    }
   ],
   "source": [
    "sample_exams = []\n",
    "views = {}\n",
    "APL_views = ['L_R', 'L_L', 'AP_L', 'AP_R', 'HBL_L', 'HBL_R']\n",
    "tot_APL_views = []\n",
    "\n",
    "for pid in patient_ids:\n",
    "    item = {}\n",
    "    item['patient id'] = pid\n",
    "    apl_views = 0\n",
    "    \n",
    "    if pid in labels:\n",
    "        item['report'] = reports[labels.index(pid)]\n",
    "        item['tokens'] = reports_tok[labels.index(pid)]\n",
    "        images = []\n",
    "        for image in os.listdir(dir + 'data/Images/' + str(pid)):\n",
    "            if image.endswith('.jpg'):\n",
    "                image_dict = {}\n",
    "                imid = os.path.splitext(image)[0] \n",
    "                image_dict['impath'] = pid + '/' + image\n",
    "\n",
    "                if '_' in image and len(imid.split('_')) > 2:  \n",
    "                    leg = imid.split('_')[-1]\n",
    "                    view = imid.split('_')[-2] + '_' + leg\n",
    "\n",
    "                    image_dict['imid'] = imid.split('_')[0]\n",
    "                    image_dict['leg'] = leg\n",
    "                    image_dict['view'] = view\n",
    "                    if view in APL_views:\n",
    "                        apl_views = apl_views+1\n",
    "                    if view not in views.keys():\n",
    "                        views[view] = 0\n",
    "                    views[view] = views[view] + 1\n",
    "                else:\n",
    "                    apl_views = -1\n",
    "                    image_dict['imid'] = imid\n",
    "\n",
    "                images.append(image_dict)\n",
    "        item['images'] = images\n",
    "        sample_exams.append(item)\n",
    "    else: continue     \n",
    "\n",
    "    tot_APL_views.append(apl_views)\n",
    "\n",
    "print 'Total exams: ', len(sample_exams)\n",
    "print ''\n",
    "print 'Views:'\n",
    "print views\n",
    "with_views = [views for views in tot_APL_views if views>=0]\n",
    "more_than_4 = [views for views in tot_APL_views if views>=4]\n",
    "print len(with_views), len(more_than_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patient id': 'RJ109723379', 'report': u'. images acquired orthopaedic . significant bony abnormality seen .', 'images': [{'imid': 'CR.1.2.840.113564.1018830201.20150216093656437190.1003000225002', 'impath': 'RJ109723379/CR.1.2.840.113564.1018830201.20150216093656437190.1003000225002.jpg'}, {'imid': 'CR.1.2.840.113564.1018830201.20150216093656468220.1003000225002', 'impath': 'RJ109723379/CR.1.2.840.113564.1018830201.20150216093656468220.1003000225002.jpg'}, {'imid': 'CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000344', 'impath': 'RJ109723379/CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000344.jpg'}, {'imid': 'CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000336', 'impath': 'RJ109723379/CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000336.jpg'}, {'imid': 'CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000330', 'impath': 'RJ109723379/CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000330.jpg'}, {'imid': 'CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000342', 'impath': 'RJ109723379/CR.1.3.12.2.1107.5.4.4.1302.30000015021410095576500000342.jpg'}], 'tokens': [u'.', u'images', u'acquired', u'orthopaedic', u'.', u'significant', u'bony', u'abnormality', u'seen', u'.']}\n"
     ]
    }
   ],
   "source": [
    "print sample_exams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = Counter()\n",
    "pids = set()\n",
    "for item in sample_exams:\n",
    "    pid = item['patient id']\n",
    "    if pid not in pids:\n",
    "        tokens = item['tokens']\n",
    "        allwords.update(tokens)\n",
    "\n",
    "    pids.add(pid) \n",
    "\n",
    "vocab = [k for k, v in allwords.items() if v >= 5]\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sample_exams:\n",
    "    old_tokens = item['tokens']\n",
    "    item['tokens'] = [word for word in old_tokens if word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_val = model_selection.train_test_split(sample_exams, train_size=0.8, random_state=42)\n",
    "test, val = model_selection.train_test_split(test_val, train_size=0.5, random_state=42)\n",
    "\n",
    "for item in train:\n",
    "    item['split'] = 'train'\n",
    "\n",
    "for item in val:\n",
    "    item['split'] = 'val'\n",
    "\n",
    "for item in test:\n",
    "    item['split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  261\n",
      "Val:  33\n",
      "Test:  33\n",
      "Total 327\n"
     ]
    }
   ],
   "source": [
    "sample_exams = train+test+val\n",
    "print 'Train: ', len(train)\n",
    "print 'Val: ', len(val)\n",
    "print 'Test: ', len(test)\n",
    "print 'Total', len(sample_exams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sample_exams, open('sample_exams_train_test_split.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "1\n",
      "6\n",
      "7\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "1\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "7\n",
      "7\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "1\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for exam in sample_exams:\n",
    "    print len(exam['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TheanoLasagne]",
   "language": "python",
   "name": "conda-env-TheanoLasagne-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
