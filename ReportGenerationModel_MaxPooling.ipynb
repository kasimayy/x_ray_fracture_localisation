{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python27.zip',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/plat-linux2',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-tk',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-old',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-dynload',\n",
       " '/homes/ag6516/.local/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/PIL',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/homes/ag6516/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set local python and nltk paths\n",
    "import sys\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 164, in <module>\n",
      "    use(config.device)\n",
      "  File \"/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 151, in use\n",
      "    init_dev(device)\n",
      "  File \"/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 60, in init_dev\n",
      "    sched=config.gpuarray.sched)\n",
      "  File \"pygpu/gpuarray.pyx\", line 634, in pygpu.gpuarray.init\n",
      "  File \"pygpu/gpuarray.pyx\", line 584, in pygpu.gpuarray.pygpu_init\n",
      "  File \"pygpu/gpuarray.pyx\", line 1057, in pygpu.gpuarray.GpuContext.__cinit__\n",
      "GpuArrayException: cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import randint\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Set THEANO_FLAGS='device=cuda0,floatX=float32' to run notebook on gpu\n",
    "import lasagne\n",
    "from lasagne.utils import floatX\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from collections import Counter\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = ('/vol/medic02/users/ag6516/x_ray_fracture_localisation/')\n",
    "# dir = ('/Users/Aydan/PhD/x_ray_fracture_localisation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2154\n"
     ]
    }
   ],
   "source": [
    "#sample_exams = pickle.load(open('sample_exams_bbox_features.pkl'))\n",
    "sample_exams = pickle.load(open('augmented_exams_bbox_features.pkl'))\n",
    "print len(sample_exams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for item in sample_exams:\n",
    "    if item['split']!='train':\n",
    "        #print item\n",
    "\n",
    "        counter = counter +1\n",
    "print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = Counter()\n",
    "for item in sample_exams:\n",
    "    tokens = item['tokens']\n",
    "    #for sentence in item['sentences']:\n",
    "    allwords.update(tokens)\n",
    "        \n",
    "vocab = [k for k, v in allwords.items()]\n",
    "vocab.insert(0, '#START#')\n",
    "vocab.append('#END#')\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exam_ids = []\n",
    "for folder in os.listdir(dir+'data/Images'):\n",
    "    #os.path.exists(self.labelfilename)\n",
    "    exam_ids.append(str(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "print len(exam_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pool CNN image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = [item['patient id'] for item in sample_exams]\n",
    "pids = set(pids)\n",
    "print len(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sample_exams[2]['images'][0]['bbox cnn features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_sample_exams = []\n",
    "for exam in sample_exams:\n",
    "    item = exam\n",
    "    all_cnn_features = [image['bbox cnn features'] for image in exam['images']]\n",
    "    all_cnn_features_array = np.array(all_cnn_features)\n",
    "    #print all_cnn_features_array.shape\n",
    "    item['max_bbox_cnn_features'] = np.max(all_cnn_features_array, axis=0)\n",
    "    pooled_sample_exams.append(exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print pooled_sample_exams[0]['max_bbox_cnn_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(pooled_sample_exams, open('aug_exams_bbox_pooled_features.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Image Caption Model\n",
    "<img src=\"caption_model.png\">\n",
    "\n",
    "Implemented in Theano/Lasagne based on __[pydata2015 tutorial](https://github.com/ebenolson/pydata2015/tree/master/4%20-%20Recurrent%20Networks)__ for natural image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2154\n"
     ]
    }
   ],
   "source": [
    "pooled_sample_exams = pickle.load(open('aug_exams_bbox_pooled_features.pkl'))\n",
    "print len(pooled_sample_exams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 33\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token\n",
    "#BATCH_SIZE = 50\n",
    "BATCH_SIZE = 30\n",
    "CNN_FEATURE_SIZE = 1024\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a list of tuples (cnn features, list of words, image ID)\n",
    "def get_data_batch(dataset, size, split='train'):\n",
    "    items = []\n",
    "    \n",
    "    while len(items) < size:\n",
    "        item = random.choice(dataset)\n",
    "        if item['split'] != split:\n",
    "            continue\n",
    "        sentence = item['tokens']\n",
    "        if len(sentence) > MAX_SENTENCE_LENGTH:\n",
    "            sentence = sentence[1:MAX_SENTENCE_LENGTH]\n",
    "            #continue\n",
    "        items.append((item['max_bbox_cnn_features'], sentence, item['patient id']))\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a list of tuples into arrays that can be fed into the network\n",
    "def prep_batch_for_network(batch):\n",
    "    x_cnn = floatX(np.zeros((len(batch), CNN_FEATURE_SIZE)))\n",
    "    x_sentence = np.zeros((len(batch), SEQUENCE_LENGTH - 1), dtype='int32')\n",
    "    y_sentence = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='int32')\n",
    "    mask = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='bool')\n",
    "\n",
    "    for j, (cnn_features, sentence, _) in enumerate(batch):\n",
    "        x_cnn[j] = cnn_features\n",
    "        i = 0\n",
    "        for word in ['#START#'] + sentence + ['#END#']:\n",
    "            if word in word_to_index:\n",
    "                mask[j, i] = True\n",
    "                y_sentence[j, i] = word_to_index[word]\n",
    "                x_sentence[j, i] = word_to_index[word]\n",
    "                i += 1\n",
    "        #mask[j, 0] = False\n",
    "                \n",
    "    return x_cnn, x_sentence, y_sentence, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding maps integer sequence with dim (BATCH_SIZE, SEQUENCE_LENGTH - 1) to \n",
    "# (BATCH_SIZE, SEQUENCE_LENGTH-1, EMBEDDING_SIZE)\n",
    "l_input_sentence = lasagne.layers.InputLayer((BATCH_SIZE, SEQUENCE_LENGTH - 1))\n",
    "l_sentence_embedding = lasagne.layers.EmbeddingLayer(l_input_sentence,\n",
    "                                                     input_size=len(vocab),\n",
    "                                                     output_size=EMBEDDING_SIZE,\n",
    "                                                    )\n",
    "\n",
    "# cnn embedding changes the dimensionality of the representation from 1024 to EMBEDDING_SIZE, \n",
    "# and reshapes to add the time dimension - final dim (BATCH_SIZE, 1, EMBEDDING_SIZE)\n",
    "l_input_cnn = lasagne.layers.InputLayer((BATCH_SIZE, CNN_FEATURE_SIZE))\n",
    "l_cnn_embedding = lasagne.layers.DenseLayer(l_input_cnn, num_units=EMBEDDING_SIZE,\n",
    "                                            nonlinearity=lasagne.nonlinearities.identity)\n",
    "\n",
    "l_cnn_embedding = lasagne.layers.ReshapeLayer(l_cnn_embedding, ([0], 1, [1]))\n",
    "\n",
    "# the two are concatenated to form the RNN input with dim (BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_rnn_input = lasagne.layers.ConcatLayer([l_cnn_embedding, l_sentence_embedding])\n",
    "\n",
    "\n",
    "l_dropout_input = lasagne.layers.DropoutLayer(l_rnn_input, p=0.5)\n",
    "\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_dropout_input,\n",
    "                                  num_units=EMBEDDING_SIZE,\n",
    "                                  unroll_scan=True,\n",
    "                                  grad_clipping=5.)\n",
    "\n",
    "l_dropout_output = lasagne.layers.DropoutLayer(l_lstm, p=0.5)\n",
    "\n",
    "# the RNN output is reshaped to combine the batch and time dimensions\n",
    "# dim (BATCH_SIZE * SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_shp = lasagne.layers.ReshapeLayer(l_dropout_output, (-1, EMBEDDING_SIZE))\n",
    "\n",
    "# decoder is a fully connected layer with one output unit for each word in the vocabulary\n",
    "l_decoder = lasagne.layers.DenseLayer(l_shp, num_units=len(vocab), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# finally, the separation between batch and time dimension is restored\n",
    "l_out = lasagne.layers.ReshapeLayer(l_decoder, (BATCH_SIZE, SEQUENCE_LENGTH, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn feature vector\n",
    "x_cnn_sym = T.matrix()\n",
    "\n",
    "# sentence encoded as sequence of integer word tokens\n",
    "x_sentence_sym = T.imatrix()\n",
    "\n",
    "# mask defines which elements of the sequence should be predicted\n",
    "mask_sym = T.imatrix()\n",
    "\n",
    "# ground truth for the RNN output\n",
    "y_sentence_sym = T.imatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(l_out, {\n",
    "                l_input_sentence: x_sentence_sym,\n",
    "                l_input_cnn: x_cnn_sym\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cross_ent(net_output, mask, targets):\n",
    "    # Helper function to calculate the cross entropy error\n",
    "    preds = T.reshape(net_output, (-1, len(vocab)))\n",
    "    targets = T.flatten(targets)\n",
    "    cost = T.nnet.categorical_crossentropy(preds, targets)[T.flatten(mask).nonzero()]\n",
    "    return cost\n",
    "\n",
    "loss = T.mean(calc_cross_ent(output, mask_sym, y_sentence_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 15\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = T.grad(loss, all_params)\n",
    "all_grads = [T.clip(g, -5, 5) for g in all_grads]\n",
    "all_grads, norm = lasagne.updates.total_norm_constraint(\n",
    "    all_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=0.00001)\n",
    "\n",
    "f_train = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym],\n",
    "                          [loss, norm],\n",
    "                          updates=updates\n",
    "                         )\n",
    "\n",
    "f_val = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss_train: 5.11798620224, norm: 0.47153237462\n",
      "Val loss: 5.11996221542\n",
      "Iteration 250, loss_train: 4.74483728409, norm: 0.897225558758\n",
      "Val loss: 4.79544782639\n",
      "Iteration 500, loss_train: 4.15996026993, norm: 0.606463491917\n",
      "Val loss: 4.29827451706\n",
      "Iteration 750, loss_train: 3.9942214489, norm: 0.546978473663\n",
      "Val loss: 4.22887182236\n",
      "Iteration 1000, loss_train: 4.01808023453, norm: 0.43532627821\n",
      "Val loss: 4.04338502884\n",
      "Iteration 1250, loss_train: 3.96464633942, norm: 0.464104354382\n",
      "Val loss: 4.11902952194\n",
      "Iteration 1500, loss_train: 3.94541192055, norm: 0.396647065878\n",
      "Val loss: 4.04048442841\n",
      "Iteration 1750, loss_train: 3.90302729607, norm: 0.508979678154\n",
      "Val loss: 4.02652406693\n",
      "Iteration 2000, loss_train: 3.9363117218, norm: 0.45281213522\n",
      "Val loss: 3.97546458244\n",
      "Iteration 2250, loss_train: 3.97146129608, norm: 0.503706634045\n",
      "Val loss: 4.02056312561\n",
      "Iteration 2500, loss_train: 3.86403107643, norm: 0.682198047638\n",
      "Val loss: 3.9173989296\n",
      "Iteration 2750, loss_train: 3.92726635933, norm: 0.512647688389\n",
      "Val loss: 4.02324008942\n",
      "Iteration 3000, loss_train: 3.86843585968, norm: 0.779738426208\n",
      "Val loss: 3.97183823586\n",
      "Iteration 3250, loss_train: 3.83658719063, norm: 0.791488409042\n",
      "Val loss: 3.88485193253\n",
      "Iteration 3500, loss_train: 3.78006124496, norm: 1.06079816818\n",
      "Val loss: 3.92508029938\n",
      "Iteration 3750, loss_train: 3.80648112297, norm: 1.07160162926\n",
      "Val loss: 3.68219661713\n",
      "Iteration 4000, loss_train: 3.7186858654, norm: 0.586526334286\n",
      "Val loss: 3.81058645248\n",
      "Iteration 4250, loss_train: 3.75922870636, norm: 1.26397895813\n",
      "Val loss: 3.83499741554\n",
      "Iteration 4500, loss_train: 3.78581762314, norm: 1.63190805912\n",
      "Val loss: 3.77890133858\n",
      "Iteration 4750, loss_train: 3.62181949615, norm: 0.910091280937\n",
      "Val loss: 3.91079187393\n",
      "Iteration 5000, loss_train: 3.63504314423, norm: 1.01563775539\n",
      "Val loss: 3.85083270073\n",
      "Iteration 5250, loss_train: 3.71050977707, norm: 1.31180131435\n",
      "Val loss: 3.93137097359\n",
      "Iteration 5500, loss_train: 3.64363455772, norm: 1.04080402851\n",
      "Val loss: 3.98810863495\n",
      "Iteration 5750, loss_train: 3.61506581306, norm: 1.02267384529\n",
      "Val loss: 3.92782449722\n",
      "Iteration 6000, loss_train: 3.48943924904, norm: 0.895261764526\n",
      "Val loss: 3.73500490189\n",
      "Iteration 6250, loss_train: 3.6150226593, norm: 1.51647102833\n",
      "Val loss: 3.66527295113\n",
      "Iteration 6500, loss_train: 3.62182927132, norm: 2.3745496273\n",
      "Val loss: 3.73346590996\n",
      "Iteration 6750, loss_train: 3.55729413033, norm: 1.32394373417\n",
      "Val loss: 3.76452827454\n",
      "Iteration 7000, loss_train: 3.46520090103, norm: 0.856230974197\n",
      "Val loss: 3.66612267494\n",
      "Iteration 7250, loss_train: 3.53886651993, norm: 1.3031103611\n",
      "Val loss: 3.9838757515\n",
      "Iteration 7500, loss_train: 3.4407479763, norm: 1.91217780113\n",
      "Val loss: 3.69912958145\n",
      "Iteration 7750, loss_train: 3.36500692368, norm: 1.30984711647\n",
      "Val loss: 3.6280810833\n",
      "Iteration 8000, loss_train: 3.46906280518, norm: 1.66412508488\n",
      "Val loss: 3.69816207886\n",
      "Iteration 8250, loss_train: 3.40798902512, norm: 1.3123383522\n",
      "Val loss: 3.48964762688\n",
      "Iteration 8500, loss_train: 3.39895701408, norm: 2.73951458931\n",
      "Val loss: 3.78712058067\n",
      "Iteration 8750, loss_train: 3.32745718956, norm: 1.46461570263\n",
      "Val loss: 3.65512442589\n",
      "Iteration 9000, loss_train: 3.27628445625, norm: 1.26593291759\n",
      "Val loss: 3.68801617622\n",
      "Iteration 9250, loss_train: 3.24400353432, norm: 1.33516073227\n",
      "Val loss: 3.65830087662\n",
      "Iteration 9500, loss_train: 3.25218343735, norm: 1.02824020386\n",
      "Val loss: 3.56569242477\n",
      "Iteration 9750, loss_train: 3.282797575, norm: 1.14875245094\n",
      "Val loss: 3.59542918205\n",
      "Iteration 10000, loss_train: 3.30715203285, norm: 1.43032336235\n",
      "Val loss: 3.43077588081\n",
      "Iteration 10250, loss_train: 3.28744411469, norm: 2.02617144585\n",
      "Val loss: 3.61739206314\n",
      "Iteration 10500, loss_train: 3.22828626633, norm: 1.71248209476\n",
      "Val loss: 3.41296863556\n",
      "Iteration 10750, loss_train: 3.11883664131, norm: 1.72637629509\n",
      "Val loss: 3.64512348175\n",
      "Iteration 11000, loss_train: 3.15021848679, norm: 1.28403413296\n",
      "Val loss: 3.4618370533\n",
      "Iteration 11250, loss_train: 3.13241434097, norm: 1.30807316303\n",
      "Val loss: 3.69715499878\n",
      "Iteration 11500, loss_train: 3.1052210331, norm: 1.24063074589\n",
      "Val loss: 3.4649605751\n",
      "Iteration 11750, loss_train: 3.12293148041, norm: 1.76611590385\n",
      "Val loss: 3.50634169579\n",
      "Iteration 12000, loss_train: 3.1534216404, norm: 1.9230837822\n",
      "Val loss: 3.60799717903\n",
      "Iteration 12250, loss_train: 3.33118677139, norm: 1.41414940357\n",
      "Val loss: 3.56347775459\n",
      "Iteration 12500, loss_train: 3.0919752121, norm: 1.81834864616\n",
      "Val loss: 3.60593414307\n",
      "Iteration 12750, loss_train: 3.07291579247, norm: 1.57734167576\n",
      "Val loss: 3.68985390663\n",
      "Iteration 13000, loss_train: 3.15843868256, norm: 1.52341222763\n",
      "Val loss: 3.40097355843\n",
      "Iteration 13250, loss_train: 2.99376749992, norm: 1.57227408886\n",
      "Val loss: 3.26729345322\n",
      "Iteration 13500, loss_train: 2.95660543442, norm: 1.98755645752\n",
      "Val loss: 3.44944906235\n",
      "Iteration 13750, loss_train: 2.94840455055, norm: 2.59093236923\n",
      "Val loss: 3.57381820679\n",
      "Iteration 14000, loss_train: 3.03838229179, norm: 1.21638846397\n",
      "Val loss: 3.46130275726\n",
      "Iteration 14250, loss_train: 2.89258313179, norm: 1.30328202248\n",
      "Val loss: 3.27996969223\n",
      "Iteration 14500, loss_train: 2.92785429955, norm: 1.8926692009\n",
      "Val loss: 3.53812050819\n",
      "Iteration 14750, loss_train: 2.8644156456, norm: 1.4111084938\n",
      "Val loss: 3.40247750282\n",
      "Iteration 15000, loss_train: 2.87605619431, norm: 1.51290440559\n",
      "Val loss: 3.43928599358\n",
      "Iteration 15250, loss_train: 3.00084877014, norm: 1.42923688889\n",
      "Val loss: 3.51388001442\n",
      "Iteration 15500, loss_train: 2.86285829544, norm: 2.26058506966\n",
      "Val loss: 3.40715050697\n",
      "Iteration 15750, loss_train: 2.89542388916, norm: 1.62913787365\n",
      "Val loss: 3.41441011429\n",
      "Iteration 16000, loss_train: 2.81425285339, norm: 2.04758810997\n",
      "Val loss: 3.40669131279\n",
      "Iteration 16250, loss_train: 2.88448047638, norm: 1.71585571766\n",
      "Val loss: 3.45048260689\n",
      "Iteration 16500, loss_train: 2.81190800667, norm: 1.42059600353\n",
      "Val loss: 3.31097316742\n",
      "Iteration 16750, loss_train: 2.86037659645, norm: 1.65125203133\n",
      "Val loss: 3.18831443787\n",
      "Iteration 17000, loss_train: 2.83944344521, norm: 1.15992498398\n",
      "Val loss: 3.22664260864\n",
      "Iteration 17250, loss_train: 2.90732383728, norm: 1.43025660515\n",
      "Val loss: 3.23900985718\n",
      "Iteration 17500, loss_train: 2.83670949936, norm: 1.44415283203\n",
      "Val loss: 3.26097655296\n",
      "Iteration 17750, loss_train: 2.79814982414, norm: 1.62466573715\n",
      "Val loss: 3.3993909359\n",
      "Iteration 18000, loss_train: 2.60516428947, norm: 1.325740695\n",
      "Val loss: 3.18399357796\n",
      "Iteration 18250, loss_train: 2.73358678818, norm: 1.54594027996\n",
      "Val loss: 3.55562782288\n",
      "Iteration 18500, loss_train: 2.73667120934, norm: 1.57961416245\n",
      "Val loss: 3.26201748848\n",
      "Iteration 18750, loss_train: 2.81279397011, norm: 1.62851929665\n",
      "Val loss: 3.2010743618\n",
      "Iteration 19000, loss_train: 2.70317792892, norm: 1.53986811638\n",
      "Val loss: 3.36186337471\n",
      "Iteration 19250, loss_train: 2.53304171562, norm: 1.25378668308\n",
      "Val loss: 3.4353761673\n",
      "Iteration 19500, loss_train: 2.64167380333, norm: 1.33410441875\n",
      "Val loss: 3.27560019493\n",
      "Iteration 19750, loss_train: 2.73384070396, norm: 1.24869382381\n",
      "Val loss: 3.22371077538\n",
      "Iteration 20000, loss_train: 2.69935083389, norm: 1.84661376476\n",
      "Val loss: 3.0851585865\n",
      "Iteration 20250, loss_train: 2.6339905262, norm: 1.89837133884\n",
      "Val loss: 3.15791869164\n",
      "Iteration 20500, loss_train: 2.69882941246, norm: 1.6937084198\n",
      "Val loss: 3.30132889748\n",
      "Iteration 20750, loss_train: 2.57852005959, norm: 2.03372335434\n",
      "Val loss: 3.26062226295\n",
      "Iteration 21000, loss_train: 2.78878498077, norm: 1.60761475563\n",
      "Val loss: 3.28747558594\n",
      "Iteration 21250, loss_train: 2.48764872551, norm: 1.6822334528\n",
      "Val loss: 3.24342489243\n",
      "Iteration 21500, loss_train: 2.52605605125, norm: 1.23534822464\n",
      "Val loss: 3.2821700573\n",
      "Iteration 21750, loss_train: 2.52377438545, norm: 1.34201908112\n",
      "Val loss: 3.25819349289\n",
      "Iteration 22000, loss_train: 2.56403684616, norm: 1.89095211029\n",
      "Val loss: 3.08475613594\n",
      "Iteration 22250, loss_train: 2.56814932823, norm: 1.48629975319\n",
      "Val loss: 3.04157352448\n",
      "Iteration 22500, loss_train: 2.54750847816, norm: 1.96728312969\n",
      "Val loss: 3.36989927292\n",
      "Iteration 22750, loss_train: 2.50531387329, norm: 1.54202640057\n",
      "Val loss: 2.95930314064\n",
      "Iteration 23000, loss_train: 2.496489048, norm: 1.26369249821\n",
      "Val loss: 3.03078770638\n",
      "Iteration 23250, loss_train: 2.46522283554, norm: 1.51500082016\n",
      "Val loss: 3.20487952232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23500, loss_train: 2.50849413872, norm: 1.84056401253\n",
      "Val loss: 3.18845820427\n",
      "Iteration 23750, loss_train: 2.59103369713, norm: 1.58087468147\n",
      "Val loss: 3.1325275898\n",
      "Iteration 24000, loss_train: 2.45713877678, norm: 1.21170711517\n",
      "Val loss: 2.99557638168\n",
      "Iteration 24250, loss_train: 2.5453555584, norm: 2.16495919228\n",
      "Val loss: 2.92906332016\n",
      "Iteration 24500, loss_train: 2.49381995201, norm: 1.39315342903\n",
      "Val loss: 3.15862369537\n",
      "Iteration 24750, loss_train: 2.37597894669, norm: 1.53953135014\n",
      "Val loss: 3.26950788498\n",
      "Iteration 25000, loss_train: 2.32604694366, norm: 1.7332597971\n",
      "Val loss: 3.29300761223\n",
      "Iteration 25250, loss_train: 2.48542332649, norm: 1.5904365778\n",
      "Val loss: 3.17182660103\n",
      "Iteration 25500, loss_train: 2.40562677383, norm: 1.44222640991\n",
      "Val loss: 3.006524086\n",
      "Iteration 25750, loss_train: 2.3853533268, norm: 1.33662474155\n",
      "Val loss: 3.11103844643\n",
      "Iteration 26000, loss_train: 2.44840550423, norm: 1.79363083839\n",
      "Val loss: 3.0428237915\n",
      "Iteration 26250, loss_train: 2.4386408329, norm: 1.5142390728\n",
      "Val loss: 3.09287023544\n",
      "Iteration 26500, loss_train: 2.33276581764, norm: 1.6023068428\n",
      "Val loss: 3.17066836357\n",
      "Iteration 26750, loss_train: 2.33329343796, norm: 1.6263884306\n",
      "Val loss: 3.32784843445\n",
      "Iteration 27000, loss_train: 2.29807829857, norm: 1.48893404007\n",
      "Val loss: 3.38561463356\n",
      "Iteration 27250, loss_train: 2.29311561584, norm: 1.28572654724\n",
      "Val loss: 2.9738099575\n",
      "Iteration 27500, loss_train: 2.50932788849, norm: 1.79886174202\n",
      "Val loss: 3.22786569595\n",
      "Iteration 27750, loss_train: 2.39490175247, norm: 1.7645894289\n",
      "Val loss: 2.8698656559\n",
      "Iteration 28000, loss_train: 2.37642431259, norm: 1.64804518223\n",
      "Val loss: 2.88260197639\n",
      "Iteration 28250, loss_train: 2.3532371521, norm: 1.8056858778\n",
      "Val loss: 3.26049423218\n",
      "Iteration 28500, loss_train: 2.18028092384, norm: 1.53912711143\n",
      "Val loss: 3.23537802696\n",
      "Iteration 28750, loss_train: 2.30287647247, norm: 2.15142154694\n",
      "Val loss: 3.15013432503\n",
      "Iteration 29000, loss_train: 2.31581401825, norm: 1.58119308949\n",
      "Val loss: 3.22471094131\n",
      "Iteration 29250, loss_train: 2.26080393791, norm: 1.43699872494\n",
      "Val loss: 3.08110046387\n",
      "Iteration 29500, loss_train: 2.475004673, norm: 1.71557486057\n",
      "Val loss: 3.49740576744\n",
      "Iteration 29750, loss_train: 2.22773623466, norm: 1.35302019119\n",
      "Val loss: 3.18901562691\n",
      "Iteration 30000, loss_train: 2.31284403801, norm: 1.5102622509\n",
      "Val loss: 2.96202802658\n",
      "Iteration 30250, loss_train: 2.16894292831, norm: 2.02454686165\n",
      "Val loss: 2.9418656826\n",
      "Iteration 30500, loss_train: 2.25829935074, norm: 2.81432700157\n",
      "Val loss: 3.15335583687\n",
      "Iteration 30750, loss_train: 2.19270372391, norm: 2.06142783165\n",
      "Val loss: 3.13836407661\n",
      "Iteration 31000, loss_train: 2.28868937492, norm: 1.64475345612\n",
      "Val loss: 3.29957222939\n",
      "Iteration 31250, loss_train: 2.18426847458, norm: 1.57563090324\n",
      "Val loss: 2.79019117355\n",
      "Iteration 31500, loss_train: 2.08304619789, norm: 1.91854512691\n",
      "Val loss: 2.89891886711\n",
      "Iteration 31750, loss_train: 2.19380354881, norm: 2.05043292046\n",
      "Val loss: 3.0075507164\n",
      "Iteration 32000, loss_train: 2.31559443474, norm: 1.4597427845\n",
      "Val loss: 3.17510342598\n",
      "Iteration 32250, loss_train: 2.17604088783, norm: 1.48277056217\n",
      "Val loss: 3.00376439095\n",
      "Iteration 32500, loss_train: 2.1812274456, norm: 1.4009809494\n",
      "Val loss: 2.94522595406\n",
      "Iteration 32750, loss_train: 2.10255026817, norm: 1.36789977551\n",
      "Val loss: 3.10507917404\n",
      "Iteration 33000, loss_train: 2.14342474937, norm: 1.4661873579\n",
      "Val loss: 3.11519694328\n",
      "Iteration 33250, loss_train: 2.1980137825, norm: 1.85628914833\n",
      "Val loss: 2.88503432274\n",
      "Iteration 33500, loss_train: 2.21404147148, norm: 1.42964315414\n",
      "Val loss: 3.06694722176\n",
      "Iteration 33750, loss_train: 2.16553878784, norm: 1.56493198872\n",
      "Val loss: 3.14276123047\n",
      "Iteration 34000, loss_train: 2.33152890205, norm: 2.292283535\n",
      "Val loss: 3.4004983902\n",
      "Iteration 34250, loss_train: 2.13154745102, norm: 1.7565946579\n",
      "Val loss: 3.29650974274\n",
      "Iteration 34500, loss_train: 2.1249127388, norm: 1.76731050014\n",
      "Val loss: 2.8806746006\n",
      "Iteration 34750, loss_train: 2.08026170731, norm: 1.5952039957\n",
      "Val loss: 2.97113990784\n",
      "Iteration 35000, loss_train: 2.19860219955, norm: 1.93717718124\n",
      "Val loss: 3.06751918793\n",
      "Iteration 35250, loss_train: 2.09241843224, norm: 1.81948149204\n",
      "Val loss: 3.16161346436\n",
      "Iteration 35500, loss_train: 2.10590529442, norm: 1.88975191116\n",
      "Val loss: 3.12873506546\n",
      "Iteration 35750, loss_train: 2.01315021515, norm: 2.16595029831\n",
      "Val loss: 2.98954224586\n",
      "Iteration 36000, loss_train: 2.14933800697, norm: 1.74434566498\n",
      "Val loss: 3.40205788612\n",
      "Iteration 36250, loss_train: 2.10195541382, norm: 1.49346649647\n",
      "Val loss: 3.02806615829\n",
      "Iteration 36500, loss_train: 2.10740613937, norm: 1.88475430012\n",
      "Val loss: 3.40806174278\n",
      "Iteration 36750, loss_train: 2.07225894928, norm: 1.6954100132\n",
      "Val loss: 3.09469795227\n",
      "Iteration 37000, loss_train: 1.99638092518, norm: 1.38613069057\n",
      "Val loss: 2.90147471428\n",
      "Iteration 37250, loss_train: 2.11816620827, norm: 1.58997356892\n",
      "Val loss: 3.3015165329\n",
      "Iteration 37500, loss_train: 1.9731991291, norm: 1.45089495182\n",
      "Val loss: 3.00226593018\n",
      "Iteration 37750, loss_train: 2.08299255371, norm: 1.47226917744\n",
      "Val loss: 2.91422629356\n",
      "Iteration 38000, loss_train: 2.20316362381, norm: 1.70534145832\n",
      "Val loss: 3.42226743698\n",
      "Iteration 38250, loss_train: 2.12616300583, norm: 1.60335683823\n",
      "Val loss: 3.09484124184\n",
      "Iteration 38500, loss_train: 2.03226971626, norm: 1.46683204174\n",
      "Val loss: 2.94072294235\n",
      "Iteration 38750, loss_train: 2.05826568604, norm: 1.98071980476\n",
      "Val loss: 2.91189599037\n",
      "Iteration 39000, loss_train: 2.03167080879, norm: 2.30280375481\n",
      "Val loss: 3.22663617134\n",
      "Iteration 39250, loss_train: 2.01852202415, norm: 2.11919903755\n",
      "Val loss: 3.14987635612\n",
      "Iteration 39500, loss_train: 2.03335762024, norm: 1.79985368252\n",
      "Val loss: 3.00137782097\n",
      "Iteration 39750, loss_train: 2.01353406906, norm: 2.24805474281\n",
      "Val loss: 2.87928938866\n",
      "Iteration 40000, loss_train: 2.02519655228, norm: 1.67543113232\n",
      "Val loss: 3.267141819\n",
      "Iteration 40250, loss_train: 2.06483340263, norm: 1.39348018169\n",
      "Val loss: 3.0433485508\n",
      "Iteration 40500, loss_train: 1.97989785671, norm: 1.34557712078\n",
      "Val loss: 3.08468580246\n",
      "Iteration 40750, loss_train: 1.98893487453, norm: 1.59770774841\n",
      "Val loss: 3.29165625572\n",
      "Iteration 41000, loss_train: 2.10066175461, norm: 1.70880198479\n",
      "Val loss: 3.21181511879\n",
      "Iteration 41250, loss_train: 1.95985019207, norm: 1.79731369019\n",
      "Val loss: 2.98961377144\n",
      "Iteration 41500, loss_train: 2.01758337021, norm: 1.54363775253\n",
      "Val loss: 3.11054444313\n",
      "Iteration 41750, loss_train: 1.95438539982, norm: 1.60384941101\n",
      "Val loss: 3.12163424492\n",
      "Iteration 42000, loss_train: 1.99778020382, norm: 1.62485337257\n",
      "Val loss: 3.21754646301\n",
      "Iteration 42250, loss_train: 1.94029736519, norm: 1.99741721153\n",
      "Val loss: 3.22744607925\n",
      "Iteration 42500, loss_train: 1.94516372681, norm: 1.82603061199\n",
      "Val loss: 3.01197338104\n",
      "Iteration 42750, loss_train: 1.82319307327, norm: 1.57500290871\n",
      "Val loss: 3.3284072876\n",
      "Iteration 43000, loss_train: 2.01433944702, norm: 1.7631354332\n",
      "Val loss: 3.1715362072\n",
      "Iteration 43250, loss_train: 1.873437047, norm: 1.92658662796\n",
      "Val loss: 3.09056377411\n",
      "Iteration 43500, loss_train: 1.97160255909, norm: 1.62877047062\n",
      "Val loss: 2.86495614052\n",
      "Iteration 43750, loss_train: 1.81778490543, norm: 1.86305713654\n",
      "Val loss: 3.2278034687\n",
      "Iteration 44000, loss_train: 1.97237563133, norm: 1.61167991161\n",
      "Val loss: 3.23974847794\n",
      "Iteration 44250, loss_train: 1.85680103302, norm: 1.75199341774\n",
      "Val loss: 3.15637111664\n",
      "Iteration 44500, loss_train: 1.83585751057, norm: 1.90521228313\n",
      "Val loss: 3.00401210785\n",
      "Iteration 44750, loss_train: 1.83228135109, norm: 1.72097957134\n",
      "Val loss: 2.73620629311\n",
      "Iteration 45000, loss_train: 1.91531074047, norm: 1.54335916042\n",
      "Val loss: 2.98891091347\n",
      "Iteration 45250, loss_train: 1.86842572689, norm: 1.62251293659\n",
      "Val loss: 3.22267413139\n",
      "Iteration 45500, loss_train: 1.76336097717, norm: 1.44404006004\n",
      "Val loss: 3.15505838394\n",
      "Iteration 45750, loss_train: 1.88175165653, norm: 1.70375597477\n",
      "Val loss: 3.12888932228\n",
      "Iteration 46000, loss_train: 1.87915849686, norm: 1.62451827526\n",
      "Val loss: 3.30036258698\n",
      "Iteration 46250, loss_train: 1.93196702003, norm: 1.53393113613\n",
      "Val loss: 3.14182925224\n",
      "Iteration 46500, loss_train: 1.88388502598, norm: 1.68884313107\n",
      "Val loss: 3.15684843063\n",
      "Iteration 46750, loss_train: 1.77822124958, norm: 1.3971657753\n",
      "Val loss: 3.28431344032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47000, loss_train: 1.88040304184, norm: 1.99961519241\n",
      "Val loss: 3.29884696007\n",
      "Iteration 47250, loss_train: 1.89502048492, norm: 1.84867465496\n",
      "Val loss: 2.99142122269\n",
      "Iteration 47500, loss_train: 1.78564620018, norm: 1.55723977089\n",
      "Val loss: 3.11719512939\n",
      "Iteration 47750, loss_train: 1.84905159473, norm: 1.54200208187\n",
      "Val loss: 3.66166162491\n",
      "Iteration 48000, loss_train: 1.76266753674, norm: 1.55755746365\n",
      "Val loss: 3.09131932259\n",
      "Iteration 48250, loss_train: 1.85749363899, norm: 1.68874537945\n",
      "Val loss: 3.12291121483\n",
      "Iteration 48500, loss_train: 1.85549390316, norm: 1.72529315948\n",
      "Val loss: 2.91463446617\n",
      "Iteration 48750, loss_train: 1.80429935455, norm: 1.69543004036\n",
      "Val loss: 2.88680028915\n",
      "Iteration 49000, loss_train: 1.84607338905, norm: 1.64996373653\n",
      "Val loss: 3.20621705055\n",
      "Iteration 49250, loss_train: 1.75817012787, norm: 1.92368006706\n",
      "Val loss: 3.27400517464\n",
      "Iteration 49500, loss_train: 1.8195387125, norm: 2.41644692421\n",
      "Val loss: 3.2839910984\n",
      "Iteration 49750, loss_train: 1.89997410774, norm: 1.73453390598\n",
      "Val loss: 3.20446825027\n",
      "Iteration 50000, loss_train: 1.81367611885, norm: 1.7719963789\n",
      "Val loss: 3.11721301079\n",
      "Iteration 50250, loss_train: 1.74492239952, norm: 1.76081621647\n",
      "Val loss: 3.41865038872\n",
      "Iteration 50500, loss_train: 1.83111214638, norm: 1.56863880157\n",
      "Val loss: 3.20533585548\n",
      "Iteration 50750, loss_train: 1.71820139885, norm: 1.85489082336\n",
      "Val loss: 3.22941756248\n",
      "Iteration 51000, loss_train: 1.75224244595, norm: 1.45313644409\n",
      "Val loss: 3.24611592293\n",
      "Iteration 51250, loss_train: 1.83407497406, norm: 1.77816522121\n",
      "Val loss: 3.20653605461\n",
      "Iteration 51500, loss_train: 1.58578026295, norm: 1.45139753819\n",
      "Val loss: 2.94352340698\n",
      "Iteration 51750, loss_train: 1.72130084038, norm: 1.76188981533\n",
      "Val loss: 3.27634263039\n",
      "Iteration 52000, loss_train: 1.76666426659, norm: 1.96017551422\n",
      "Val loss: 2.78935790062\n",
      "Iteration 52250, loss_train: 1.80487775803, norm: 1.75185883045\n",
      "Val loss: 3.12798285484\n",
      "Iteration 52500, loss_train: 1.81415247917, norm: 1.7108438015\n",
      "Val loss: 3.08881688118\n",
      "Iteration 52750, loss_train: 1.79741358757, norm: 1.74508595467\n",
      "Val loss: 3.21738791466\n",
      "Iteration 53000, loss_train: 1.65305268764, norm: 1.34173595905\n",
      "Val loss: 3.32180833817\n",
      "Iteration 53250, loss_train: 1.69399404526, norm: 2.26720690727\n",
      "Val loss: 3.19737052917\n",
      "Iteration 53500, loss_train: 1.79207885265, norm: 2.34455084801\n",
      "Val loss: 3.22437214851\n",
      "Iteration 53750, loss_train: 1.69913959503, norm: 2.12584614754\n",
      "Val loss: 3.25158762932\n",
      "Iteration 54000, loss_train: 1.79632616043, norm: 1.59708702564\n",
      "Val loss: 3.27353334427\n",
      "Iteration 54250, loss_train: 1.69501256943, norm: 1.87216508389\n",
      "Val loss: 3.19565796852\n",
      "Iteration 54500, loss_train: 1.78054428101, norm: 1.61889481544\n",
      "Val loss: 3.29472160339\n",
      "Iteration 54750, loss_train: 1.6769618988, norm: 2.11816692352\n",
      "Val loss: 3.38382935524\n",
      "Iteration 55000, loss_train: 1.62179982662, norm: 1.57536780834\n",
      "Val loss: 3.09287858009\n",
      "Iteration 55250, loss_train: 1.58836209774, norm: 2.1206111908\n",
      "Val loss: 3.0819671154\n",
      "Iteration 55500, loss_train: 1.70300161839, norm: 1.74437868595\n",
      "Val loss: 3.15481376648\n",
      "Iteration 55750, loss_train: 1.67063784599, norm: 1.61837387085\n",
      "Val loss: 3.11881446838\n",
      "Iteration 56000, loss_train: 1.76955473423, norm: 2.14326930046\n",
      "Val loss: 2.94451403618\n",
      "Iteration 56250, loss_train: 1.60334515572, norm: 1.92458319664\n",
      "Val loss: 2.85346984863\n",
      "Iteration 56500, loss_train: 1.61746907234, norm: 2.10222053528\n",
      "Val loss: 3.00027608871\n",
      "Iteration 56750, loss_train: 1.65532159805, norm: 1.38356053829\n",
      "Val loss: 3.47671842575\n",
      "Iteration 57000, loss_train: 1.77633655071, norm: 1.86284434795\n",
      "Val loss: 3.13997578621\n",
      "Iteration 57250, loss_train: 1.59676134586, norm: 1.86177003384\n",
      "Val loss: 3.0039832592\n",
      "Iteration 57500, loss_train: 1.59908390045, norm: 1.66042947769\n",
      "Val loss: 3.29184031487\n",
      "Iteration 57750, loss_train: 1.62180519104, norm: 1.91973888874\n",
      "Val loss: 3.10615491867\n",
      "Iteration 58000, loss_train: 1.55064833164, norm: 1.51215970516\n",
      "Val loss: 3.31335330009\n",
      "Iteration 58250, loss_train: 1.61197686195, norm: 1.53286039829\n",
      "Val loss: 3.23536467552\n",
      "Iteration 58500, loss_train: 1.57937395573, norm: 1.57840192318\n",
      "Val loss: 2.98305273056\n",
      "Iteration 58750, loss_train: 1.5358812809, norm: 1.86997842789\n",
      "Val loss: 3.28313732147\n",
      "Iteration 59000, loss_train: 1.64677345753, norm: 1.87530326843\n",
      "Val loss: 3.08705425262\n",
      "Iteration 59250, loss_train: 1.51388216019, norm: 1.81377911568\n",
      "Val loss: 3.30541443825\n",
      "Iteration 59500, loss_train: 1.58213317394, norm: 1.80163085461\n",
      "Val loss: 3.16482639313\n",
      "Iteration 59750, loss_train: 1.55148470402, norm: 1.76442658901\n",
      "Val loss: 2.94887280464\n",
      "Iteration 60000, loss_train: 1.63886833191, norm: 1.64174151421\n",
      "Val loss: 3.0398671627\n",
      "Iteration 60250, loss_train: 1.68097054958, norm: 1.94239282608\n",
      "Val loss: 3.00658512115\n",
      "Iteration 60500, loss_train: 1.59027850628, norm: 1.91893804073\n",
      "Val loss: 3.06247115135\n",
      "Iteration 60750, loss_train: 1.64777195454, norm: 1.87766861916\n",
      "Val loss: 3.24019527435\n",
      "Iteration 61000, loss_train: 1.54711484909, norm: 1.60518157482\n",
      "Val loss: 3.65230631828\n",
      "Iteration 61250, loss_train: 1.61880230904, norm: 1.69310665131\n",
      "Val loss: 3.44151568413\n",
      "Iteration 61500, loss_train: 1.49077868462, norm: 1.54260098934\n",
      "Val loss: 3.15801692009\n",
      "Iteration 61750, loss_train: 1.51230013371, norm: 1.83063101768\n",
      "Val loss: 3.47960591316\n",
      "Iteration 62000, loss_train: 1.52915751934, norm: 1.54380714893\n",
      "Val loss: 2.98032164574\n",
      "Iteration 62250, loss_train: 1.49938952923, norm: 1.8480643034\n",
      "Val loss: 3.35016536713\n",
      "Iteration 62500, loss_train: 1.5274181366, norm: 1.86560511589\n",
      "Val loss: 3.10607767105\n",
      "Iteration 62750, loss_train: 1.63980150223, norm: 1.86468875408\n",
      "Val loss: 3.1977622509\n",
      "Iteration 63000, loss_train: 1.54496896267, norm: 1.99822700024\n",
      "Val loss: 3.12054014206\n",
      "Iteration 63250, loss_train: 1.44082295895, norm: 1.68107628822\n",
      "Val loss: 2.97324299812\n",
      "Iteration 63500, loss_train: 1.54086148739, norm: 1.77119266987\n",
      "Val loss: 3.12415313721\n",
      "Iteration 63750, loss_train: 1.49305093288, norm: 1.85952484608\n",
      "Val loss: 3.45068597794\n",
      "Iteration 64000, loss_train: 1.58103585243, norm: 1.48473727703\n",
      "Val loss: 3.40435481071\n",
      "Iteration 64250, loss_train: 1.53650832176, norm: 1.64447939396\n",
      "Val loss: 3.2192530632\n",
      "Iteration 64500, loss_train: 1.61527860165, norm: 2.16210818291\n",
      "Val loss: 3.35697126389\n",
      "Iteration 64750, loss_train: 1.5241727829, norm: 1.76872336864\n",
      "Val loss: 3.18124580383\n",
      "Iteration 65000, loss_train: 1.62325954437, norm: 2.51713681221\n",
      "Val loss: 3.420129776\n",
      "Iteration 65250, loss_train: 1.59364628792, norm: 1.90094792843\n",
      "Val loss: 3.30698823929\n",
      "Iteration 65500, loss_train: 1.4523601532, norm: 1.42131638527\n",
      "Val loss: 3.12281799316\n",
      "Iteration 65750, loss_train: 1.48410952091, norm: 2.05122327805\n",
      "Val loss: 3.67477536201\n",
      "Iteration 66000, loss_train: 1.46792793274, norm: 1.66185390949\n",
      "Val loss: 3.34131550789\n",
      "Iteration 66250, loss_train: 1.5990306139, norm: 1.83029603958\n",
      "Val loss: 2.96405792236\n",
      "Iteration 66500, loss_train: 1.47796940804, norm: 2.03275799751\n",
      "Val loss: 3.36641263962\n",
      "Iteration 66750, loss_train: 1.49563574791, norm: 1.84805381298\n",
      "Val loss: 3.08881044388\n",
      "Iteration 67000, loss_train: 1.49310851097, norm: 2.11534547806\n",
      "Val loss: 3.1843123436\n",
      "Iteration 67250, loss_train: 1.52116203308, norm: 1.92931973934\n",
      "Val loss: 3.26596426964\n",
      "Iteration 67500, loss_train: 1.4871224165, norm: 2.05767035484\n",
      "Val loss: 3.40142989159\n",
      "Iteration 67750, loss_train: 1.53156042099, norm: 2.08990526199\n",
      "Val loss: 3.24383234978\n",
      "Iteration 68000, loss_train: 1.50526642799, norm: 1.56600916386\n",
      "Val loss: 3.48606228828\n",
      "Iteration 68250, loss_train: 1.50808179379, norm: 1.54151177406\n",
      "Val loss: 3.38098073006\n",
      "Iteration 68500, loss_train: 1.51100575924, norm: 1.77480840683\n",
      "Val loss: 3.26038455963\n",
      "Iteration 68750, loss_train: 1.46070253849, norm: 1.4918295145\n",
      "Val loss: 3.29150891304\n",
      "Iteration 69000, loss_train: 1.55978941917, norm: 1.56320559978\n",
      "Val loss: 3.20057535172\n",
      "Iteration 69250, loss_train: 1.44551455975, norm: 2.0081911087\n",
      "Val loss: 3.54299521446\n",
      "Iteration 69500, loss_train: 1.44654655457, norm: 1.54931271076\n",
      "Val loss: 3.29073405266\n",
      "Iteration 69750, loss_train: 1.33739364147, norm: 1.51820683479\n",
      "Val loss: 3.32716751099\n",
      "Iteration 70000, loss_train: 1.57242417336, norm: 1.73719835281\n",
      "Val loss: 3.42201566696\n",
      "Iteration 70250, loss_train: 1.3694447279, norm: 1.77036595345\n",
      "Val loss: 3.39438652992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70500, loss_train: 1.45651066303, norm: 1.83062124252\n",
      "Val loss: 3.47296118736\n",
      "Iteration 70750, loss_train: 1.47821485996, norm: 1.66563725471\n",
      "Val loss: 3.29878544807\n",
      "Iteration 71000, loss_train: 1.51640331745, norm: 1.66282558441\n",
      "Val loss: 3.42839336395\n",
      "Iteration 71250, loss_train: 1.39008867741, norm: 2.28835844994\n",
      "Val loss: 3.22186660767\n",
      "Iteration 71500, loss_train: 1.38287436962, norm: 1.7904446125\n",
      "Val loss: 3.57697224617\n",
      "Iteration 71750, loss_train: 1.40616297722, norm: 2.35901856422\n",
      "Val loss: 3.33194541931\n",
      "Iteration 72000, loss_train: 1.43934226036, norm: 2.12216162682\n",
      "Val loss: 3.6637468338\n",
      "Iteration 72250, loss_train: 1.32425796986, norm: 1.55065560341\n",
      "Val loss: 3.2310693264\n",
      "Iteration 72500, loss_train: 1.35271716118, norm: 1.53700697422\n",
      "Val loss: 3.4615175724\n",
      "Iteration 72750, loss_train: 1.3802267313, norm: 1.63140928745\n",
      "Val loss: 3.58521080017\n",
      "Iteration 73000, loss_train: 1.38844239712, norm: 1.72284531593\n",
      "Val loss: 3.48099303246\n",
      "Iteration 73250, loss_train: 1.38942265511, norm: 1.89699137211\n",
      "Val loss: 3.11894655228\n",
      "Iteration 73500, loss_train: 1.30364251137, norm: 1.80766236782\n",
      "Val loss: 3.18436646461\n",
      "Iteration 73750, loss_train: 1.39581537247, norm: 1.9499322176\n",
      "Val loss: 3.07466506958\n",
      "Iteration 74000, loss_train: 1.35647952557, norm: 1.6656358242\n",
      "Val loss: 2.84738492966\n",
      "Iteration 74250, loss_train: 1.34065043926, norm: 1.77989017963\n",
      "Val loss: 3.4996535778\n",
      "Iteration 74500, loss_train: 1.43403327465, norm: 1.53006589413\n",
      "Val loss: 3.46271800995\n",
      "Iteration 74750, loss_train: 1.35578370094, norm: 1.96538579464\n",
      "Val loss: 3.84791040421\n",
      "Iteration 75000, loss_train: 1.44922494888, norm: 1.89788603783\n",
      "Val loss: 3.08213472366\n",
      "Iteration 75250, loss_train: 1.40161061287, norm: 1.85792398453\n",
      "Val loss: 3.4922990799\n",
      "Iteration 75500, loss_train: 1.32170116901, norm: 1.65388214588\n",
      "Val loss: 3.47150087357\n",
      "Iteration 75750, loss_train: 1.36992073059, norm: 1.69568347931\n",
      "Val loss: 3.34167718887\n",
      "Iteration 76000, loss_train: 1.44158434868, norm: 1.97920703888\n",
      "Val loss: 3.32562446594\n",
      "Iteration 76250, loss_train: 1.3857114315, norm: 1.86538863182\n",
      "Val loss: 3.36010289192\n",
      "Iteration 76500, loss_train: 1.307472229, norm: 1.93247377872\n",
      "Val loss: 3.32616019249\n",
      "Iteration 76750, loss_train: 1.46502530575, norm: 1.53860712051\n",
      "Val loss: 3.58901071548\n",
      "Iteration 77000, loss_train: 1.37226557732, norm: 1.71254837513\n",
      "Val loss: 3.41133308411\n",
      "Iteration 77250, loss_train: 1.3262244463, norm: 1.44038105011\n",
      "Val loss: 3.41600632668\n",
      "Iteration 77500, loss_train: 1.38762879372, norm: 1.52280855179\n",
      "Val loss: 3.17321920395\n",
      "Iteration 77750, loss_train: 1.36971175671, norm: 1.60843241215\n",
      "Val loss: 3.53096199036\n",
      "Iteration 78000, loss_train: 1.36750841141, norm: 2.01612901688\n",
      "Val loss: 3.69778752327\n",
      "Iteration 78250, loss_train: 1.23728609085, norm: 1.74947392941\n",
      "Val loss: 3.46588802338\n",
      "Iteration 78500, loss_train: 1.35627985001, norm: 1.70800209045\n",
      "Val loss: 3.51466345787\n",
      "Iteration 78750, loss_train: 1.34617114067, norm: 1.55785155296\n",
      "Val loss: 3.25854110718\n",
      "Iteration 79000, loss_train: 1.37507963181, norm: 1.73032152653\n",
      "Val loss: 3.55191302299\n",
      "Iteration 79250, loss_train: 1.37739372253, norm: 1.86901211739\n",
      "Val loss: 3.35697102547\n",
      "Iteration 79500, loss_train: 1.29869389534, norm: 1.49437189102\n",
      "Val loss: 3.59092617035\n",
      "Iteration 79750, loss_train: 1.31066036224, norm: 1.45383417606\n",
      "Val loss: 3.67774748802\n",
      "Iteration 80000, loss_train: 1.30184900761, norm: 1.77961111069\n",
      "Val loss: 2.94552826881\n",
      "Iteration 80250, loss_train: 1.34708082676, norm: 1.68433451653\n",
      "Val loss: 3.0159702301\n",
      "Iteration 80500, loss_train: 1.16486060619, norm: 1.61700153351\n",
      "Val loss: 3.60417079926\n",
      "Iteration 80750, loss_train: 1.35257017612, norm: 1.60001516342\n",
      "Val loss: 3.3477025032\n",
      "Iteration 81000, loss_train: 1.30799841881, norm: 1.68611478806\n",
      "Val loss: 3.73703575134\n",
      "Iteration 81250, loss_train: 1.22829735279, norm: 1.73982083797\n",
      "Val loss: 3.30869722366\n",
      "Iteration 81500, loss_train: 1.26144087315, norm: 1.62317800522\n",
      "Val loss: 3.56364440918\n",
      "Iteration 81750, loss_train: 1.31014585495, norm: 1.92681121826\n",
      "Val loss: 3.66090655327\n",
      "Iteration 82000, loss_train: 1.24657952785, norm: 1.46330499649\n",
      "Val loss: 3.08578658104\n",
      "Iteration 82250, loss_train: 1.31985473633, norm: 1.65083765984\n",
      "Val loss: 3.74453568459\n",
      "Iteration 82500, loss_train: 1.33122110367, norm: 2.00360822678\n",
      "Val loss: 2.749355793\n",
      "Iteration 82750, loss_train: 1.26699793339, norm: 1.5888466835\n",
      "Val loss: 3.31378626823\n",
      "Iteration 83000, loss_train: 1.24963450432, norm: 2.1580851078\n",
      "Val loss: 3.25089883804\n",
      "Iteration 83250, loss_train: 1.30839085579, norm: 1.61734676361\n",
      "Val loss: 3.77963852882\n",
      "Iteration 83500, loss_train: 1.35594916344, norm: 1.60533869267\n",
      "Val loss: 3.46683382988\n",
      "Iteration 83750, loss_train: 1.16226017475, norm: 1.62689650059\n",
      "Val loss: 3.40927219391\n",
      "Iteration 84000, loss_train: 1.29485297203, norm: 1.44103896618\n",
      "Val loss: 3.77663016319\n",
      "Iteration 84250, loss_train: 1.29058730602, norm: 1.84343540668\n",
      "Val loss: 3.56813192368\n",
      "Iteration 84500, loss_train: 1.24315714836, norm: 1.62689471245\n",
      "Val loss: 3.58152079582\n",
      "Iteration 84750, loss_train: 1.25115716457, norm: 1.66387009621\n",
      "Val loss: 3.40009522438\n",
      "Iteration 85000, loss_train: 1.26928067207, norm: 1.89045798779\n",
      "Val loss: 3.39946842194\n",
      "Iteration 85250, loss_train: 1.27087044716, norm: 2.49084472656\n",
      "Val loss: 3.31340003014\n",
      "Iteration 85500, loss_train: 1.27376902103, norm: 1.80998456478\n",
      "Val loss: 3.39247345924\n",
      "Iteration 85750, loss_train: 1.28858268261, norm: 1.57650649548\n",
      "Val loss: 3.43495821953\n",
      "Iteration 86000, loss_train: 1.21932590008, norm: 1.6562987566\n",
      "Val loss: 3.54552578926\n",
      "Iteration 86250, loss_train: 1.245429039, norm: 1.97220098972\n",
      "Val loss: 3.37393450737\n",
      "Iteration 86500, loss_train: 1.15241658688, norm: 1.69246101379\n",
      "Val loss: 3.61380934715\n",
      "Iteration 86750, loss_train: 1.21390748024, norm: 2.32269454002\n",
      "Val loss: 3.5090842247\n",
      "Iteration 87000, loss_train: 1.17276835442, norm: 1.57108855247\n",
      "Val loss: 3.55660128593\n",
      "Iteration 87250, loss_train: 1.20789086819, norm: 1.58873736858\n",
      "Val loss: 3.44135832787\n",
      "Iteration 87500, loss_train: 1.22012162209, norm: 1.72602820396\n",
      "Val loss: 4.05787038803\n",
      "Iteration 87750, loss_train: 1.12851500511, norm: 1.62015783787\n",
      "Val loss: 3.72977185249\n",
      "Iteration 88000, loss_train: 1.2411659956, norm: 1.88696098328\n",
      "Val loss: 3.14272713661\n",
      "Iteration 88250, loss_train: 1.17953383923, norm: 1.5656671524\n",
      "Val loss: 3.33738279343\n",
      "Iteration 88500, loss_train: 1.19128513336, norm: 1.61714994907\n",
      "Val loss: 3.53754472733\n",
      "Iteration 88750, loss_train: 1.26015412807, norm: 1.87145650387\n",
      "Val loss: 3.76077604294\n",
      "Iteration 89000, loss_train: 1.22520577908, norm: 2.0954720974\n",
      "Val loss: 4.06914615631\n",
      "Iteration 89250, loss_train: 1.2330313921, norm: 1.45925140381\n",
      "Val loss: 3.56767749786\n",
      "Iteration 89500, loss_train: 1.19394350052, norm: 1.44308924675\n",
      "Val loss: 3.62354230881\n",
      "Iteration 89750, loss_train: 1.17364394665, norm: 1.72526717186\n",
      "Val loss: 3.34274053574\n",
      "Iteration 90000, loss_train: 1.21425044537, norm: 1.72895240784\n",
      "Val loss: 3.28732848167\n",
      "Iteration 90250, loss_train: 1.13539648056, norm: 1.7231272459\n",
      "Val loss: 3.23742127419\n",
      "Iteration 90500, loss_train: 1.13365197182, norm: 1.60159265995\n",
      "Val loss: 3.59693694115\n",
      "Iteration 90750, loss_train: 1.13489198685, norm: 1.52538442612\n",
      "Val loss: 3.80446028709\n",
      "Iteration 91000, loss_train: 1.09449768066, norm: 1.4567835331\n",
      "Val loss: 3.59638881683\n",
      "Iteration 91250, loss_train: 1.18536782265, norm: 1.46836054325\n",
      "Val loss: 3.57121324539\n",
      "Iteration 91500, loss_train: 1.19054448605, norm: 1.56804347038\n",
      "Val loss: 3.65861225128\n",
      "Iteration 91750, loss_train: 1.19248485565, norm: 1.91871249676\n",
      "Val loss: 3.89959025383\n",
      "Iteration 92000, loss_train: 1.10462117195, norm: 1.50794243813\n",
      "Val loss: 3.67494750023\n",
      "Iteration 92250, loss_train: 1.18939483166, norm: 1.85852575302\n",
      "Val loss: 3.46055531502\n",
      "Iteration 92500, loss_train: 1.21113610268, norm: 1.78919577599\n",
      "Val loss: 3.43758010864\n",
      "Iteration 92750, loss_train: 1.11436629295, norm: 1.5987200737\n",
      "Val loss: 3.38891816139\n",
      "Iteration 93000, loss_train: 1.12653660774, norm: 1.41856098175\n",
      "Val loss: 4.0252737999\n",
      "Iteration 93250, loss_train: 1.10605752468, norm: 2.04266238213\n",
      "Val loss: 3.7490735054\n",
      "Iteration 93500, loss_train: 1.12578082085, norm: 2.02956223488\n",
      "Val loss: 3.7091422081\n",
      "Iteration 93750, loss_train: 1.13769376278, norm: 2.4454331398\n",
      "Val loss: 3.55709314346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 94000, loss_train: 1.18915867805, norm: 1.62773573399\n",
      "Val loss: 3.86365413666\n",
      "Iteration 94250, loss_train: 1.17069578171, norm: 1.65645492077\n",
      "Val loss: 3.33945560455\n",
      "Iteration 94500, loss_train: 1.14403951168, norm: 1.79713153839\n",
      "Val loss: 3.81969618797\n",
      "Iteration 94750, loss_train: 1.19704449177, norm: 2.00193953514\n",
      "Val loss: 3.99678134918\n",
      "Iteration 95000, loss_train: 1.10642015934, norm: 2.39348077774\n",
      "Val loss: 3.86284184456\n",
      "Iteration 95250, loss_train: 1.12835133076, norm: 1.73801875114\n",
      "Val loss: 4.09640645981\n",
      "Iteration 95500, loss_train: 1.12133646011, norm: 1.72093307972\n",
      "Val loss: 3.87421703339\n",
      "Iteration 95750, loss_train: 1.11028277874, norm: 1.91868042946\n",
      "Val loss: 3.49869346619\n",
      "Iteration 96000, loss_train: 1.2336666584, norm: 1.6736741066\n",
      "Val loss: 3.41858124733\n",
      "Iteration 96250, loss_train: 1.12607419491, norm: 2.03186178207\n",
      "Val loss: 4.05981683731\n",
      "Iteration 96500, loss_train: 1.18508386612, norm: 1.86972308159\n",
      "Val loss: 3.65179586411\n",
      "Iteration 96750, loss_train: 1.10077810287, norm: 1.56316614151\n",
      "Val loss: 3.88910937309\n",
      "Iteration 97000, loss_train: 1.06961488724, norm: 1.65732622147\n",
      "Val loss: 3.20613980293\n",
      "Iteration 97250, loss_train: 1.11061608791, norm: 1.4644151926\n",
      "Val loss: 3.4904589653\n",
      "Iteration 97500, loss_train: 1.09228897095, norm: 1.72033452988\n",
      "Val loss: 3.36340165138\n",
      "Iteration 97750, loss_train: 1.02486681938, norm: 1.54030144215\n",
      "Val loss: 3.49871587753\n",
      "Iteration 98000, loss_train: 1.12229573727, norm: 1.5783880949\n",
      "Val loss: 3.5451335907\n",
      "Iteration 98250, loss_train: 1.10498821735, norm: 1.71014869213\n",
      "Val loss: 3.63300442696\n",
      "Iteration 98500, loss_train: 1.04352748394, norm: 1.55354332924\n",
      "Val loss: 3.44969630241\n",
      "Iteration 98750, loss_train: 1.08468103409, norm: 1.43442118168\n",
      "Val loss: 3.68482518196\n",
      "Iteration 99000, loss_train: 1.09152090549, norm: 2.11866211891\n",
      "Val loss: 3.65564227104\n",
      "Iteration 99250, loss_train: 1.09954619408, norm: 1.42738318443\n",
      "Val loss: 3.64158987999\n",
      "Iteration 99500, loss_train: 1.03179597855, norm: 1.50891470909\n",
      "Val loss: 3.79104733467\n",
      "Iteration 99750, loss_train: 1.0708142519, norm: 1.68451917171\n",
      "Val loss: 3.70179128647\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100000):\n",
    "    x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(get_data_batch(pooled_sample_exams, BATCH_SIZE))\n",
    "    loss_train, norm = f_train(x_cnn, x_sentence, mask, y_sentence)\n",
    "    if not iteration % 250:\n",
    "        print('Iteration {}, loss_train: {}, norm: {}'.format(iteration, loss_train, norm))\n",
    "        try:\n",
    "            batch = get_data_batch(pooled_sample_exams, BATCH_SIZE, split='val')\n",
    "            x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(batch)\n",
    "            loss_val = f_val(x_cnn, x_sentence, mask, y_sentence)\n",
    "            print('Val loss: {}'.format(loss_val))\n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_values = lasagne.layers.get_all_param_values(l_out)\n",
    "d = {'param values': param_values,\n",
    "     'vocab': vocab,\n",
    "     'word_to_index': word_to_index,\n",
    "     'index_to_word': index_to_word,\n",
    "    }\n",
    "pickle.dump(d, open('aug_maxpooling_bbox_trained_v_167.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TheanoLasagne]",
   "language": "python",
   "name": "conda-env-TheanoLasagne-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
