{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python27.zip',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/plat-linux2',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-tk',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-old',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-dynload',\n",
       " '/homes/ag6516/.local/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/PIL',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/homes/ag6516/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set local python and nltk paths\n",
    "import sys\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1050 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import randint\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Set THEANO_FLAGS='device=cuda0,floatX=float32' to run notebook on gpu\n",
    "import lasagne\n",
    "from lasagne.utils import floatX\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from collections import Counter\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = ('/vol/medic02/users/ag6516/x_ray_fracture_localisation/')\n",
    "# dir = ('/Users/Aydan/PhD/x_ray_fracture_localisation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1432\n"
     ]
    }
   ],
   "source": [
    "sample_images = pickle.load(open('sample_images_baseline.pkl'))\n",
    "print len(sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = Counter()\n",
    "for item in sample_images:\n",
    "    tokens = item['tokens']\n",
    "    #for sentence in item['sentences']:\n",
    "    allwords.update(tokens)\n",
    "        \n",
    "vocab = [k for k, v in allwords.items()]\n",
    "vocab.insert(0, '#START#')\n",
    "vocab.append('#END#')\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exam_ids = []\n",
    "for folder in os.listdir(dir+'data/Images'):\n",
    "    #os.path.exists(self.labelfilename)\n",
    "    exam_ids.append(str(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "print len(exam_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pool CNN image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "pids = [item['patient id'] for item in sample_images]\n",
    "pids = set(pids)\n",
    "print len(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_exams = []\n",
    "for pid in exam_ids:\n",
    "    if pid in pids:\n",
    "        all_cnn_features = [item['cnn features'] for item in sample_images if item['patient id']==pid]\n",
    "        exam = [item for item in sample_images if item['patient id'] == pid][0]\n",
    "        all_cnn_features_array = np.array(all_cnn_features)\n",
    "        exam['max_cnn_features'] = np.max(all_cnn_features_array, axis=0)\n",
    "        sample_exams.append(exam)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(sample_exams, open('sample_exams_cnn_pooled_features.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Image Caption Model\n",
    "<img src=\"caption_model.png\">\n",
    "\n",
    "Implemented in Theano/Lasagne based on __[pydata2015 tutorial](https://github.com/ebenolson/pydata2015/tree/master/4%20-%20Recurrent%20Networks)__ for natural image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "sample_exams = pickle.load(open('sample_exams_cnn_pooled_features.pkl'))\n",
    "print len(sample_exams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 33\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token\n",
    "#BATCH_SIZE = 50\n",
    "BATCH_SIZE = 10\n",
    "CNN_FEATURE_SIZE = 1024\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a list of tuples (cnn features, list of words, image ID)\n",
    "def get_data_batch(dataset, size, split='train'):\n",
    "    items = []\n",
    "    \n",
    "    while len(items) < size:\n",
    "        item = random.choice(dataset)\n",
    "        if item['split'] != split:\n",
    "            continue\n",
    "        sentence = item['tokens']\n",
    "        if len(sentence) > MAX_SENTENCE_LENGTH:\n",
    "            sentence = sentence[1:MAX_SENTENCE_LENGTH]\n",
    "            #continue\n",
    "        items.append((item['max_cnn_features'], sentence, item['patient id']))\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a list of tuples into arrays that can be fed into the network\n",
    "def prep_batch_for_network(batch):\n",
    "    x_cnn = floatX(np.zeros((len(batch), CNN_FEATURE_SIZE)))\n",
    "    x_sentence = np.zeros((len(batch), SEQUENCE_LENGTH - 1), dtype='int32')\n",
    "    y_sentence = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='int32')\n",
    "    mask = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='bool')\n",
    "\n",
    "    for j, (cnn_features, sentence, _) in enumerate(batch):\n",
    "        x_cnn[j] = cnn_features\n",
    "        i = 0\n",
    "        for word in ['#START#'] + sentence + ['#END#']:\n",
    "            if word in word_to_index:\n",
    "                mask[j, i] = True\n",
    "                y_sentence[j, i] = word_to_index[word]\n",
    "                x_sentence[j, i] = word_to_index[word]\n",
    "                i += 1\n",
    "        #mask[j, 0] = False\n",
    "                \n",
    "    return x_cnn, x_sentence, y_sentence, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence embedding maps integer sequence with dim (BATCH_SIZE, SEQUENCE_LENGTH - 1) to \n",
    "# (BATCH_SIZE, SEQUENCE_LENGTH-1, EMBEDDING_SIZE)\n",
    "l_input_sentence = lasagne.layers.InputLayer((BATCH_SIZE, SEQUENCE_LENGTH - 1))\n",
    "l_sentence_embedding = lasagne.layers.EmbeddingLayer(l_input_sentence,\n",
    "                                                     input_size=len(vocab),\n",
    "                                                     output_size=EMBEDDING_SIZE,\n",
    "                                                    )\n",
    "\n",
    "# cnn embedding changes the dimensionality of the representation from 1024 to EMBEDDING_SIZE, \n",
    "# and reshapes to add the time dimension - final dim (BATCH_SIZE, 1, EMBEDDING_SIZE)\n",
    "l_input_cnn = lasagne.layers.InputLayer((BATCH_SIZE, CNN_FEATURE_SIZE))\n",
    "l_cnn_embedding = lasagne.layers.DenseLayer(l_input_cnn, num_units=EMBEDDING_SIZE,\n",
    "                                            nonlinearity=lasagne.nonlinearities.identity)\n",
    "\n",
    "l_cnn_embedding = lasagne.layers.ReshapeLayer(l_cnn_embedding, ([0], 1, [1]))\n",
    "\n",
    "# the two are concatenated to form the RNN input with dim (BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_rnn_input = lasagne.layers.ConcatLayer([l_cnn_embedding, l_sentence_embedding])\n",
    "\n",
    "\n",
    "l_dropout_input = lasagne.layers.DropoutLayer(l_rnn_input, p=0.5)\n",
    "\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_dropout_input,\n",
    "                                  num_units=EMBEDDING_SIZE,\n",
    "                                  unroll_scan=True,\n",
    "                                  grad_clipping=5.)\n",
    "\n",
    "l_dropout_output = lasagne.layers.DropoutLayer(l_lstm, p=0.5)\n",
    "\n",
    "# the RNN output is reshaped to combine the batch and time dimensions\n",
    "# dim (BATCH_SIZE * SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_shp = lasagne.layers.ReshapeLayer(l_dropout_output, (-1, EMBEDDING_SIZE))\n",
    "\n",
    "# decoder is a fully connected layer with one output unit for each word in the vocabulary\n",
    "l_decoder = lasagne.layers.DenseLayer(l_shp, num_units=len(vocab), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# finally, the separation between batch and time dimension is restored\n",
    "l_out = lasagne.layers.ReshapeLayer(l_decoder, (BATCH_SIZE, SEQUENCE_LENGTH, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn feature vector\n",
    "x_cnn_sym = T.matrix()\n",
    "\n",
    "# sentence encoded as sequence of integer word tokens\n",
    "x_sentence_sym = T.imatrix()\n",
    "\n",
    "# mask defines which elements of the sequence should be predicted\n",
    "mask_sym = T.imatrix()\n",
    "\n",
    "# ground truth for the RNN output\n",
    "y_sentence_sym = T.imatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(l_out, {\n",
    "                l_input_sentence: x_sentence_sym,\n",
    "                l_input_cnn: x_cnn_sym\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cross_ent(net_output, mask, targets):\n",
    "    # Helper function to calculate the cross entropy error\n",
    "    preds = T.reshape(net_output, (-1, len(vocab)))\n",
    "    targets = T.flatten(targets)\n",
    "    cost = T.nnet.categorical_crossentropy(preds, targets)[T.flatten(mask).nonzero()]\n",
    "    return cost\n",
    "\n",
    "loss = T.mean(calc_cross_ent(output, mask_sym, y_sentence_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 15\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = T.grad(loss, all_params)\n",
    "all_grads = [T.clip(g, -5, 5) for g in all_grads]\n",
    "all_grads, norm = lasagne.updates.total_norm_constraint(\n",
    "    all_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=0.001)\n",
    "\n",
    "f_train = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym],\n",
    "                          [loss, norm],\n",
    "                          updates=updates\n",
    "                         )\n",
    "\n",
    "f_val = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss_train: 4.66846942902, norm: 0.817887842655\n",
      "Val loss: 4.5886926651\n",
      "Iteration 250, loss_train: 2.31142282486, norm: 0.739516079426\n",
      "Val loss: 1.75672996044\n",
      "Iteration 500, loss_train: 2.25775527954, norm: 1.05124402046\n",
      "Val loss: 1.20261228085\n",
      "Iteration 750, loss_train: 1.264980793, norm: 0.964316725731\n",
      "Val loss: 0.895298361778\n",
      "Iteration 1000, loss_train: 1.33384406567, norm: 1.20700645447\n",
      "Val loss: 1.02093696594\n",
      "Iteration 1250, loss_train: 1.23437571526, norm: 1.17228233814\n",
      "Val loss: 0.853420734406\n",
      "Iteration 1500, loss_train: 1.09603011608, norm: 1.30654478073\n",
      "Val loss: 0.881110846996\n",
      "Iteration 1750, loss_train: 0.908065080643, norm: 1.029281497\n",
      "Val loss: 0.789424955845\n",
      "Iteration 2000, loss_train: 0.840369462967, norm: 1.05683624744\n",
      "Val loss: 1.04973554611\n",
      "Iteration 2250, loss_train: 0.981441140175, norm: 1.20361673832\n",
      "Val loss: 0.84905731678\n",
      "Iteration 2500, loss_train: 0.653212845325, norm: 0.832866668701\n",
      "Val loss: 0.909407734871\n",
      "Iteration 2750, loss_train: 0.684979379177, norm: 1.15748500824\n",
      "Val loss: 0.86323004961\n",
      "Iteration 3000, loss_train: 0.615938186646, norm: 0.905945718288\n",
      "Val loss: 0.882873475552\n",
      "Iteration 3250, loss_train: 0.653273403645, norm: 1.05134487152\n",
      "Val loss: 0.971426963806\n",
      "Iteration 3500, loss_train: 0.582516133785, norm: 0.798070549965\n",
      "Val loss: 0.917066931725\n",
      "Iteration 3750, loss_train: 0.489701926708, norm: 0.736962914467\n",
      "Val loss: 0.847554862499\n",
      "Iteration 4000, loss_train: 0.506980597973, norm: 0.917536377907\n",
      "Val loss: 0.980016648769\n",
      "Iteration 4250, loss_train: 0.44364079833, norm: 0.654936790466\n",
      "Val loss: 0.989153623581\n",
      "Iteration 4500, loss_train: 0.414884567261, norm: 0.764758586884\n",
      "Val loss: 1.05388116837\n",
      "Iteration 4750, loss_train: 0.513409256935, norm: 0.822607636452\n",
      "Val loss: 0.890723288059\n",
      "Iteration 5000, loss_train: 0.467535585165, norm: 0.80181324482\n",
      "Val loss: 0.756485044956\n",
      "Iteration 5250, loss_train: 0.479338139296, norm: 0.660689711571\n",
      "Val loss: 0.712451934814\n",
      "Iteration 5500, loss_train: 0.47504991293, norm: 0.703653335571\n",
      "Val loss: 0.731031477451\n",
      "Iteration 5750, loss_train: 0.37365052104, norm: 0.579544246197\n",
      "Val loss: 0.64004355669\n",
      "Iteration 6000, loss_train: 0.341563373804, norm: 0.674631655216\n",
      "Val loss: 0.750245451927\n",
      "Iteration 6250, loss_train: 0.348073422909, norm: 0.61298584938\n",
      "Val loss: 0.774010777473\n",
      "Iteration 6500, loss_train: 0.434788495302, norm: 0.850061297417\n",
      "Val loss: 0.75149166584\n",
      "Iteration 6750, loss_train: 0.372600376606, norm: 0.587540626526\n",
      "Val loss: 0.80567085743\n",
      "Iteration 7000, loss_train: 0.430698275566, norm: 0.526417315006\n",
      "Val loss: 0.724484145641\n",
      "Iteration 7250, loss_train: 0.392281115055, norm: 0.533251404762\n",
      "Val loss: 0.896126866341\n",
      "Iteration 7500, loss_train: 0.430226296186, norm: 0.749939084053\n",
      "Val loss: 0.921115756035\n",
      "Iteration 7750, loss_train: 0.485749185085, norm: 0.607661366463\n",
      "Val loss: 0.957969784737\n",
      "Iteration 8000, loss_train: 0.39497512579, norm: 0.751935124397\n",
      "Val loss: 1.08763349056\n",
      "Iteration 8250, loss_train: 0.459605783224, norm: 0.570577919483\n",
      "Val loss: 0.772679686546\n",
      "Iteration 8500, loss_train: 0.472163558006, norm: 1.06671082973\n",
      "Val loss: 0.881175994873\n",
      "Iteration 8750, loss_train: 0.418453216553, norm: 0.619556307793\n",
      "Val loss: 0.847294807434\n",
      "Iteration 9000, loss_train: 0.351975917816, norm: 0.529602885246\n",
      "Val loss: 0.883329689503\n",
      "Iteration 9250, loss_train: 0.431746989489, norm: 0.746057987213\n",
      "Val loss: 0.827358424664\n",
      "Iteration 9500, loss_train: 0.403575271368, norm: 1.44759714603\n",
      "Val loss: 0.605406939983\n",
      "Iteration 9750, loss_train: 0.404666930437, norm: 0.607039749622\n",
      "Val loss: 0.805514097214\n",
      "Iteration 10000, loss_train: 0.394543617964, norm: 0.78297662735\n",
      "Val loss: 0.800590217113\n",
      "Iteration 10250, loss_train: 0.37194827199, norm: 0.519226431847\n",
      "Val loss: 1.07685554028\n",
      "Iteration 10500, loss_train: 0.423705995083, norm: 0.874808132648\n",
      "Val loss: 0.830291450024\n",
      "Iteration 10750, loss_train: 0.464876651764, norm: 0.55136936903\n",
      "Val loss: 0.814756691456\n",
      "Iteration 11000, loss_train: 0.404916673899, norm: 0.650211751461\n",
      "Val loss: 0.896958827972\n",
      "Iteration 11250, loss_train: 0.410449355841, norm: 0.784524559975\n",
      "Val loss: 0.853219985962\n",
      "Iteration 11500, loss_train: 0.397289127111, norm: 0.701113820076\n",
      "Val loss: 0.961945772171\n",
      "Iteration 11750, loss_train: 0.386006981134, norm: 0.707401156425\n",
      "Val loss: 0.842378675938\n",
      "Iteration 12000, loss_train: 0.378246068954, norm: 0.554380118847\n",
      "Val loss: 0.980274617672\n",
      "Iteration 12250, loss_train: 0.361968696117, norm: 0.446161717176\n",
      "Val loss: 0.99532699585\n",
      "Iteration 12500, loss_train: 0.31479755044, norm: 0.355156540871\n",
      "Val loss: 0.744981050491\n",
      "Iteration 12750, loss_train: 0.38870331645, norm: 0.546323776245\n",
      "Val loss: 0.913782835007\n",
      "Iteration 13000, loss_train: 0.352848619223, norm: 0.418689638376\n",
      "Val loss: 0.874485731125\n",
      "Iteration 13250, loss_train: 0.402278989553, norm: 0.436745494604\n",
      "Val loss: 0.72030466795\n",
      "Iteration 13500, loss_train: 0.335796684027, norm: 0.471649080515\n",
      "Val loss: 0.813877701759\n",
      "Iteration 13750, loss_train: 0.388529807329, norm: 0.554265916348\n",
      "Val loss: 0.736814200878\n",
      "Iteration 14000, loss_train: 0.406788259745, norm: 0.465934038162\n",
      "Val loss: 0.858788251877\n",
      "Iteration 14250, loss_train: 0.345671981573, norm: 1.00546526909\n",
      "Val loss: 0.7974421978\n",
      "Iteration 14500, loss_train: 0.384988307953, norm: 0.619076907635\n",
      "Val loss: 0.758973538876\n",
      "Iteration 14750, loss_train: 0.280294954777, norm: 0.460059732199\n",
      "Val loss: 0.864761352539\n",
      "Iteration 15000, loss_train: 0.31737986207, norm: 0.503096461296\n",
      "Val loss: 0.863902568817\n",
      "Iteration 15250, loss_train: 0.432219028473, norm: 0.77770537138\n",
      "Val loss: 0.951269090176\n",
      "Iteration 15500, loss_train: 0.383127212524, norm: 0.402332663536\n",
      "Val loss: 0.747842371464\n",
      "Iteration 15750, loss_train: 0.36364993453, norm: 0.606897592545\n",
      "Val loss: 0.768424332142\n",
      "Iteration 16000, loss_train: 0.289953649044, norm: 0.618071854115\n",
      "Val loss: 0.763399779797\n",
      "Iteration 16250, loss_train: 0.332414150238, norm: 0.539791107178\n",
      "Val loss: 0.770142734051\n",
      "Iteration 16500, loss_train: 0.38016140461, norm: 0.660823404789\n",
      "Val loss: 0.716008603573\n",
      "Iteration 16750, loss_train: 0.285254299641, norm: 0.710242390633\n",
      "Val loss: 0.787844002247\n",
      "Iteration 17000, loss_train: 0.369855731726, norm: 1.30905020237\n",
      "Val loss: 0.665739536285\n",
      "Iteration 17250, loss_train: 0.357480764389, norm: 0.356584072113\n",
      "Val loss: 0.738105356693\n",
      "Iteration 17500, loss_train: 0.37651643157, norm: 0.609593212605\n",
      "Val loss: 0.875206291676\n",
      "Iteration 17750, loss_train: 0.360639005899, norm: 0.398961335421\n",
      "Val loss: 0.735076785088\n",
      "Iteration 18000, loss_train: 0.410432100296, norm: 0.351629406214\n",
      "Val loss: 0.762098193169\n",
      "Iteration 18250, loss_train: 0.375289440155, norm: 0.669209778309\n",
      "Val loss: 0.86091029644\n",
      "Iteration 18500, loss_train: 0.377108484507, norm: 0.446362286806\n",
      "Val loss: 0.815734565258\n",
      "Iteration 18750, loss_train: 0.29977157712, norm: 0.541487276554\n",
      "Val loss: 0.736390113831\n",
      "Iteration 19000, loss_train: 0.419810146093, norm: 0.50333237648\n",
      "Val loss: 0.80896115303\n",
      "Iteration 19250, loss_train: 0.385330736637, norm: 0.528788745403\n",
      "Val loss: 0.774381995201\n",
      "Iteration 19500, loss_train: 0.35803976655, norm: 0.540014624596\n",
      "Val loss: 0.923136591911\n",
      "Iteration 19750, loss_train: 0.336215615273, norm: 0.353901445866\n",
      "Val loss: 0.842835009098\n",
      "Iteration 20000, loss_train: 0.400795340538, norm: 0.767882108688\n",
      "Val loss: 0.67014425993\n",
      "Iteration 20250, loss_train: 0.391587793827, norm: 0.644789814949\n",
      "Val loss: 0.823415577412\n",
      "Iteration 20500, loss_train: 0.385294437408, norm: 0.612344622612\n",
      "Val loss: 0.733786225319\n",
      "Iteration 20750, loss_train: 0.338915407658, norm: 0.446052879095\n",
      "Val loss: 0.798338115215\n",
      "Iteration 21000, loss_train: 0.275294065475, norm: 0.305878311396\n",
      "Val loss: 0.733701407909\n",
      "Iteration 21250, loss_train: 0.336164981127, norm: 0.333617627621\n",
      "Val loss: 0.773491740227\n",
      "Iteration 21500, loss_train: 0.437248200178, norm: 0.357568740845\n",
      "Val loss: 0.79029160738\n",
      "Iteration 21750, loss_train: 0.314002484083, norm: 0.648295640945\n",
      "Val loss: 0.772446572781\n",
      "Iteration 22000, loss_train: 0.357325732708, norm: 0.412273526192\n",
      "Val loss: 0.642165780067\n",
      "Iteration 22250, loss_train: 0.332926154137, norm: 1.92392528057\n",
      "Val loss: 0.717747807503\n",
      "Iteration 22500, loss_train: 0.37154135108, norm: 0.464204907417\n",
      "Val loss: 0.932392120361\n",
      "Iteration 22750, loss_train: 0.282935827971, norm: 0.530696690083\n",
      "Val loss: 0.761458277702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23000, loss_train: 0.311948567629, norm: 0.627546429634\n",
      "Val loss: 0.76567709446\n",
      "Iteration 23250, loss_train: 0.292551994324, norm: 0.40446922183\n",
      "Val loss: 0.753806769848\n",
      "Iteration 23500, loss_train: 0.3649738729, norm: 0.83100181818\n",
      "Val loss: 0.751128077507\n",
      "Iteration 23750, loss_train: 0.338958829641, norm: 1.37261664867\n",
      "Val loss: 0.88844782114\n",
      "Iteration 24000, loss_train: 0.273982435465, norm: 0.340812444687\n",
      "Val loss: 0.775251567364\n",
      "Iteration 24250, loss_train: 0.281099826097, norm: 0.40333122015\n",
      "Val loss: 0.865962684155\n",
      "Iteration 24500, loss_train: 0.317500829697, norm: 0.350274980068\n",
      "Val loss: 0.704030692577\n",
      "Iteration 24750, loss_train: 0.373214423656, norm: 0.424488067627\n",
      "Val loss: 0.827071428299\n",
      "Iteration 25000, loss_train: 0.432173937559, norm: 0.777556836605\n",
      "Val loss: 0.759003281593\n",
      "Iteration 25250, loss_train: 0.294577658176, norm: 0.245822653174\n",
      "Val loss: 0.797252714634\n",
      "Iteration 25500, loss_train: 0.372259646654, norm: 0.523230671883\n",
      "Val loss: 0.721362054348\n",
      "Iteration 25750, loss_train: 0.320590823889, norm: 0.727771103382\n",
      "Val loss: 0.726449370384\n",
      "Iteration 26000, loss_train: 0.297479063272, norm: 0.337581664324\n",
      "Val loss: 0.675029456615\n",
      "Iteration 26250, loss_train: 0.47462528944, norm: 0.590773701668\n",
      "Val loss: 0.720633685589\n",
      "Iteration 26500, loss_train: 0.291408509016, norm: 0.28752976656\n",
      "Val loss: 0.677919626236\n",
      "Iteration 26750, loss_train: 0.312125056982, norm: 0.378054916859\n",
      "Val loss: 0.710180222988\n",
      "Iteration 27000, loss_train: 0.438826084137, norm: 0.660355806351\n",
      "Val loss: 0.756673157215\n",
      "Iteration 27250, loss_train: 0.358889400959, norm: 0.575350761414\n",
      "Val loss: 0.662508547306\n",
      "Iteration 27500, loss_train: 0.42778596282, norm: 0.751629769802\n",
      "Val loss: 0.793136715889\n",
      "Iteration 27750, loss_train: 0.38394805789, norm: 0.289790779352\n",
      "Val loss: 0.765994727612\n",
      "Iteration 28000, loss_train: 0.272131919861, norm: 0.380608052015\n",
      "Val loss: 0.705792844296\n",
      "Iteration 28250, loss_train: 0.320191353559, norm: 0.351193785667\n",
      "Val loss: 0.756153166294\n",
      "Iteration 28500, loss_train: 0.313688486814, norm: 0.480689585209\n",
      "Val loss: 0.805241346359\n",
      "Iteration 28750, loss_train: 0.379442870617, norm: 0.774137496948\n",
      "Val loss: 0.755349397659\n",
      "Iteration 29000, loss_train: 0.38338470459, norm: 0.680010139942\n",
      "Val loss: 0.78684169054\n",
      "Iteration 29250, loss_train: 0.329962849617, norm: 1.13532888889\n",
      "Val loss: 0.781673550606\n",
      "Iteration 29500, loss_train: 0.39174464345, norm: 0.410680621862\n",
      "Val loss: 0.788670063019\n",
      "Iteration 29750, loss_train: 0.345715284348, norm: 0.819099903107\n",
      "Val loss: 0.776255846024\n",
      "Iteration 30000, loss_train: 0.3301230371, norm: 0.361823379993\n",
      "Val loss: 0.79212474823\n",
      "Iteration 30250, loss_train: 0.394164353609, norm: 0.454348444939\n",
      "Val loss: 0.753550052643\n",
      "Iteration 30500, loss_train: 0.431877225637, norm: 0.353110551834\n",
      "Val loss: 0.746421635151\n",
      "Iteration 30750, loss_train: 0.345457881689, norm: 0.269893497229\n",
      "Val loss: 0.726185321808\n",
      "Iteration 31000, loss_train: 0.310673087835, norm: 0.379930913448\n",
      "Val loss: 0.807097792625\n",
      "Iteration 31250, loss_train: 0.514513909817, norm: 1.38062405586\n",
      "Val loss: 0.764355659485\n",
      "Iteration 31500, loss_train: 0.342843532562, norm: 0.381140887737\n",
      "Val loss: 0.8644785285\n",
      "Iteration 31750, loss_train: 0.28477960825, norm: 0.396276593208\n",
      "Val loss: 0.801017999649\n",
      "Iteration 32000, loss_train: 0.401012212038, norm: 0.705886900425\n",
      "Val loss: 0.856384515762\n",
      "Iteration 32250, loss_train: 0.324088186026, norm: 0.521307766438\n",
      "Val loss: 0.695900797844\n",
      "Iteration 32500, loss_train: 0.359596520662, norm: 0.304658800364\n",
      "Val loss: 0.740350365639\n",
      "Iteration 32750, loss_train: 0.343036353588, norm: 0.755283474922\n",
      "Val loss: 0.826787471771\n",
      "Iteration 33000, loss_train: 0.384755253792, norm: 0.696744024754\n",
      "Val loss: 0.800602912903\n",
      "Iteration 33250, loss_train: 0.323750853539, norm: 0.393261551857\n",
      "Val loss: 0.866368114948\n",
      "Iteration 33500, loss_train: 0.332315146923, norm: 0.394638359547\n",
      "Val loss: 0.768556892872\n",
      "Iteration 33750, loss_train: 0.327802300453, norm: 0.402601540089\n",
      "Val loss: 0.770549654961\n",
      "Iteration 34000, loss_train: 0.321850806475, norm: 0.292669683695\n",
      "Val loss: 0.737538039684\n",
      "Iteration 34250, loss_train: 0.302380263805, norm: 0.626477479935\n",
      "Val loss: 0.73249745369\n",
      "Iteration 34500, loss_train: 0.377492934465, norm: 0.844872355461\n",
      "Val loss: 0.856810748577\n",
      "Iteration 34750, loss_train: 0.339746534824, norm: 0.550834834576\n",
      "Val loss: 0.790554881096\n",
      "Iteration 35000, loss_train: 0.319161117077, norm: 0.377844184637\n",
      "Val loss: 0.849126040936\n",
      "Iteration 35250, loss_train: 0.377286732197, norm: 0.492923021317\n",
      "Val loss: 0.79597902298\n",
      "Iteration 35500, loss_train: 0.336070746183, norm: 0.495773106813\n",
      "Val loss: 0.749332010746\n",
      "Iteration 35750, loss_train: 0.3741081357, norm: 0.4632409513\n",
      "Val loss: 0.759909152985\n",
      "Iteration 36000, loss_train: 0.412788957357, norm: 0.646081387997\n",
      "Val loss: 0.768848776817\n",
      "Iteration 36250, loss_train: 0.34465944767, norm: 0.41466704011\n",
      "Val loss: 0.728634119034\n",
      "Iteration 36500, loss_train: 0.363192796707, norm: 0.35709169507\n",
      "Val loss: 0.705941736698\n",
      "Iteration 36750, loss_train: 0.391286194324, norm: 0.493064701557\n",
      "Val loss: 0.824098706245\n",
      "Iteration 37000, loss_train: 0.30665743351, norm: 0.236749872565\n",
      "Val loss: 0.847027182579\n",
      "Iteration 37250, loss_train: 0.323612242937, norm: 0.524773299694\n",
      "Val loss: 0.675302743912\n",
      "Iteration 37500, loss_train: 0.371324777603, norm: 0.278779745102\n",
      "Val loss: 0.829523324966\n",
      "Iteration 37750, loss_train: 0.328924000263, norm: 0.526220023632\n",
      "Val loss: 0.738283097744\n",
      "Iteration 38000, loss_train: 0.494052261114, norm: 2.68989014626\n",
      "Val loss: 0.761809110641\n",
      "Iteration 38250, loss_train: 0.325210899115, norm: 0.883007884026\n",
      "Val loss: 0.765859365463\n",
      "Iteration 38500, loss_train: 0.376310706139, norm: 0.870208442211\n",
      "Val loss: 0.759893000126\n",
      "Iteration 38750, loss_train: 0.330732315779, norm: 0.472989410162\n",
      "Val loss: 0.703435122967\n",
      "Iteration 39000, loss_train: 0.304245173931, norm: 0.901380062103\n",
      "Val loss: 0.791713237762\n",
      "Iteration 39250, loss_train: 0.342219084501, norm: 0.408751159906\n",
      "Val loss: 0.756663560867\n",
      "Iteration 39500, loss_train: 0.295305728912, norm: 0.461248636246\n",
      "Val loss: 0.718521773815\n",
      "Iteration 39750, loss_train: 0.314919531345, norm: 0.464592993259\n",
      "Val loss: 0.681090354919\n",
      "Iteration 40000, loss_train: 0.403193235397, norm: 0.866350352764\n",
      "Val loss: 0.747793138027\n",
      "Iteration 40250, loss_train: 0.357520490885, norm: 0.390905052423\n",
      "Val loss: 0.650558531284\n",
      "Iteration 40500, loss_train: 0.313922375441, norm: 0.395538806915\n",
      "Val loss: 0.831148207188\n",
      "Iteration 40750, loss_train: 0.341532856226, norm: 0.955071628094\n",
      "Val loss: 0.808952152729\n",
      "Iteration 41000, loss_train: 0.408323824406, norm: 0.511253654957\n",
      "Val loss: 0.715998709202\n",
      "Iteration 41250, loss_train: 0.36336222291, norm: 0.567369759083\n",
      "Val loss: 0.721014738083\n",
      "Iteration 41500, loss_train: 0.361387491226, norm: 0.310567349195\n",
      "Val loss: 0.764574587345\n",
      "Iteration 41750, loss_train: 0.313877165318, norm: 0.349136531353\n",
      "Val loss: 0.768566846848\n",
      "Iteration 42000, loss_train: 0.305377304554, norm: 0.287716925144\n",
      "Val loss: 0.758319735527\n",
      "Iteration 42250, loss_train: 0.433920383453, norm: 0.923457503319\n",
      "Val loss: 0.869844675064\n",
      "Iteration 42500, loss_train: 0.335493355989, norm: 0.865130186081\n",
      "Val loss: 0.874337732792\n",
      "Iteration 42750, loss_train: 0.365802407265, norm: 0.619785904884\n",
      "Val loss: 0.763591527939\n",
      "Iteration 43000, loss_train: 0.268536865711, norm: 0.388560324907\n",
      "Val loss: 0.753221690655\n",
      "Iteration 43250, loss_train: 0.354714095592, norm: 0.347907036543\n",
      "Val loss: 0.756554365158\n",
      "Iteration 43500, loss_train: 0.233246102929, norm: 0.349804013968\n",
      "Val loss: 0.799181640148\n",
      "Iteration 43750, loss_train: 0.30172726512, norm: 0.827488124371\n",
      "Val loss: 0.866911113262\n",
      "Iteration 44000, loss_train: 0.320935577154, norm: 0.416672050953\n",
      "Val loss: 0.635920226574\n",
      "Iteration 44250, loss_train: 0.279501974583, norm: 0.346399247646\n",
      "Val loss: 0.794007122517\n",
      "Iteration 44500, loss_train: 0.360691964626, norm: 0.400951892138\n",
      "Val loss: 0.745141744614\n",
      "Iteration 44750, loss_train: 0.284012675285, norm: 0.406629890203\n",
      "Val loss: 0.853436589241\n",
      "Iteration 45000, loss_train: 0.335229277611, norm: 0.540654957294\n",
      "Val loss: 0.777840852737\n",
      "Iteration 45250, loss_train: 0.319462895393, norm: 0.311940431595\n",
      "Val loss: 0.694441199303\n",
      "Iteration 45500, loss_train: 0.365567356348, norm: 0.648177862167\n",
      "Val loss: 0.758094668388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45750, loss_train: 0.37110888958, norm: 0.626123845577\n",
      "Val loss: 0.754658699036\n",
      "Iteration 46000, loss_train: 0.408224493265, norm: 0.993426382542\n",
      "Val loss: 0.668911337852\n",
      "Iteration 46250, loss_train: 0.415863156319, norm: 1.19144368172\n",
      "Val loss: 0.693383812904\n",
      "Iteration 46500, loss_train: 0.304676175117, norm: 0.233259916306\n",
      "Val loss: 0.823782324791\n",
      "Iteration 46750, loss_train: 0.298696041107, norm: 0.345302313566\n",
      "Val loss: 0.814334988594\n",
      "Iteration 47000, loss_train: 0.40731459856, norm: 0.560810387135\n",
      "Val loss: 0.790430903435\n",
      "Iteration 47250, loss_train: 0.377935647964, norm: 0.31066557765\n",
      "Val loss: 0.794009268284\n",
      "Iteration 47500, loss_train: 0.360856980085, norm: 0.294677108526\n",
      "Val loss: 0.780932307243\n",
      "Iteration 47750, loss_train: 0.331382542849, norm: 0.633453309536\n",
      "Val loss: 0.691099762917\n",
      "Iteration 48000, loss_train: 0.318385690451, norm: 0.739931404591\n",
      "Val loss: 0.719907164574\n",
      "Iteration 48250, loss_train: 0.355801641941, norm: 0.470222175121\n",
      "Val loss: 0.755940675735\n",
      "Iteration 48500, loss_train: 0.464926838875, norm: 0.480604559183\n",
      "Val loss: 0.64732336998\n",
      "Iteration 48750, loss_train: 0.264890193939, norm: 0.424502909184\n",
      "Val loss: 0.759420871735\n",
      "Iteration 49000, loss_train: 0.400228530169, norm: 0.540364086628\n",
      "Val loss: 0.778941690922\n",
      "Iteration 49250, loss_train: 0.456292003393, norm: 2.63878083229\n",
      "Val loss: 0.714829683304\n",
      "Iteration 49500, loss_train: 0.320469766855, norm: 0.69582849741\n",
      "Val loss: 0.79781550169\n",
      "Iteration 49750, loss_train: 0.377396851778, norm: 0.587515890598\n",
      "Val loss: 0.744029223919\n",
      "Iteration 50000, loss_train: 0.364818274975, norm: 0.606550693512\n",
      "Val loss: 0.770273387432\n",
      "Iteration 50250, loss_train: 0.372052371502, norm: 0.518023848534\n",
      "Val loss: 0.683313727379\n",
      "Iteration 50500, loss_train: 0.351125091314, norm: 0.790434360504\n",
      "Val loss: 0.835226297379\n",
      "Iteration 50750, loss_train: 0.417307794094, norm: 0.732099890709\n",
      "Val loss: 0.703580856323\n",
      "Iteration 51000, loss_train: 0.333342194557, norm: 0.384455323219\n",
      "Val loss: 0.779748141766\n",
      "Iteration 51250, loss_train: 0.31497952342, norm: 0.268618673086\n",
      "Val loss: 0.747347712517\n",
      "Iteration 51500, loss_train: 0.352115094662, norm: 0.365622103214\n",
      "Val loss: 0.799756228924\n",
      "Iteration 51750, loss_train: 0.308519423008, norm: 0.308553159237\n",
      "Val loss: 0.66392159462\n",
      "Iteration 52000, loss_train: 0.303135216236, norm: 0.397781848907\n",
      "Val loss: 0.781628608704\n",
      "Iteration 52250, loss_train: 0.341186314821, norm: 0.566701412201\n",
      "Val loss: 0.733773469925\n",
      "Iteration 52500, loss_train: 0.337100565434, norm: 0.527329921722\n",
      "Val loss: 0.782502233982\n",
      "Iteration 52750, loss_train: 0.321429014206, norm: 0.357871741056\n",
      "Val loss: 0.722251534462\n",
      "Iteration 53000, loss_train: 0.293926239014, norm: 0.720825076103\n",
      "Val loss: 0.73576849699\n",
      "Iteration 53250, loss_train: 0.482857584953, norm: 3.66409873962\n",
      "Val loss: 0.745850205421\n",
      "Iteration 53500, loss_train: 0.46787071228, norm: 3.63964152336\n",
      "Val loss: 0.741499900818\n",
      "Iteration 53750, loss_train: 0.490833967924, norm: 3.14121246338\n",
      "Val loss: 0.694644093513\n",
      "Iteration 54000, loss_train: 0.368666142225, norm: 1.34746813774\n",
      "Val loss: 0.810779511929\n",
      "Iteration 54250, loss_train: 0.300811201334, norm: 1.14725601673\n",
      "Val loss: 0.800493299961\n",
      "Iteration 54500, loss_train: 0.314545035362, norm: 0.545155704021\n",
      "Val loss: 0.679498374462\n",
      "Iteration 54750, loss_train: 0.351744681597, norm: 0.624989509583\n",
      "Val loss: 0.705768108368\n",
      "Iteration 55000, loss_train: 0.343334585428, norm: 0.278608620167\n",
      "Val loss: 0.707358121872\n",
      "Iteration 55250, loss_train: 0.327053874731, norm: 1.5817617178\n",
      "Val loss: 0.704118430614\n",
      "Iteration 55500, loss_train: 0.339091300964, norm: 0.446790575981\n",
      "Val loss: 0.79946064949\n",
      "Iteration 55750, loss_train: 0.324892252684, norm: 0.662230491638\n",
      "Val loss: 0.709184110165\n",
      "Iteration 56000, loss_train: 0.446176528931, norm: 0.638863384724\n",
      "Val loss: 0.747602820396\n",
      "Iteration 56250, loss_train: 0.338737547398, norm: 0.391219437122\n",
      "Val loss: 0.85376304388\n",
      "Iteration 56500, loss_train: 0.368075579405, norm: 0.860861003399\n",
      "Val loss: 0.746343672276\n",
      "Iteration 56750, loss_train: 0.298208534718, norm: 0.357693940401\n",
      "Val loss: 0.772061884403\n",
      "Iteration 57000, loss_train: 0.298380106688, norm: 0.384477198124\n",
      "Val loss: 0.792328119278\n",
      "Iteration 57250, loss_train: 0.30489268899, norm: 0.664721369743\n",
      "Val loss: 0.692814767361\n",
      "Iteration 57500, loss_train: 0.325091987848, norm: 0.966313898563\n",
      "Val loss: 0.750847518444\n",
      "Iteration 57750, loss_train: 0.319046527147, norm: 0.845168054104\n",
      "Val loss: 0.757898211479\n",
      "Iteration 58000, loss_train: 0.317627489567, norm: 0.64225924015\n",
      "Val loss: 0.867309808731\n",
      "Iteration 58250, loss_train: 0.500613093376, norm: 1.97050023079\n",
      "Val loss: 0.793809711933\n",
      "Iteration 58500, loss_train: 0.636075913906, norm: 6.67696046829\n",
      "Val loss: 0.766470730305\n",
      "Iteration 58750, loss_train: 0.350116074085, norm: 0.781739294529\n",
      "Val loss: 0.825242578983\n",
      "Iteration 59000, loss_train: 0.34302765131, norm: 0.87337321043\n",
      "Val loss: 0.763435482979\n",
      "Iteration 59250, loss_train: 0.304208427668, norm: 0.542022824287\n",
      "Val loss: 0.852252066135\n",
      "Iteration 59500, loss_train: 0.324683666229, norm: 0.866464734077\n",
      "Val loss: 0.907429754734\n",
      "Iteration 59750, loss_train: 0.328088611364, norm: 0.542869985104\n",
      "Val loss: 0.805704414845\n",
      "Iteration 60000, loss_train: 0.334346175194, norm: 0.724109590054\n",
      "Val loss: 0.704991281033\n",
      "Iteration 60250, loss_train: 0.33570587635, norm: 0.894120693207\n",
      "Val loss: 0.745226383209\n",
      "Iteration 60500, loss_train: 0.287873029709, norm: 0.287625461817\n",
      "Val loss: 0.684470951557\n",
      "Iteration 60750, loss_train: 0.342906475067, norm: 0.913849771023\n",
      "Val loss: 0.750407636166\n",
      "Iteration 61000, loss_train: 0.33581623435, norm: 0.909016907215\n",
      "Val loss: 0.774593770504\n",
      "Iteration 61250, loss_train: 0.321933716536, norm: 0.52093398571\n",
      "Val loss: 0.663345336914\n",
      "Iteration 61500, loss_train: 0.338747084141, norm: 1.01301217079\n",
      "Val loss: 0.735179483891\n",
      "Iteration 61750, loss_train: 0.390622109175, norm: 0.438439190388\n",
      "Val loss: 0.774202644825\n",
      "Iteration 62000, loss_train: 0.312679946423, norm: 0.757740974426\n",
      "Val loss: 0.735369503498\n",
      "Iteration 62250, loss_train: 0.375202029943, norm: 0.596858263016\n",
      "Val loss: 0.805456101894\n",
      "Iteration 62500, loss_train: 0.321641296148, norm: 0.833400666714\n",
      "Val loss: 0.790576934814\n",
      "Iteration 62750, loss_train: 0.352568745613, norm: 0.41970410943\n",
      "Val loss: 0.78177267313\n",
      "Iteration 63000, loss_train: 0.313154876232, norm: 1.22363841534\n",
      "Val loss: 0.712867617607\n",
      "Iteration 63250, loss_train: 0.427274703979, norm: 1.87986707687\n",
      "Val loss: 0.764043986797\n",
      "Iteration 63500, loss_train: 0.376020103693, norm: 2.14171934128\n",
      "Val loss: 0.716293036938\n",
      "Iteration 63750, loss_train: 0.531706869602, norm: 3.22782754898\n",
      "Val loss: 0.828643143177\n",
      "Iteration 64000, loss_train: 0.387340575457, norm: 1.83095085621\n",
      "Val loss: 0.74950760603\n",
      "Iteration 64250, loss_train: 0.463699936867, norm: 2.95039319992\n",
      "Val loss: 0.719880402088\n",
      "Iteration 64500, loss_train: 0.336074054241, norm: 0.604849457741\n",
      "Val loss: 0.730370938778\n",
      "Iteration 64750, loss_train: 0.342792659998, norm: 0.785130977631\n",
      "Val loss: 0.795158088207\n",
      "Iteration 65000, loss_train: 0.286717623472, norm: 2.0004940033\n",
      "Val loss: 0.713542640209\n",
      "Iteration 65250, loss_train: 0.286776214838, norm: 0.844222605228\n",
      "Val loss: 0.793360769749\n",
      "Iteration 65500, loss_train: 0.284176975489, norm: 0.458671241999\n",
      "Val loss: 0.778728187084\n",
      "Iteration 65750, loss_train: 0.373221755028, norm: 1.39333987236\n",
      "Val loss: 0.706015586853\n",
      "Iteration 66000, loss_train: 0.376871317625, norm: 1.58373808861\n",
      "Val loss: 0.76826184988\n",
      "Iteration 66250, loss_train: 0.359118998051, norm: 0.714936316013\n",
      "Val loss: 0.735140860081\n",
      "Iteration 66500, loss_train: 0.292906612158, norm: 1.61967611313\n",
      "Val loss: 0.738328456879\n",
      "Iteration 66750, loss_train: 0.33847618103, norm: 1.96448683739\n",
      "Val loss: 0.762736141682\n",
      "Iteration 67000, loss_train: 0.329116970301, norm: 0.522708296776\n",
      "Val loss: 0.793979465961\n",
      "Iteration 67250, loss_train: 0.384290069342, norm: 0.477575093508\n",
      "Val loss: 0.725941479206\n",
      "Iteration 67500, loss_train: 0.37335857749, norm: 0.546193659306\n",
      "Val loss: 0.750282526016\n",
      "Iteration 67750, loss_train: 0.293203651905, norm: 1.13390612602\n",
      "Val loss: 0.679059147835\n",
      "Iteration 68000, loss_train: 0.334618449211, norm: 1.57204282284\n",
      "Val loss: 0.811596751213\n",
      "Iteration 68250, loss_train: 0.369399100542, norm: 0.681284308434\n",
      "Val loss: 0.761046767235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68500, loss_train: 0.323946177959, norm: 1.1880825758\n",
      "Val loss: 0.78806167841\n",
      "Iteration 68750, loss_train: 0.318909525871, norm: 0.344785064459\n",
      "Val loss: 0.745927512646\n",
      "Iteration 69000, loss_train: 0.319739252329, norm: 0.91435700655\n",
      "Val loss: 0.736110508442\n",
      "Iteration 69250, loss_train: 0.451804757118, norm: 4.69880819321\n",
      "Val loss: 0.761819124222\n",
      "Iteration 69500, loss_train: 0.442890673876, norm: 3.99261903763\n",
      "Val loss: 0.760464191437\n",
      "Iteration 69750, loss_train: 0.394435942173, norm: 1.54993927479\n",
      "Val loss: 0.723653972149\n",
      "Iteration 70000, loss_train: 0.335575968027, norm: 2.77436304092\n",
      "Val loss: 0.723720669746\n",
      "Iteration 70250, loss_train: 0.306185692549, norm: 0.426524877548\n",
      "Val loss: 0.816475212574\n",
      "Iteration 70500, loss_train: 0.36001098156, norm: 0.78386503458\n",
      "Val loss: 0.721702754498\n",
      "Iteration 70750, loss_train: 0.298007667065, norm: 0.994714319706\n",
      "Val loss: 0.749407887459\n",
      "Iteration 71000, loss_train: 0.298131942749, norm: 0.874167740345\n",
      "Val loss: 0.755921721458\n",
      "Iteration 71250, loss_train: 0.355635911226, norm: 0.800904035568\n",
      "Val loss: 0.657544136047\n",
      "Iteration 71500, loss_train: 0.343728899956, norm: 3.7980055809\n",
      "Val loss: 0.664657592773\n",
      "Iteration 71750, loss_train: 0.322361797094, norm: 1.09272861481\n",
      "Val loss: 0.696483671665\n",
      "Iteration 72000, loss_train: 0.395927906036, norm: 0.559653460979\n",
      "Val loss: 0.734644889832\n",
      "Iteration 72250, loss_train: 0.331490457058, norm: 1.42340731621\n",
      "Val loss: 0.692861914635\n",
      "Iteration 72500, loss_train: 0.309089452028, norm: 0.548029899597\n",
      "Val loss: 0.720760941505\n",
      "Iteration 72750, loss_train: 0.37867590785, norm: 2.1103105545\n",
      "Val loss: 0.754325330257\n",
      "Iteration 73000, loss_train: 0.388906925917, norm: 0.640174031258\n",
      "Val loss: 0.793422818184\n",
      "Iteration 73250, loss_train: 0.435382664204, norm: 1.59122824669\n",
      "Val loss: 0.699294865131\n",
      "Iteration 73500, loss_train: 0.384533971548, norm: 1.05506837368\n",
      "Val loss: 0.670723438263\n",
      "Iteration 73750, loss_train: 0.302211105824, norm: 2.85601782799\n",
      "Val loss: 0.785085380077\n",
      "Iteration 74000, loss_train: 0.302174627781, norm: 3.04038572311\n",
      "Val loss: 0.770171105862\n",
      "Iteration 74250, loss_train: 0.290530532598, norm: 1.1804472208\n",
      "Val loss: 0.696276366711\n",
      "Iteration 74500, loss_train: 0.330979824066, norm: 1.2954928875\n",
      "Val loss: 0.68027818203\n",
      "Iteration 74750, loss_train: 0.401612192392, norm: 0.850667417049\n",
      "Val loss: 0.795429944992\n",
      "Iteration 75000, loss_train: 0.334474891424, norm: 1.18601942062\n",
      "Val loss: 0.768061280251\n",
      "Iteration 75250, loss_train: 0.314218431711, norm: 0.837947189808\n",
      "Val loss: 0.739219427109\n",
      "Iteration 75500, loss_train: 0.343743622303, norm: 0.37300285697\n",
      "Val loss: 0.738346815109\n",
      "Iteration 75750, loss_train: 0.276736199856, norm: 0.581742465496\n",
      "Val loss: 0.73966550827\n",
      "Iteration 76000, loss_train: 0.32024949789, norm: 1.13335716724\n",
      "Val loss: 0.703283011913\n",
      "Iteration 76250, loss_train: 0.457021683455, norm: 0.709558010101\n",
      "Val loss: 0.765620648861\n",
      "Iteration 76500, loss_train: 0.369222611189, norm: 1.20258963108\n",
      "Val loss: 0.764642417431\n",
      "Iteration 76750, loss_train: 0.367971867323, norm: 1.2492736578\n",
      "Val loss: 0.720133543015\n",
      "Iteration 77000, loss_train: 0.437997430563, norm: 1.04868042469\n",
      "Val loss: 0.68867713213\n",
      "Iteration 77250, loss_train: 0.435762733221, norm: 1.50735723972\n",
      "Val loss: 0.701004505157\n",
      "Iteration 77500, loss_train: 0.337998062372, norm: 1.8034965992\n",
      "Val loss: 0.776230990887\n",
      "Iteration 77750, loss_train: 0.343078911304, norm: 0.837191224098\n",
      "Val loss: 0.736649572849\n",
      "Iteration 78000, loss_train: 0.438671469688, norm: 1.3945633173\n",
      "Val loss: 0.847529292107\n",
      "Iteration 78250, loss_train: 0.294648140669, norm: 2.21368646622\n",
      "Val loss: 0.678643226624\n",
      "Iteration 78500, loss_train: 0.452098250389, norm: 0.922787368298\n",
      "Val loss: 0.776653826237\n",
      "Iteration 78750, loss_train: 0.357410132885, norm: 0.639854252338\n",
      "Val loss: 0.755771636963\n",
      "Iteration 79000, loss_train: 0.30158418417, norm: 1.11155557632\n",
      "Val loss: 0.763698518276\n",
      "Iteration 79250, loss_train: 0.32639580965, norm: 2.00789022446\n",
      "Val loss: 0.76005500555\n",
      "Iteration 79500, loss_train: 0.258295059204, norm: 0.801507294178\n",
      "Val loss: 0.666718006134\n",
      "Iteration 79750, loss_train: 0.362568855286, norm: 1.25306081772\n",
      "Val loss: 0.797890841961\n",
      "Iteration 80000, loss_train: 0.359655231237, norm: 0.40537366271\n",
      "Val loss: 0.738160073757\n",
      "Iteration 80250, loss_train: 0.322178453207, norm: 0.335800021887\n",
      "Val loss: 0.752094209194\n",
      "Iteration 80500, loss_train: 0.327491343021, norm: 1.05985271931\n",
      "Val loss: 0.765756309032\n",
      "Iteration 80750, loss_train: 0.297497272491, norm: 0.723853468895\n",
      "Val loss: 0.746226966381\n",
      "Iteration 81000, loss_train: 0.346399784088, norm: 1.12686753273\n",
      "Val loss: 0.78375685215\n",
      "Iteration 81250, loss_train: 0.35720705986, norm: 0.572197198868\n",
      "Val loss: 0.741369783878\n",
      "Iteration 81500, loss_train: 0.380529046059, norm: 0.648025393486\n",
      "Val loss: 0.764212667942\n",
      "Iteration 81750, loss_train: 0.33779746294, norm: 0.729606747627\n",
      "Val loss: 0.742901861668\n",
      "Iteration 82000, loss_train: 0.37872081995, norm: 0.860478341579\n",
      "Val loss: 0.731855452061\n",
      "Iteration 82250, loss_train: 0.374769359827, norm: 1.22995364666\n",
      "Val loss: 0.775332152843\n",
      "Iteration 82500, loss_train: 1.17854177952, norm: 55.1247673035\n",
      "Val loss: 0.865171611309\n",
      "Iteration 82750, loss_train: 0.589822888374, norm: 6.02604150772\n",
      "Val loss: 0.76789855957\n",
      "Iteration 83000, loss_train: 0.65502756834, norm: 5.98211050034\n",
      "Val loss: 0.899982631207\n",
      "Iteration 83250, loss_train: 0.450678884983, norm: 3.05386447906\n",
      "Val loss: 0.835717797279\n",
      "Iteration 83500, loss_train: 0.577915549278, norm: 3.85437750816\n",
      "Val loss: 0.757506847382\n",
      "Iteration 83750, loss_train: 0.446174174547, norm: 2.4603638649\n",
      "Val loss: 0.749835073948\n",
      "Iteration 84000, loss_train: 0.43657335639, norm: 2.71618008614\n",
      "Val loss: 0.784283697605\n",
      "Iteration 84250, loss_train: 0.324557244778, norm: 1.03708779812\n",
      "Val loss: 0.81772261858\n",
      "Iteration 84500, loss_train: 0.367090493441, norm: 2.66224122047\n",
      "Val loss: 0.755570173264\n",
      "Iteration 84750, loss_train: 0.329449355602, norm: 0.637533843517\n",
      "Val loss: 0.814223587513\n",
      "Iteration 85000, loss_train: 0.329062968493, norm: 0.63371014595\n",
      "Val loss: 0.850901186466\n",
      "Iteration 85250, loss_train: 0.306666910648, norm: 1.64166200161\n",
      "Val loss: 0.824894487858\n",
      "Iteration 85500, loss_train: 0.392391592264, norm: 1.56608247757\n",
      "Val loss: 0.756524562836\n",
      "Iteration 85750, loss_train: 0.337736874819, norm: 1.10564434528\n",
      "Val loss: 0.743328273296\n",
      "Iteration 86000, loss_train: 0.290289193392, norm: 3.404920578\n",
      "Val loss: 0.71843880415\n",
      "Iteration 86250, loss_train: 0.358290672302, norm: 0.641087532043\n",
      "Val loss: 0.766842901707\n",
      "Iteration 86500, loss_train: 0.346293956041, norm: 2.00724864006\n",
      "Val loss: 0.811683416367\n",
      "Iteration 86750, loss_train: 0.343222498894, norm: 1.49594008923\n",
      "Val loss: 0.700119197369\n",
      "Iteration 87000, loss_train: 0.358519732952, norm: 2.71652603149\n",
      "Val loss: 0.732121109962\n",
      "Iteration 87250, loss_train: 0.300450652838, norm: 0.818601965904\n",
      "Val loss: 0.737628996372\n",
      "Iteration 87500, loss_train: 0.378070563078, norm: 1.47675085068\n",
      "Val loss: 0.700690865517\n",
      "Iteration 87750, loss_train: 0.431050866842, norm: 1.43946611881\n",
      "Val loss: 0.677751123905\n",
      "Iteration 88000, loss_train: 0.35947778821, norm: 1.35463142395\n",
      "Val loss: 0.784571886063\n",
      "Iteration 88250, loss_train: 0.475889831781, norm: 2.97926712036\n",
      "Val loss: 0.801890552044\n",
      "Iteration 88500, loss_train: 0.388026744127, norm: 1.44002640247\n",
      "Val loss: 0.725289344788\n",
      "Iteration 88750, loss_train: 0.327052950859, norm: 0.531305551529\n",
      "Val loss: 0.844586253166\n",
      "Iteration 89000, loss_train: 0.397651106119, norm: 0.384833157063\n",
      "Val loss: 0.736998915672\n",
      "Iteration 89250, loss_train: 0.329160392284, norm: 0.58359348774\n",
      "Val loss: 0.700993061066\n",
      "Iteration 89500, loss_train: 0.356290251017, norm: 0.688413143158\n",
      "Val loss: 0.722655951977\n",
      "Iteration 89750, loss_train: 0.366989284754, norm: 6.46426534653\n",
      "Val loss: 0.837895691395\n",
      "Iteration 90000, loss_train: 0.328921049833, norm: 2.4299929142\n",
      "Val loss: 0.735642194748\n",
      "Iteration 90250, loss_train: 0.441498488188, norm: 3.16605734825\n",
      "Val loss: 0.769391357899\n",
      "Iteration 90500, loss_train: 0.298809200525, norm: 6.25209999084\n",
      "Val loss: 0.838395059109\n",
      "Iteration 90750, loss_train: 0.403911441565, norm: 0.769924819469\n",
      "Val loss: 0.933420240879\n",
      "Iteration 91000, loss_train: 0.389524817467, norm: 1.67538237572\n",
      "Val loss: 0.718209266663\n",
      "Iteration 91250, loss_train: 0.371789962053, norm: 3.59084010124\n",
      "Val loss: 0.754874646664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91500, loss_train: 0.435743242502, norm: 8.4769487381\n",
      "Val loss: 0.83804833889\n",
      "Iteration 91750, loss_train: 0.304664522409, norm: 1.57559072971\n",
      "Val loss: 0.719573974609\n",
      "Iteration 92000, loss_train: 0.387373358011, norm: 6.04626131058\n",
      "Val loss: 0.821368396282\n",
      "Iteration 92250, loss_train: 0.371883928776, norm: 3.20156478882\n",
      "Val loss: 0.876356601715\n",
      "Iteration 92500, loss_train: 0.343702346087, norm: 1.22084391117\n",
      "Val loss: 0.76218354702\n",
      "Iteration 92750, loss_train: 0.423362135887, norm: 1.29959630966\n",
      "Val loss: 0.704079687595\n",
      "Iteration 93000, loss_train: 0.407871246338, norm: 1.71606814861\n",
      "Val loss: 0.819805443287\n",
      "Iteration 93250, loss_train: 0.357493877411, norm: 0.726890683174\n",
      "Val loss: 0.719980895519\n",
      "Iteration 93500, loss_train: 0.29124262929, norm: 1.47849178314\n",
      "Val loss: 0.716050028801\n",
      "Iteration 93750, loss_train: 0.293184131384, norm: 1.65610635281\n",
      "Val loss: 0.751059651375\n",
      "Iteration 94000, loss_train: 0.380883336067, norm: 2.22458028793\n",
      "Val loss: 0.748826503754\n",
      "Iteration 94250, loss_train: 0.434381216764, norm: 3.39997768402\n",
      "Val loss: 0.747085571289\n",
      "Iteration 94500, loss_train: 0.389154583216, norm: 1.7932472229\n",
      "Val loss: 0.701668024063\n",
      "Iteration 94750, loss_train: 0.335849881172, norm: 1.36339092255\n",
      "Val loss: 0.761361896992\n",
      "Iteration 95000, loss_train: 0.320475816727, norm: 1.26906371117\n",
      "Val loss: 0.752999126911\n",
      "Iteration 95250, loss_train: 0.532688438892, norm: 1.08712053299\n",
      "Val loss: 0.755073070526\n",
      "Iteration 95500, loss_train: 0.386823445559, norm: 3.08427524567\n",
      "Val loss: 0.87309294939\n",
      "Iteration 95750, loss_train: 0.398402065039, norm: 3.70127058029\n",
      "Val loss: 0.696434378624\n",
      "Iteration 96000, loss_train: 0.44447556138, norm: 2.93599224091\n",
      "Val loss: 0.756631791592\n",
      "Iteration 96250, loss_train: 0.412757486105, norm: 3.9898917675\n",
      "Val loss: 0.758536338806\n",
      "Iteration 96500, loss_train: 0.350834429264, norm: 2.2098031044\n",
      "Val loss: 0.690641760826\n",
      "Iteration 96750, loss_train: 0.402766942978, norm: 30.4923248291\n",
      "Val loss: 0.714597404003\n",
      "Iteration 97000, loss_train: 0.354281306267, norm: 1.20086038113\n",
      "Val loss: 0.722532629967\n",
      "Iteration 97250, loss_train: 0.393630087376, norm: 1.31661772728\n",
      "Val loss: 0.787158548832\n",
      "Iteration 97500, loss_train: 0.426571160555, norm: 1.97276639938\n",
      "Val loss: 0.72523111105\n",
      "Iteration 97750, loss_train: 0.411424428225, norm: 2.05525612831\n",
      "Val loss: 0.776852667332\n",
      "Iteration 98000, loss_train: 0.291592985392, norm: 0.968362092972\n",
      "Val loss: 0.708317935467\n",
      "Iteration 98250, loss_train: 0.449022978544, norm: 2.84430909157\n",
      "Val loss: 0.789062559605\n",
      "Iteration 98500, loss_train: 0.391381561756, norm: 1.02473127842\n",
      "Val loss: 0.803619086742\n",
      "Iteration 98750, loss_train: 0.337646633387, norm: 0.418175786734\n",
      "Val loss: 0.723227739334\n",
      "Iteration 99000, loss_train: 0.332506209612, norm: 2.94711971283\n",
      "Val loss: 0.737536847591\n",
      "Iteration 99250, loss_train: 0.450602799654, norm: 2.7224881649\n",
      "Val loss: 0.67497497797\n",
      "Iteration 99500, loss_train: 0.388321220875, norm: 1.42609703541\n",
      "Val loss: 0.682779610157\n",
      "Iteration 99750, loss_train: 0.447479754686, norm: 4.76064157486\n",
      "Val loss: 0.77244412899\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100000):\n",
    "    x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(get_data_batch(sample_exams, BATCH_SIZE))\n",
    "    loss_train, norm = f_train(x_cnn, x_sentence, mask, y_sentence)\n",
    "    if not iteration % 250:\n",
    "        print('Iteration {}, loss_train: {}, norm: {}'.format(iteration, loss_train, norm))\n",
    "        try:\n",
    "            batch = get_data_batch(sample_exams, BATCH_SIZE, split='val')\n",
    "            x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(batch)\n",
    "            loss_val = f_val(x_cnn, x_sentence, mask, y_sentence)\n",
    "            print('Val loss: {}'.format(loss_val))\n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_values = lasagne.layers.get_all_param_values(l_out)\n",
    "d = {'param values': param_values,\n",
    "     'vocab': vocab,\n",
    "     'word_to_index': word_to_index,\n",
    "     'index_to_word': index_to_word,\n",
    "    }\n",
    "pickle.dump(d, open('maxpooling_trained_v_107.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TheanoLasagne]",
   "language": "python",
   "name": "conda-env-TheanoLasagne-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
