{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python27.zip',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/plat-linux2',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-tk',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-old',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-dynload',\n",
       " '/homes/ag6516/.local/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/PIL',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/homes/ag6516/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set local python and nltk paths\n",
    "import sys\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1050 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "import random\n",
    "from random import randint\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Set THEANO_FLAGS='device=cuda0,floatX=float32' to run notebook on gpu\n",
    "import lasagne\n",
    "from lasagne.utils import floatX\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from collections import Counter\n",
    "from lasagne.utils import floatX\n",
    "\n",
    "import googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = ('/vol/medic02/users/ag6516/x_ray_fracture_localisation/')\n",
    "# dir = ('/Users/Aydan/PhD/x_ray_fracture_localisation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1432\n"
     ]
    }
   ],
   "source": [
    "sample_images = pickle.load(open('sample_exams_train_test_split.pkl'))\n",
    "print len(sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = Counter()\n",
    "for item in sample_images:\n",
    "    tokens = item['tokens']\n",
    "    #for sentence in item['sentences']:\n",
    "    allwords.update(tokens)\n",
    "        \n",
    "vocab = [k for k, v in allwords.items()]\n",
    "vocab.insert(0, '#START#')\n",
    "vocab.append('#END#')\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract CNN Features from GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = googlenet.build_model()\n",
    "cnn_input_var = cnn_layers['input'].input_var\n",
    "cnn_feature_layer = cnn_layers['pool5/7x7_s1']\n",
    "cnn_output_layer = cnn_layers['prob']\n",
    "\n",
    "get_cnn_features = theano.function([cnn_input_var], lasagne.layers.get_output(cnn_feature_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_param_values = pickle.load(open('models/blvc_googlenet.pkl'))['param values']\n",
    "lasagne.layers.set_all_param_values(cnn_output_layer, model_param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN_VALUES = np.array([104, 117, 123]).reshape((3,1,1))\n",
    "\n",
    "def prep_image(im):\n",
    "    if len(im.shape) == 2:\n",
    "        im = im[:, :, np.newaxis]\n",
    "        im = np.repeat(im, 3, axis=2)\n",
    "    # Resize so smallest dim = 224, preserving aspect ratio\n",
    "    h, w, _ = im.shape\n",
    "    if h < w:\n",
    "        im = skimage.transform.resize(im, (224, w*224/h), preserve_range=True)\n",
    "    else:\n",
    "        im = skimage.transform.resize(im, (h*224/w, 224), preserve_range=True)\n",
    "\n",
    "    # Central crop to 224x224\n",
    "    h, w, _ = im.shape\n",
    "    im = im[h//2-112:h//2+112, w//2-112:w//2+112]\n",
    "    \n",
    "    rawim = np.copy(im).astype('uint8')\n",
    "    \n",
    "    # Shuffle axes to c01\n",
    "    im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "    \n",
    "    # Convert to BGR\n",
    "    im = im[::-1, :, :]\n",
    "\n",
    "    im = im - MEAN_VALUES\n",
    "    return rawim, floatX(im[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "for item in sample_images:\n",
    "    path = dir + 'data/Images/' + item['impath']\n",
    "    #try:\n",
    "    im = plt.imread(path)\n",
    "    _, cnn_input = prep_image(im)\n",
    "    #except IOError:\n",
    "    #        continue\n",
    "    features = get_cnn_features(cnn_input)\n",
    "    item['cnn features'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(sample_images, open('sample_images_baseline.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Generation Model - Single Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1432\n"
     ]
    }
   ],
   "source": [
    "sample_images = pickle.load(open('sample_images_baseline.pkl'))\n",
    "print len(sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 33\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token\n",
    "BATCH_SIZE = 10\n",
    "CNN_FEATURE_SIZE = 1024\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a list of tuples (cnn features, list of words, image ID)\n",
    "def get_data_batch(dataset, size, split='train'):\n",
    "    items = []\n",
    "    \n",
    "    while len(items) < size:\n",
    "        item = random.choice(dataset)\n",
    "        if item['split'] != split:\n",
    "            continue\n",
    "        sentence = item['tokens']\n",
    "        if len(sentence) > MAX_SENTENCE_LENGTH:\n",
    "            sentence = sentence[1:MAX_SENTENCE_LENGTH]\n",
    "            #continue\n",
    "        items.append((item['cnn features'], sentence, item['imid']))\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a list of tuples into arrays that can be fed into the network\n",
    "def prep_batch_for_network(batch):\n",
    "    x_cnn = floatX(np.zeros((len(batch), CNN_FEATURE_SIZE)))\n",
    "    x_sentence = np.zeros((len(batch), SEQUENCE_LENGTH - 1), dtype='int32')\n",
    "    y_sentence = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='int32')\n",
    "    mask = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='bool')\n",
    "\n",
    "    for j, (cnn_features, sentence, _) in enumerate(batch):\n",
    "        x_cnn[j] = cnn_features\n",
    "        i = 0\n",
    "        for word in ['#START#'] + sentence + ['#END#']:\n",
    "            if word in word_to_index:\n",
    "                mask[j, i] = True\n",
    "                y_sentence[j, i] = word_to_index[word]\n",
    "                x_sentence[j, i] = word_to_index[word]\n",
    "                i += 1\n",
    "        #mask[j, 0] = False\n",
    "                \n",
    "    return x_cnn, x_sentence, y_sentence, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence embedding maps integer sequence with dim (BATCH_SIZE, SEQUENCE_LENGTH - 1) to \n",
    "# (BATCH_SIZE, SEQUENCE_LENGTH-1, EMBEDDING_SIZE)\n",
    "l_input_sentence = lasagne.layers.InputLayer((BATCH_SIZE, SEQUENCE_LENGTH - 1))\n",
    "l_sentence_embedding = lasagne.layers.EmbeddingLayer(l_input_sentence,\n",
    "                                                     input_size=len(vocab),\n",
    "                                                     output_size=EMBEDDING_SIZE,\n",
    "                                                    )\n",
    "\n",
    "# cnn embedding changes the dimensionality of the representation from 1024 to EMBEDDING_SIZE, \n",
    "# and reshapes to add the time dimension - final dim (BATCH_SIZE, 1, EMBEDDING_SIZE)\n",
    "l_input_cnn = lasagne.layers.InputLayer((BATCH_SIZE, CNN_FEATURE_SIZE))\n",
    "l_cnn_embedding = lasagne.layers.DenseLayer(l_input_cnn, num_units=EMBEDDING_SIZE,\n",
    "                                            nonlinearity=lasagne.nonlinearities.identity)\n",
    "\n",
    "l_cnn_embedding = lasagne.layers.ReshapeLayer(l_cnn_embedding, ([0], 1, [1]))\n",
    "\n",
    "# the two are concatenated to form the RNN input with dim (BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_rnn_input = lasagne.layers.ConcatLayer([l_cnn_embedding, l_sentence_embedding])\n",
    "\n",
    "\n",
    "l_dropout_input = lasagne.layers.DropoutLayer(l_rnn_input, p=0.5)\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_dropout_input,\n",
    "                                  num_units=EMBEDDING_SIZE,\n",
    "                                  unroll_scan=True,\n",
    "                                  grad_clipping=5.)\n",
    "l_dropout_output = lasagne.layers.DropoutLayer(l_lstm, p=0.5)\n",
    "\n",
    "# the RNN output is reshaped to combine the batch and time dimensions\n",
    "# dim (BATCH_SIZE * SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_shp = lasagne.layers.ReshapeLayer(l_dropout_output, (-1, EMBEDDING_SIZE))\n",
    "\n",
    "# decoder is a fully connected layer with one output unit for each word in the vocabulary\n",
    "l_decoder = lasagne.layers.DenseLayer(l_shp, num_units=len(vocab), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# finally, the separation between batch and time dimension is restored\n",
    "l_out = lasagne.layers.ReshapeLayer(l_decoder, (BATCH_SIZE, SEQUENCE_LENGTH, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn feature vector\n",
    "x_cnn_sym = T.matrix()\n",
    "\n",
    "# sentence encoded as sequence of integer word tokens\n",
    "x_sentence_sym = T.imatrix()\n",
    "\n",
    "# mask defines which elements of the sequence should be predicted\n",
    "mask_sym = T.imatrix()\n",
    "\n",
    "# ground truth for the RNN output\n",
    "y_sentence_sym = T.imatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(l_out, {\n",
    "                l_input_sentence: x_sentence_sym,\n",
    "                l_input_cnn: x_cnn_sym\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cross_ent(net_output, mask, targets):\n",
    "    # Helper function to calculate the cross entropy error\n",
    "    preds = T.reshape(net_output, (-1, len(vocab)))\n",
    "    targets = T.flatten(targets)\n",
    "    cost = T.nnet.categorical_crossentropy(preds, targets)[T.flatten(mask).nonzero()]\n",
    "    return cost\n",
    "\n",
    "loss = T.mean(calc_cross_ent(output, mask_sym, y_sentence_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 15\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = T.grad(loss, all_params)\n",
    "all_grads = [T.clip(g, -5, 5) for g in all_grads]\n",
    "all_grads, norm = lasagne.updates.total_norm_constraint(\n",
    "    all_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=0.001)\n",
    "\n",
    "f_train = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym],\n",
    "                          [loss, norm],\n",
    "                          updates=updates\n",
    "                         )\n",
    "\n",
    "f_val = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss_train: 4.68226194382, norm: 0.594060063362\n",
      "Val loss: 4.64461946487\n",
      "Iteration 1000, loss_train: 1.49503552914, norm: 1.07000732422\n",
      "Val loss: 1.44321966171\n",
      "Iteration 2000, loss_train: 0.673087894917, norm: 1.06585860252\n",
      "Val loss: 1.14956712723\n",
      "Iteration 3000, loss_train: 0.540391325951, norm: 0.97417318821\n",
      "Val loss: 0.658056914806\n",
      "Iteration 4000, loss_train: 0.507909595966, norm: 0.824677884579\n",
      "Val loss: 0.600943505764\n",
      "Iteration 5000, loss_train: 0.527909398079, norm: 0.854552030563\n",
      "Val loss: 1.52849411964\n",
      "Iteration 6000, loss_train: 0.374013125896, norm: 0.650315344334\n",
      "Val loss: 0.858984291553\n",
      "Iteration 7000, loss_train: 0.39216119051, norm: 0.47963526845\n",
      "Val loss: 0.435785651207\n",
      "Iteration 8000, loss_train: 0.388289839029, norm: 0.533978760242\n",
      "Val loss: 0.512874007225\n",
      "Iteration 9000, loss_train: 0.383747458458, norm: 0.469817280769\n",
      "Val loss: 0.533946633339\n",
      "Iteration 10000, loss_train: 0.379906356335, norm: 0.70736438036\n",
      "Val loss: 0.418379724026\n",
      "Iteration 11000, loss_train: 0.354558438063, norm: 0.782350897789\n",
      "Val loss: 0.413664758205\n",
      "Iteration 12000, loss_train: 0.30226662755, norm: 0.481912732124\n",
      "Val loss: 1.01358067989\n",
      "Iteration 13000, loss_train: 0.430108904839, norm: 0.641174852848\n",
      "Val loss: 0.329021066427\n",
      "Iteration 14000, loss_train: 0.347759395838, norm: 0.439661592245\n",
      "Val loss: 0.418328732252\n",
      "Iteration 15000, loss_train: 0.29383957386, norm: 0.775658071041\n",
      "Val loss: 0.784994006157\n",
      "Iteration 16000, loss_train: 0.317796736956, norm: 0.83367139101\n",
      "Val loss: 0.417863428593\n",
      "Iteration 17000, loss_train: 0.341584980488, norm: 0.819212436676\n",
      "Val loss: 0.728708803654\n",
      "Iteration 18000, loss_train: 0.271331191063, norm: 0.837700128555\n",
      "Val loss: 1.45921683311\n",
      "Iteration 19000, loss_train: 0.273660928011, norm: 0.898302912712\n",
      "Val loss: 0.492799133062\n",
      "Iteration 20000, loss_train: 0.313877135515, norm: 0.421420037746\n",
      "Val loss: 0.283272236586\n",
      "Iteration 21000, loss_train: 0.310671299696, norm: 0.871128857136\n",
      "Val loss: 0.402479559183\n",
      "Iteration 22000, loss_train: 0.300974607468, norm: 0.766286790371\n",
      "Val loss: 0.367588579655\n",
      "Iteration 23000, loss_train: 0.342149078846, norm: 0.866327822208\n",
      "Val loss: 0.32545748353\n",
      "Iteration 24000, loss_train: 0.267790466547, norm: 0.330333262682\n",
      "Val loss: 0.431710988283\n",
      "Iteration 25000, loss_train: 0.240581423044, norm: 0.554103195667\n",
      "Val loss: 0.837540924549\n",
      "Iteration 26000, loss_train: 0.342228919268, norm: 0.616300821304\n",
      "Val loss: 0.417233735323\n",
      "Iteration 27000, loss_train: 0.310582756996, norm: 0.52651232481\n",
      "Val loss: 0.400703281164\n",
      "Iteration 28000, loss_train: 0.259687364101, norm: 0.434071034193\n",
      "Val loss: 0.36083060503\n",
      "Iteration 29000, loss_train: 0.285933762789, norm: 0.364111423492\n",
      "Val loss: 0.553410708904\n",
      "Iteration 30000, loss_train: 0.365267038345, norm: 0.572118639946\n",
      "Val loss: 0.368445694447\n",
      "Iteration 31000, loss_train: 0.281603097916, norm: 0.6337454319\n",
      "Val loss: 0.428343802691\n",
      "Iteration 32000, loss_train: 0.301309674978, norm: 2.53951215744\n",
      "Val loss: 0.412882804871\n",
      "Iteration 33000, loss_train: 0.275857239962, norm: 0.420734226704\n",
      "Val loss: 0.759120881557\n",
      "Iteration 34000, loss_train: 0.310906022787, norm: 0.748178720474\n",
      "Val loss: 1.04703569412\n",
      "Iteration 35000, loss_train: 0.245205059648, norm: 0.91438138485\n",
      "Val loss: 0.363147854805\n",
      "Iteration 36000, loss_train: 0.334302544594, norm: 1.24366593361\n",
      "Val loss: 1.11367809772\n",
      "Iteration 37000, loss_train: 0.336053550243, norm: 1.37172913551\n",
      "Val loss: 0.319867104292\n",
      "Iteration 38000, loss_train: 0.274574309587, norm: 0.530864059925\n",
      "Val loss: 1.04070043564\n",
      "Iteration 39000, loss_train: 0.388046145439, norm: 3.29641413689\n",
      "Val loss: 0.346113830805\n",
      "Iteration 40000, loss_train: 0.301162451506, norm: 0.741284310818\n",
      "Val loss: 0.378613591194\n",
      "Iteration 41000, loss_train: 0.271698415279, norm: 0.839841365814\n",
      "Val loss: 0.409502893686\n",
      "Iteration 42000, loss_train: 0.251729339361, norm: 0.710712790489\n",
      "Val loss: 0.420371353626\n",
      "Iteration 43000, loss_train: 0.275280058384, norm: 0.631411492825\n",
      "Val loss: 0.350753396749\n",
      "Iteration 44000, loss_train: 0.212654083967, norm: 0.919482767582\n",
      "Val loss: 0.877097189426\n",
      "Iteration 45000, loss_train: 0.340568125248, norm: 1.04690206051\n",
      "Val loss: 0.510169029236\n",
      "Iteration 46000, loss_train: 0.272979676723, norm: 0.44526553154\n",
      "Val loss: 0.442585796118\n",
      "Iteration 47000, loss_train: 0.271926611662, norm: 1.24956512451\n",
      "Val loss: 0.302954882383\n",
      "Iteration 48000, loss_train: 0.337287664413, norm: 2.26314330101\n",
      "Val loss: 1.08517062664\n",
      "Iteration 49000, loss_train: 0.343863040209, norm: 1.01332998276\n",
      "Val loss: 0.488136768341\n",
      "Iteration 50000, loss_train: 0.267899125814, norm: 0.47584348917\n",
      "Val loss: 1.11916458607\n",
      "Iteration 51000, loss_train: 0.265861868858, norm: 0.481567084789\n",
      "Val loss: 0.429863363504\n",
      "Iteration 52000, loss_train: 0.338432073593, norm: 1.84721267223\n",
      "Val loss: 0.384691178799\n",
      "Iteration 53000, loss_train: 0.282309383154, norm: 0.721842348576\n",
      "Val loss: 0.289709180593\n",
      "Iteration 54000, loss_train: 0.350064426661, norm: 3.03017425537\n",
      "Val loss: 0.791688084602\n",
      "Iteration 55000, loss_train: 0.332957565784, norm: 1.02497267723\n",
      "Val loss: 0.529767274857\n",
      "Iteration 56000, loss_train: 0.355775564909, norm: 1.26499199867\n",
      "Val loss: 0.926196277142\n",
      "Iteration 57000, loss_train: 0.330441445112, norm: 1.1920260191\n",
      "Val loss: 0.373806744814\n",
      "Iteration 58000, loss_train: 0.236395925283, norm: 1.22804808617\n",
      "Val loss: 0.442858815193\n",
      "Iteration 59000, loss_train: 0.240478545427, norm: 0.624294161797\n",
      "Val loss: 0.356667041779\n",
      "Iteration 60000, loss_train: 0.275536417961, norm: 1.29586195946\n",
      "Val loss: 0.956842899323\n",
      "Iteration 61000, loss_train: 0.274445295334, norm: 1.02965712547\n",
      "Val loss: 0.51130771637\n",
      "Iteration 62000, loss_train: 0.350644230843, norm: 1.20835828781\n",
      "Val loss: 0.397974342108\n",
      "Iteration 63000, loss_train: 0.259942471981, norm: 1.04838478565\n",
      "Val loss: 0.455383837223\n",
      "Iteration 64000, loss_train: 0.243470549583, norm: 0.594135761261\n",
      "Val loss: 0.394453018904\n",
      "Iteration 65000, loss_train: 0.288744837046, norm: 0.878684520721\n",
      "Val loss: 0.406687557697\n",
      "Iteration 66000, loss_train: 0.294972628355, norm: 0.618466258049\n",
      "Val loss: 1.32276725769\n",
      "Iteration 67000, loss_train: 0.263377815485, norm: 0.342231869698\n",
      "Val loss: 1.15605258942\n",
      "Iteration 68000, loss_train: 0.266334980726, norm: 0.98268866539\n",
      "Val loss: 0.823468804359\n",
      "Iteration 69000, loss_train: 0.26244187355, norm: 2.05063819885\n",
      "Val loss: 0.551144659519\n",
      "Iteration 70000, loss_train: 0.274546563625, norm: 1.30540347099\n",
      "Val loss: 0.427720844746\n",
      "Iteration 71000, loss_train: 0.322955459356, norm: 0.592453181744\n",
      "Val loss: 0.34510409832\n",
      "Iteration 72000, loss_train: 0.343159168959, norm: 1.33343625069\n",
      "Val loss: 0.42863214016\n",
      "Iteration 73000, loss_train: 0.268452435732, norm: 0.949341714382\n",
      "Val loss: 0.436208695173\n",
      "Iteration 74000, loss_train: 0.312108695507, norm: 1.29460906982\n",
      "Val loss: 0.531064331532\n",
      "Iteration 75000, loss_train: 0.348464548588, norm: 1.28537249565\n",
      "Val loss: 0.517112612724\n",
      "Iteration 76000, loss_train: 0.341625213623, norm: 2.47493314743\n",
      "Val loss: 0.357051551342\n",
      "Iteration 77000, loss_train: 0.310773491859, norm: 2.33602333069\n",
      "Val loss: 0.478747397661\n",
      "Iteration 78000, loss_train: 0.286838442087, norm: 5.67114543915\n",
      "Val loss: 0.460323095322\n",
      "Iteration 79000, loss_train: 0.458700090647, norm: 6.02990531921\n",
      "Val loss: 0.343302637339\n",
      "Iteration 80000, loss_train: 0.315275073051, norm: 1.05066716671\n",
      "Val loss: 0.39476493001\n",
      "Iteration 81000, loss_train: 0.250315129757, norm: 0.555705010891\n",
      "Val loss: 0.631488800049\n",
      "Iteration 82000, loss_train: 0.321000903845, norm: 1.25554275513\n",
      "Val loss: 0.355361223221\n",
      "Iteration 83000, loss_train: 0.335032194853, norm: 0.582597672939\n",
      "Val loss: 0.41915550828\n",
      "Iteration 84000, loss_train: 0.683975219727, norm: 9.49764060974\n",
      "Val loss: 0.497550100088\n",
      "Iteration 85000, loss_train: 0.365711122751, norm: 1.49999070168\n",
      "Val loss: 0.467908024788\n",
      "Iteration 86000, loss_train: 0.346689552069, norm: 3.09703564644\n",
      "Val loss: 0.497516334057\n",
      "Iteration 87000, loss_train: 0.426693230867, norm: 1.60140109062\n",
      "Val loss: 0.418310433626\n",
      "Iteration 88000, loss_train: 0.269450455904, norm: 0.988217771053\n",
      "Val loss: 0.510015904903\n",
      "Iteration 89000, loss_train: 0.368312776089, norm: 1.06433403492\n",
      "Val loss: 0.37090498209\n",
      "Iteration 90000, loss_train: 0.340658217669, norm: 5.91557598114\n",
      "Val loss: 0.938436210155\n",
      "Iteration 91000, loss_train: 0.323839128017, norm: 1.04453659058\n",
      "Val loss: 0.478514611721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 92000, loss_train: 0.320189803839, norm: 2.2265150547\n",
      "Val loss: 0.956682860851\n",
      "Iteration 93000, loss_train: 0.330505758524, norm: 2.19782519341\n",
      "Val loss: 0.547796428204\n",
      "Iteration 94000, loss_train: 0.306985139847, norm: 0.634790956974\n",
      "Val loss: 0.511658072472\n",
      "Iteration 95000, loss_train: 0.370091795921, norm: 1.72117245197\n",
      "Val loss: 0.385338544846\n",
      "Iteration 96000, loss_train: 0.279746085405, norm: 1.29769003391\n",
      "Val loss: 0.391466408968\n",
      "Iteration 97000, loss_train: 0.290550589561, norm: 1.76950073242\n",
      "Val loss: 1.27965414524\n",
      "Iteration 98000, loss_train: 0.346839666367, norm: 2.18422412872\n",
      "Val loss: 0.444494456053\n",
      "Iteration 99000, loss_train: 0.321956723928, norm: 3.51842212677\n",
      "Val loss: 1.01565861702\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100000):\n",
    "    x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(get_data_batch(sample_images, BATCH_SIZE))\n",
    "    loss_train, norm = f_train(x_cnn, x_sentence, mask, y_sentence)\n",
    "    if not iteration % 1000:\n",
    "        print('Iteration {}, loss_train: {}, norm: {}'.format(iteration, loss_train, norm))\n",
    "        try:\n",
    "            batch = get_data_batch(sample_images, BATCH_SIZE, split='val')\n",
    "            x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(batch)\n",
    "            loss_val = f_val(x_cnn, x_sentence, mask, y_sentence)\n",
    "            print('Val loss: {}'.format(loss_val))\n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_values = lasagne.layers.get_all_param_values(l_out)\n",
    "d = {'param values': param_values,\n",
    "     'vocab': vocab,\n",
    "     'word_to_index': word_to_index,\n",
    "     'index_to_word': index_to_word,\n",
    "    }\n",
    "pickle.dump(d, open('baseline_trained_v_107.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TheanoLasagne]",
   "language": "python",
   "name": "conda-env-TheanoLasagne-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
