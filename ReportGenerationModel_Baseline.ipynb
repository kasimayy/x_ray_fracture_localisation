{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python27.zip',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/plat-linux2',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-tk',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-old',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/lib-dynload',\n",
       " '/homes/ag6516/.local/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/PIL',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg',\n",
       " '/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/homes/ag6516/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set local python and nltk paths\n",
    "import sys\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg')\n",
    "sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/IPython/extensions')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 164, in <module>\n",
      "    use(config.device)\n",
      "  File \"/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 151, in use\n",
      "    init_dev(device)\n",
      "  File \"/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/theano/gpuarray/__init__.py\", line 60, in init_dev\n",
      "    sched=config.gpuarray.sched)\n",
      "  File \"pygpu/gpuarray.pyx\", line 634, in pygpu.gpuarray.init\n",
      "  File \"pygpu/gpuarray.pyx\", line 584, in pygpu.gpuarray.pygpu_init\n",
      "  File \"pygpu/gpuarray.pyx\", line 1057, in pygpu.gpuarray.GpuContext.__cinit__\n",
      "GpuArrayException: cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "import random\n",
    "from random import randint\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Set THEANO_FLAGS='device=cuda0,floatX=float32' to run notebook on gpu\n",
    "import lasagne\n",
    "from lasagne.utils import floatX\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = ('/vol/medic02/users/ag6516/x_ray_fracture_localisation/')\n",
    "# dir = ('/Users/Aydan/PhD/x_ray_fracture_localisation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "sample_exams = pickle.load(open('sample_exams_train_test_split.pkl'))\n",
    "#sample_exams = pickle.load(open('augmented_exams_bbox_features.pkl'))\n",
    "print len(sample_exams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = Counter()\n",
    "for exam in sample_exams:\n",
    "    tokens = exam['tokens']\n",
    "    #for sentence in item['sentences']:\n",
    "    allwords.update(tokens)\n",
    "        \n",
    "vocab = [k for k, v in allwords.items()]\n",
    "vocab.insert(0, '#START#')\n",
    "vocab.append('#END#')\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract CNN Features from GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_layers = googlenet.build_model()\n",
    "cnn_input_var = cnn_layers['input'].input_var\n",
    "cnn_feature_layer = cnn_layers['pool5/7x7_s1']\n",
    "cnn_output_layer = cnn_layers['prob']\n",
    "\n",
    "get_cnn_features = theano.function([cnn_input_var], lasagne.layers.get_output(cnn_feature_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_param_values = pickle.load(open('models/blvc_googlenet.pkl'))['param values']\n",
    "lasagne.layers.set_all_param_values(cnn_output_layer, model_param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_VALUES = np.array([104, 117, 123]).reshape((3,1,1))\n",
    "\n",
    "def prep_image(im):\n",
    "    if len(im.shape) == 2:\n",
    "        im = im[:, :, np.newaxis]\n",
    "        im = np.repeat(im, 3, axis=2)\n",
    "    # Resize so smallest dim = 256, preserving aspect ratio\n",
    "    h, w, _ = im.shape\n",
    "    if h < w:\n",
    "        im = skimage.transform.resize(im, (256, w*256/h), preserve_range=True)\n",
    "    else:\n",
    "        im = skimage.transform.resize(im, (h*256/w, 256), preserve_range=True)\n",
    "\n",
    "    # Random crop to 224x224\n",
    "    h_, w_, _ = im.shape\n",
    "    #print h_, w_\n",
    "    #print h_, w_\n",
    "    #im = im[h//2-112:h//2+112, w//2-112:w//2+112]\n",
    "    max_dx = h_-224-1\n",
    "    max_dy = w_-224-1\n",
    "    #print 'Max rand int: ', max_dx, max_dy\n",
    "    dx = random.randint(0, max_dx)\n",
    "    dy = random.randint(0, max_dy)\n",
    "    #print 'dx, dy: ', dx, dy\n",
    "    im = im[dx:dx+224, dy:dy+224, :]\n",
    "    #print 'Cropped shape: ', im.shape\n",
    "\n",
    "    rawim = np.copy(im).astype('uint8')\n",
    "    #plt.figure()\n",
    "    #plt.imshow(rawim)\n",
    "    # Shuffle axes to c01\n",
    "    im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "    \n",
    "    # Convert to BGR\n",
    "    im = im[::-1, :, :]\n",
    "\n",
    "    im = im - MEAN_VALUES\n",
    "    return rawim, floatX(im[np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment dataset, extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_sentences(tokens):\n",
    "    if len(tokens) > 1:\n",
    "        if tokens[0]== '.':\n",
    "            tokens.pop(0)\n",
    "        report = ' '.join(tokens)\n",
    "        #print report\n",
    "        sentences = report.split('.')\n",
    "        #print sentences\n",
    "        random.shuffle(sentences)\n",
    "        #print sentences\n",
    "        shuffled_sentences = ' . '.join(sentences)\n",
    "        shuffled_sentences = shuffled_sentences.split(' ')\n",
    "        s_sentences = [tok for tok in shuffled_sentences if tok!='']\n",
    "        if len(s_sentences) > 1:\n",
    "            if s_sentences[0]== '.':\n",
    "                s_sentences.pop(0)\n",
    "            if s_sentences[-1]!= '.':\n",
    "                s_sentences.append('.')\n",
    "    else:\n",
    "        s_sentences = tokens\n",
    "    return s_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2154\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "augmented_exams_set = pickle.load(open('augmented_images_baseline.pkl'))\n",
    "print len(augmented_exams_set)\n",
    "loaded_ids = []\n",
    "for exam in augmented_exams_set:\n",
    "    loaded_ids.append(exam['patient id'])\n",
    "loaded_ids = set(loaded_ids)\n",
    "print len(loaded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RJ109662085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "RJ109696044\n",
      "16\n",
      "RJ109727291\n",
      "24\n",
      "RJ109729878\n"
     ]
    }
   ],
   "source": [
    "loaded_ids = []\n",
    "augmented_exams_set = []\n",
    "for exam in sample_exams:\n",
    "    if exam['patient id'] not in loaded_ids:\n",
    "        print exam['patient id']\n",
    "        if exam['split'] == 'train':\n",
    "\n",
    "            new_exam0 = exam\n",
    "            new_images0 = []\n",
    "            \n",
    "            new_exam1 = exam\n",
    "            new_images1 = []\n",
    "\n",
    "            new_exam2 = exam\n",
    "            new_images2 = []\n",
    "\n",
    "            new_exam3 = exam\n",
    "            new_images3 = []\n",
    "\n",
    "            for image in exam['images']:\n",
    "                new_image0 = image\n",
    "                new_image1 = image\n",
    "                new_image2 = image\n",
    "                new_image3 = image\n",
    "\n",
    "                path = dir + 'data/Images/' + image['impath']\n",
    "                #print path\n",
    "                im = plt.imread(path)\n",
    "                \n",
    "                # Original\n",
    "                _, cnn_input0 = prep_image(im)\n",
    "                features0 = get_cnn_features(cnn_input0)\n",
    "                #print features.shape\n",
    "                new_image0['cnn features'] = features0\n",
    "                new_images0.append(new_image0)\n",
    "\n",
    "\n",
    "                # Mirror flip\n",
    "                im_flip1 = np.flip(im, 1)\n",
    "                _, cnn_input1 = prep_image(im_flip1)\n",
    "                features1 = get_cnn_features(cnn_input1)\n",
    "                #print features.shape\n",
    "                new_image1['cnn features'] = features1\n",
    "                new_images1.append(new_image1)\n",
    "\n",
    "                # Random Cropping\n",
    "                _, cnn_input2 = prep_image(im)\n",
    "                features2 = get_cnn_features(cnn_input2)\n",
    "                #print features.shape\n",
    "                new_image2['cnn features'] = features2\n",
    "                new_images2.append(new_image2)\n",
    "\n",
    "                # Mirror flip\n",
    "                im_flip3 = np.flip(im, 1)\n",
    "                _, cnn_input3 = prep_image(im_flip3)\n",
    "                features3 = get_cnn_features(cnn_input3)\n",
    "                #print features.shape\n",
    "                new_image3['cnn features'] = features3\n",
    "                new_images3.append(new_image3)\n",
    "\n",
    "            # image transformations\n",
    "            new_exam0['images'] = new_images0\n",
    "            augmented_exams_set.append(new_exam0)\n",
    "            new_exam1['images'] = new_images1\n",
    "            augmented_exams_set.append(new_exam1)\n",
    "            new_exam2['images'] = new_images2\n",
    "            augmented_exams_set.append(new_exam2)\n",
    "            new_exam3['images'] = new_images3\n",
    "            augmented_exams_set.append(new_exam3)\n",
    "\n",
    "            # sentence shuffling\n",
    "            new_exam00 = new_exam0\n",
    "            new_exam11 = new_exam1\n",
    "            new_exam22 = new_exam2\n",
    "            new_exam33 = new_exam3\n",
    "\n",
    "            new_exam00['tokens'] = shuffle_sentences(exam['tokens'])\n",
    "            augmented_exams_set.append(new_exam00)\n",
    "            new_exam11['tokens'] = shuffle_sentences(exam['tokens'])\n",
    "            augmented_exams_set.append(new_exam11)\n",
    "            new_exam22['tokens'] = shuffle_sentences(exam['tokens'])\n",
    "            augmented_exams_set.append(new_exam22)\n",
    "            new_exam33['tokens'] = shuffle_sentences(exam['tokens'])\n",
    "            augmented_exams_set.append(new_exam33)\n",
    "\n",
    "            print len(augmented_exams_set)\n",
    "        else:\n",
    "            new_exam = exam\n",
    "            new_images = []\n",
    "            \n",
    "            for image in exam['images']:\n",
    "                new_image = image\n",
    "                path = dir + 'data/Images/' + image['impath']\n",
    "                #print path\n",
    "                im = plt.imread(path)\n",
    "                _, cnn_input = prep_image(im)\n",
    "                features = get_cnn_features(cnn_input)\n",
    "                #print features.shape\n",
    "                new_image['cnn features'] = features\n",
    "                new_images.append(new_image)\n",
    "                \n",
    "            new_exam['images'] = new_images  \n",
    "            augmented_exams_set.append(exam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(augmented_exams_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(augmented_exams_set, open('augmented_images_baseline.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Generation Model - Single Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2154\n"
     ]
    }
   ],
   "source": [
    "augmented_exams = pickle.load(open('augmented_images_baseline.pkl'))\n",
    "print len(augmented_exams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate reports for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9812\n",
      "[{'cnn features': array([[ 0.55561978,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.02577852,  0.        ]], dtype=float32), 'imid': 'CR.1.3.12.2.1107.5.4.4.1104.30000015021609065289000000091', 'tokens': [u'.', u'moderately', u'severe', u'tricompartmental', u'degenerative', u'change', u'bilaterally', u'marked', u'left', u'right', u'.', u'loose', u'within', u'knee', u'joint', u'left', '.'], 'impath': 'RJ109724280/CR.1.3.12.2.1107.5.4.4.1104.30000015021609065289000000091.jpg', 'split': 'train'}]\n"
     ]
    }
   ],
   "source": [
    "augmented_images = []\n",
    "for exam in augmented_exams:\n",
    "    for image in exam['images']:\n",
    "        item = image\n",
    "        item['tokens'] = exam['tokens']\n",
    "        item['split'] = exam['split']\n",
    "        if 'cnn features' not in item.keys():\n",
    "            print exam['patient id']\n",
    "        augmented_images.append(item)\n",
    "print len(augmented_images)\n",
    "print random.sample(augmented_images,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 33\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token\n",
    "BATCH_SIZE = 20\n",
    "CNN_FEATURE_SIZE = 1024\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a list of tuples (cnn features, list of words, image ID)\n",
    "def get_data_batch(dataset, size, split='train'):\n",
    "    items = []\n",
    "    \n",
    "    while len(items) < size:\n",
    "        item = random.choice(dataset)\n",
    "        if item['split'] != split:\n",
    "            continue\n",
    "        sentence = item['tokens']\n",
    "        if len(sentence) > MAX_SENTENCE_LENGTH:\n",
    "            sentence = sentence[1:MAX_SENTENCE_LENGTH]\n",
    "            #continue\n",
    "        items.append((item['cnn features'], sentence, item['imid']))\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a list of tuples into arrays that can be fed into the network\n",
    "def prep_batch_for_network(batch):\n",
    "    x_cnn = floatX(np.zeros((len(batch), CNN_FEATURE_SIZE)))\n",
    "    x_sentence = np.zeros((len(batch), SEQUENCE_LENGTH - 1), dtype='int32')\n",
    "    y_sentence = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='int32')\n",
    "    mask = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='bool')\n",
    "\n",
    "    for j, (cnn_features, sentence, _) in enumerate(batch):\n",
    "        x_cnn[j] = cnn_features\n",
    "        i = 0\n",
    "        for word in ['#START#'] + sentence + ['#END#']:\n",
    "            if word in word_to_index:\n",
    "                mask[j, i] = True\n",
    "                y_sentence[j, i] = word_to_index[word]\n",
    "                x_sentence[j, i] = word_to_index[word]\n",
    "                i += 1\n",
    "        #mask[j, 0] = False\n",
    "                \n",
    "    return x_cnn, x_sentence, y_sentence, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence embedding maps integer sequence with dim (BATCH_SIZE, SEQUENCE_LENGTH - 1) to \n",
    "# (BATCH_SIZE, SEQUENCE_LENGTH-1, EMBEDDING_SIZE)\n",
    "l_input_sentence = lasagne.layers.InputLayer((BATCH_SIZE, SEQUENCE_LENGTH - 1))\n",
    "l_sentence_embedding = lasagne.layers.EmbeddingLayer(l_input_sentence,\n",
    "                                                     input_size=len(vocab),\n",
    "                                                     output_size=EMBEDDING_SIZE,\n",
    "                                                    )\n",
    "\n",
    "# cnn embedding changes the dimensionality of the representation from 1024 to EMBEDDING_SIZE, \n",
    "# and reshapes to add the time dimension - final dim (BATCH_SIZE, 1, EMBEDDING_SIZE)\n",
    "l_input_cnn = lasagne.layers.InputLayer((BATCH_SIZE, CNN_FEATURE_SIZE))\n",
    "l_cnn_embedding = lasagne.layers.DenseLayer(l_input_cnn, num_units=EMBEDDING_SIZE,\n",
    "                                            nonlinearity=lasagne.nonlinearities.identity)\n",
    "\n",
    "l_cnn_embedding = lasagne.layers.ReshapeLayer(l_cnn_embedding, ([0], 1, [1]))\n",
    "\n",
    "# the two are concatenated to form the RNN input with dim (BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_rnn_input = lasagne.layers.ConcatLayer([l_cnn_embedding, l_sentence_embedding])\n",
    "\n",
    "\n",
    "l_dropout_input = lasagne.layers.DropoutLayer(l_rnn_input, p=0.5)\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_dropout_input,\n",
    "                                  num_units=EMBEDDING_SIZE,\n",
    "                                  unroll_scan=True,\n",
    "                                  grad_clipping=5.)\n",
    "l_dropout_output = lasagne.layers.DropoutLayer(l_lstm, p=0.5)\n",
    "\n",
    "# the RNN output is reshaped to combine the batch and time dimensions\n",
    "# dim (BATCH_SIZE * SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_shp = lasagne.layers.ReshapeLayer(l_dropout_output, (-1, EMBEDDING_SIZE))\n",
    "\n",
    "# decoder is a fully connected layer with one output unit for each word in the vocabulary\n",
    "l_decoder = lasagne.layers.DenseLayer(l_shp, num_units=len(vocab), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# finally, the separation between batch and time dimension is restored\n",
    "l_out = lasagne.layers.ReshapeLayer(l_decoder, (BATCH_SIZE, SEQUENCE_LENGTH, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn feature vector\n",
    "x_cnn_sym = T.matrix()\n",
    "\n",
    "# sentence encoded as sequence of integer word tokens\n",
    "x_sentence_sym = T.imatrix()\n",
    "\n",
    "# mask defines which elements of the sequence should be predicted\n",
    "mask_sym = T.imatrix()\n",
    "\n",
    "# ground truth for the RNN output\n",
    "y_sentence_sym = T.imatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(l_out, {\n",
    "                l_input_sentence: x_sentence_sym,\n",
    "                l_input_cnn: x_cnn_sym\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cross_ent(net_output, mask, targets):\n",
    "    # Helper function to calculate the cross entropy error\n",
    "    preds = T.reshape(net_output, (-1, len(vocab)))\n",
    "    targets = T.flatten(targets)\n",
    "    cost = T.nnet.categorical_crossentropy(preds, targets)[T.flatten(mask).nonzero()]\n",
    "    return cost\n",
    "\n",
    "loss = T.mean(calc_cross_ent(output, mask_sym, y_sentence_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 15\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = T.grad(loss, all_params)\n",
    "all_grads = [T.clip(g, -5, 5) for g in all_grads]\n",
    "all_grads, norm = lasagne.updates.total_norm_constraint(\n",
    "    all_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=0.00001)\n",
    "\n",
    "f_train = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym],\n",
    "                          [loss, norm],\n",
    "                          updates=updates\n",
    "                         )\n",
    "\n",
    "f_val = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss_train: 5.12412261963, norm: 0.46596300602\n",
      "Val loss: 5.12394857407\n",
      "Iteration 250, loss_train: 4.92777585983, norm: 0.769301533699\n",
      "Val loss: 4.89264774323\n",
      "Iteration 500, loss_train: 4.26689529419, norm: 0.835332930088\n",
      "Val loss: 4.4990439415\n",
      "Iteration 750, loss_train: 4.24816560745, norm: 0.607361674309\n",
      "Val loss: 4.37673950195\n",
      "Iteration 1000, loss_train: 4.07771539688, norm: 0.605389893055\n",
      "Val loss: 4.22698020935\n",
      "Iteration 1250, loss_train: 3.99026179314, norm: 0.527254343033\n",
      "Val loss: 4.27441883087\n",
      "Iteration 1500, loss_train: 4.04906892776, norm: 0.503723561764\n",
      "Val loss: 4.13545131683\n",
      "Iteration 1750, loss_train: 3.94799566269, norm: 0.530963182449\n",
      "Val loss: 3.97902870178\n",
      "Iteration 2000, loss_train: 3.98307204247, norm: 0.561355173588\n",
      "Val loss: 3.9612429142\n",
      "Iteration 2250, loss_train: 3.95651817322, norm: 0.526784241199\n",
      "Val loss: 4.01541805267\n",
      "Iteration 2500, loss_train: 3.84807133675, norm: 0.502494096756\n",
      "Val loss: 3.93274569511\n",
      "Iteration 2750, loss_train: 3.85119819641, norm: 0.482685297728\n",
      "Val loss: 3.82062101364\n",
      "Iteration 3000, loss_train: 3.86830615997, norm: 0.576238036156\n",
      "Val loss: 3.98017382622\n",
      "Iteration 3250, loss_train: 3.82820820808, norm: 0.776203393936\n",
      "Val loss: 4.00292682648\n",
      "Iteration 3500, loss_train: 3.78647422791, norm: 0.917145133018\n",
      "Val loss: 3.97019028664\n",
      "Iteration 3750, loss_train: 3.82604837418, norm: 0.702312290668\n",
      "Val loss: 3.77103042603\n",
      "Iteration 4000, loss_train: 3.83406329155, norm: 1.50677716732\n",
      "Val loss: 3.9390039444\n",
      "Iteration 4250, loss_train: 3.77063179016, norm: 1.52914357185\n",
      "Val loss: 3.8837659359\n",
      "Iteration 4500, loss_train: 3.70078349113, norm: 0.99444937706\n",
      "Val loss: 3.87922239304\n",
      "Iteration 4750, loss_train: 3.88356590271, norm: 0.901865184307\n",
      "Val loss: 4.00394201279\n",
      "Iteration 5000, loss_train: 3.70193529129, norm: 0.710703969002\n",
      "Val loss: 3.89532160759\n",
      "Iteration 5250, loss_train: 3.65106463432, norm: 1.14055883884\n",
      "Val loss: 3.88041996956\n",
      "Iteration 5500, loss_train: 3.61184191704, norm: 0.689168512821\n",
      "Val loss: 3.8430056572\n",
      "Iteration 5750, loss_train: 3.73596286774, norm: 1.29723453522\n",
      "Val loss: 3.94201374054\n",
      "Iteration 6000, loss_train: 3.68240118027, norm: 1.08725321293\n",
      "Val loss: 3.85963654518\n",
      "Iteration 6250, loss_train: 3.64239096642, norm: 0.963762164116\n",
      "Val loss: 3.90529465675\n",
      "Iteration 6500, loss_train: 3.69095349312, norm: 1.62062609196\n",
      "Val loss: 3.80788993835\n",
      "Iteration 6750, loss_train: 3.64351987839, norm: 1.51034045219\n",
      "Val loss: 4.00023794174\n",
      "Iteration 7000, loss_train: 3.61573076248, norm: 1.45938515663\n",
      "Val loss: 3.93789935112\n",
      "Iteration 7250, loss_train: 3.70006752014, norm: 0.894997537136\n",
      "Val loss: 3.70843935013\n",
      "Iteration 7500, loss_train: 3.60643720627, norm: 0.798661768436\n",
      "Val loss: 3.63933563232\n",
      "Iteration 7750, loss_train: 3.55455374718, norm: 1.69997739792\n",
      "Val loss: 3.71497321129\n",
      "Iteration 8000, loss_train: 3.59412503242, norm: 1.4478520155\n",
      "Val loss: 3.75405907631\n",
      "Iteration 8250, loss_train: 3.48359107971, norm: 2.25391602516\n",
      "Val loss: 3.70358037949\n",
      "Iteration 8500, loss_train: 3.51819086075, norm: 4.34786081314\n",
      "Val loss: 3.82609629631\n",
      "Iteration 8750, loss_train: 3.49857282639, norm: 1.35514044762\n",
      "Val loss: 3.91760158539\n",
      "Iteration 9000, loss_train: 3.49884915352, norm: 1.3321903944\n",
      "Val loss: 3.66622138023\n",
      "Iteration 9250, loss_train: 3.45731902122, norm: 2.23137283325\n",
      "Val loss: 3.59585285187\n",
      "Iteration 9500, loss_train: 3.36098861694, norm: 2.53379225731\n",
      "Val loss: 3.82421183586\n",
      "Iteration 9750, loss_train: 3.52289295197, norm: 1.35175347328\n",
      "Val loss: 3.61075234413\n",
      "Iteration 10000, loss_train: 3.33688473701, norm: 1.79007565975\n",
      "Val loss: 3.77038574219\n",
      "Iteration 10250, loss_train: 3.43602395058, norm: 1.02190721035\n",
      "Val loss: 3.94681763649\n",
      "Iteration 10500, loss_train: 3.3858268261, norm: 1.7886762619\n",
      "Val loss: 3.72491884232\n",
      "Iteration 10750, loss_train: 3.37507843971, norm: 1.10398864746\n",
      "Val loss: 3.62551736832\n",
      "Iteration 11000, loss_train: 3.35463142395, norm: 1.73598432541\n",
      "Val loss: 3.72198033333\n",
      "Iteration 11250, loss_train: 3.39991950989, norm: 1.46100234985\n",
      "Val loss: 3.82231068611\n",
      "Iteration 11500, loss_train: 3.31392002106, norm: 1.59240615368\n",
      "Val loss: 3.69543766975\n",
      "Iteration 11750, loss_train: 3.29251050949, norm: 1.28772974014\n",
      "Val loss: 3.60425257683\n",
      "Iteration 12000, loss_train: 3.21148085594, norm: 2.86426091194\n",
      "Val loss: 3.50457906723\n",
      "Iteration 12250, loss_train: 3.43526864052, norm: 3.33818507195\n",
      "Val loss: 3.49917459488\n",
      "Iteration 12500, loss_train: 3.23567914963, norm: 1.62994277477\n",
      "Val loss: 3.54716730118\n",
      "Iteration 12750, loss_train: 3.40468335152, norm: 1.59692466259\n",
      "Val loss: 3.46202611923\n",
      "Iteration 13000, loss_train: 3.1468334198, norm: 2.83798122406\n",
      "Val loss: 3.71758246422\n",
      "Iteration 13250, loss_train: 3.07771444321, norm: 1.23658072948\n",
      "Val loss: 3.9167535305\n",
      "Iteration 13500, loss_train: 3.26981186867, norm: 1.69243276119\n",
      "Val loss: 3.50106310844\n",
      "Iteration 13750, loss_train: 3.14477539062, norm: 1.71139109135\n",
      "Val loss: 3.52303624153\n",
      "Iteration 14000, loss_train: 3.1365442276, norm: 2.69094872475\n",
      "Val loss: 3.69282317162\n",
      "Iteration 14250, loss_train: 3.20133757591, norm: 1.26589107513\n",
      "Val loss: 3.56519579887\n",
      "Iteration 14500, loss_train: 3.08302164078, norm: 2.56873583794\n",
      "Val loss: 3.33561372757\n",
      "Iteration 14750, loss_train: 3.14117455482, norm: 1.5947766304\n",
      "Val loss: 3.32348275185\n",
      "Iteration 15000, loss_train: 3.16681265831, norm: 3.09838247299\n",
      "Val loss: 3.63260960579\n",
      "Iteration 15250, loss_train: 2.96721363068, norm: 1.34552645683\n",
      "Val loss: 3.20644140244\n",
      "Iteration 15500, loss_train: 2.9638645649, norm: 1.27279520035\n",
      "Val loss: 3.3537800312\n",
      "Iteration 15750, loss_train: 3.06344103813, norm: 2.1283864975\n",
      "Val loss: 3.43017077446\n",
      "Iteration 16000, loss_train: 2.96270012856, norm: 1.37900936604\n",
      "Val loss: 3.50606870651\n",
      "Iteration 16250, loss_train: 2.96642541885, norm: 1.50923931599\n",
      "Val loss: 3.60757017136\n",
      "Iteration 16500, loss_train: 2.957675457, norm: 1.10133945942\n",
      "Val loss: 3.34838414192\n",
      "Iteration 16750, loss_train: 2.93150186539, norm: 1.90864109993\n",
      "Val loss: 3.51444244385\n",
      "Iteration 17000, loss_train: 2.98117375374, norm: 1.801471591\n",
      "Val loss: 3.43491983414\n",
      "Iteration 17250, loss_train: 2.83797383308, norm: 2.47623968124\n",
      "Val loss: 3.23528194427\n",
      "Iteration 17500, loss_train: 2.8745265007, norm: 1.39452111721\n",
      "Val loss: 3.41886806488\n",
      "Iteration 17750, loss_train: 3.05206203461, norm: 1.18688035011\n",
      "Val loss: 3.36530661583\n",
      "Iteration 18000, loss_train: 2.81732034683, norm: 1.43384587765\n",
      "Val loss: 3.29099941254\n",
      "Iteration 18250, loss_train: 2.98322319984, norm: 1.84056723118\n",
      "Val loss: 3.27763748169\n",
      "Iteration 18500, loss_train: 3.0700199604, norm: 2.06224918365\n",
      "Val loss: 3.36306285858\n",
      "Iteration 18750, loss_train: 2.86698317528, norm: 1.93982684612\n",
      "Val loss: 3.39884781837\n",
      "Iteration 19000, loss_train: 2.78205728531, norm: 1.55644178391\n",
      "Val loss: 3.41367697716\n",
      "Iteration 19250, loss_train: 2.82774949074, norm: 1.20862662792\n",
      "Val loss: 3.56754422188\n",
      "Iteration 19500, loss_train: 2.85917806625, norm: 1.53110182285\n",
      "Val loss: 3.3271651268\n",
      "Iteration 19750, loss_train: 2.83810138702, norm: 1.42179369926\n",
      "Val loss: 3.5442302227\n",
      "Iteration 20000, loss_train: 2.71048545837, norm: 1.64344143867\n",
      "Val loss: 3.3636534214\n",
      "Iteration 20250, loss_train: 2.8464152813, norm: 2.06574749947\n",
      "Val loss: 3.40213465691\n",
      "Iteration 20500, loss_train: 2.85269284248, norm: 1.57430887222\n",
      "Val loss: 3.34241318703\n",
      "Iteration 20750, loss_train: 2.6959695816, norm: 2.25600481033\n",
      "Val loss: 3.29648995399\n",
      "Iteration 21000, loss_train: 2.83887338638, norm: 1.74505317211\n",
      "Val loss: 3.44395065308\n",
      "Iteration 21250, loss_train: 2.78206419945, norm: 2.0401391983\n",
      "Val loss: 3.31333661079\n",
      "Iteration 21500, loss_train: 2.80904459953, norm: 1.51751124859\n",
      "Val loss: 3.46062397957\n",
      "Iteration 21750, loss_train: 2.85181689262, norm: 1.46797692776\n",
      "Val loss: 3.20626616478\n",
      "Iteration 22000, loss_train: 2.78436422348, norm: 1.11497044563\n",
      "Val loss: 3.28729391098\n",
      "Iteration 22250, loss_train: 2.82925724983, norm: 1.86233055592\n",
      "Val loss: 3.28450775146\n",
      "Iteration 22500, loss_train: 2.69995999336, norm: 1.53486800194\n",
      "Val loss: 3.36468529701\n",
      "Iteration 22750, loss_train: 2.81430673599, norm: 1.48004734516\n",
      "Val loss: 3.4077963829\n",
      "Iteration 23000, loss_train: 2.73256969452, norm: 1.46903026104\n",
      "Val loss: 3.43894124031\n",
      "Iteration 23250, loss_train: 2.79712438583, norm: 1.67245018482\n",
      "Val loss: 3.28347992897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23500, loss_train: 2.65410327911, norm: 2.10554003716\n",
      "Val loss: 3.36878228188\n",
      "Iteration 23750, loss_train: 2.56280064583, norm: 2.0782957077\n",
      "Val loss: 3.45648026466\n",
      "Iteration 24000, loss_train: 2.72524476051, norm: 2.10793423653\n",
      "Val loss: 3.08804130554\n",
      "Iteration 24250, loss_train: 2.56240940094, norm: 2.4070084095\n",
      "Val loss: 3.25433349609\n",
      "Iteration 24500, loss_train: 2.68536806107, norm: 2.4822883606\n",
      "Val loss: 3.55413126945\n",
      "Iteration 24750, loss_train: 2.48333764076, norm: 1.68692934513\n",
      "Val loss: 3.24612569809\n",
      "Iteration 25000, loss_train: 2.66186547279, norm: 1.58483219147\n",
      "Val loss: 3.24375128746\n",
      "Iteration 25250, loss_train: 2.46181941032, norm: 1.7790210247\n",
      "Val loss: 3.18561530113\n",
      "Iteration 25500, loss_train: 2.51687836647, norm: 1.95052230358\n",
      "Val loss: 3.25824522972\n",
      "Iteration 25750, loss_train: 2.51445817947, norm: 1.31469261646\n",
      "Val loss: 3.05422019958\n",
      "Iteration 26000, loss_train: 2.63461875916, norm: 1.73013651371\n",
      "Val loss: 3.15489339828\n",
      "Iteration 26250, loss_train: 2.68597793579, norm: 1.41248857975\n",
      "Val loss: 3.05244278908\n",
      "Iteration 26500, loss_train: 2.51415657997, norm: 1.80050468445\n",
      "Val loss: 3.26776099205\n",
      "Iteration 26750, loss_train: 2.51211357117, norm: 1.44144797325\n",
      "Val loss: 3.09805083275\n",
      "Iteration 27000, loss_train: 2.54384183884, norm: 4.31438684464\n",
      "Val loss: 3.26615977287\n",
      "Iteration 27250, loss_train: 2.32940649986, norm: 1.71352183819\n",
      "Val loss: 3.15076851845\n",
      "Iteration 27500, loss_train: 2.6503367424, norm: 1.79278600216\n",
      "Val loss: 3.16216182709\n",
      "Iteration 27750, loss_train: 2.60464596748, norm: 1.81630897522\n",
      "Val loss: 3.2249186039\n",
      "Iteration 28000, loss_train: 2.61288285255, norm: 1.65731310844\n",
      "Val loss: 3.24383568764\n",
      "Iteration 28250, loss_train: 2.48613905907, norm: 1.38188755512\n",
      "Val loss: 3.30168652534\n",
      "Iteration 28500, loss_train: 2.46507287025, norm: 1.8884691\n",
      "Val loss: 3.54379940033\n",
      "Iteration 28750, loss_train: 2.53637266159, norm: 2.035384655\n",
      "Val loss: 3.0087685585\n",
      "Iteration 29000, loss_train: 2.48329854012, norm: 1.62290811539\n",
      "Val loss: 3.15522313118\n",
      "Iteration 29250, loss_train: 2.53555059433, norm: 1.43421959877\n",
      "Val loss: 3.20495700836\n",
      "Iteration 29500, loss_train: 2.31721019745, norm: 1.83141648769\n",
      "Val loss: 3.07543325424\n",
      "Iteration 29750, loss_train: 2.46316838264, norm: 2.14756155014\n",
      "Val loss: 2.80547094345\n",
      "Iteration 30000, loss_train: 2.4933578968, norm: 2.35920357704\n",
      "Val loss: 3.09396195412\n",
      "Iteration 30250, loss_train: 2.4938185215, norm: 1.61426353455\n",
      "Val loss: 3.09698295593\n",
      "Iteration 30500, loss_train: 2.32149887085, norm: 1.33958411217\n",
      "Val loss: 3.15332150459\n",
      "Iteration 30750, loss_train: 2.43598175049, norm: 2.20968198776\n",
      "Val loss: 3.2407553196\n",
      "Iteration 31000, loss_train: 2.50208497047, norm: 1.97272443771\n",
      "Val loss: 3.44769620895\n",
      "Iteration 31250, loss_train: 2.38661360741, norm: 1.65909063816\n",
      "Val loss: 2.89582896233\n",
      "Iteration 31500, loss_train: 2.33489155769, norm: 2.15717673302\n",
      "Val loss: 3.04681277275\n",
      "Iteration 31750, loss_train: 2.48029327393, norm: 1.81840085983\n",
      "Val loss: 3.22320723534\n",
      "Iteration 32000, loss_train: 2.55852437019, norm: 1.77849447727\n",
      "Val loss: 3.22035336494\n",
      "Iteration 32250, loss_train: 2.24660992622, norm: 1.73363804817\n",
      "Val loss: 3.22600984573\n",
      "Iteration 32500, loss_train: 2.31130814552, norm: 2.08311462402\n",
      "Val loss: 3.5208029747\n",
      "Iteration 32750, loss_train: 2.3975777626, norm: 1.76162552834\n",
      "Val loss: 2.89328217506\n",
      "Iteration 33000, loss_train: 2.3935213089, norm: 2.94484114647\n",
      "Val loss: 3.29074788094\n",
      "Iteration 33250, loss_train: 2.42429184914, norm: 1.85224974155\n",
      "Val loss: 3.25144767761\n",
      "Iteration 33500, loss_train: 2.40429472923, norm: 2.22529315948\n",
      "Val loss: 3.14461112022\n",
      "Iteration 33750, loss_train: 2.48290610313, norm: 1.6875897646\n",
      "Val loss: 3.01738905907\n",
      "Iteration 34000, loss_train: 2.55645036697, norm: 1.62386155128\n",
      "Val loss: 3.00258016586\n",
      "Iteration 34250, loss_train: 2.28416442871, norm: 1.55996119976\n",
      "Val loss: 2.94753932953\n",
      "Iteration 34500, loss_train: 2.45387578011, norm: 2.08722352982\n",
      "Val loss: 2.92206978798\n",
      "Iteration 34750, loss_train: 2.18712377548, norm: 1.64735007286\n",
      "Val loss: 2.98616123199\n",
      "Iteration 35000, loss_train: 2.25628232956, norm: 3.83743476868\n",
      "Val loss: 3.15429568291\n",
      "Iteration 35250, loss_train: 2.19911336899, norm: 1.78450238705\n",
      "Val loss: 3.27461981773\n",
      "Iteration 35500, loss_train: 2.10420536995, norm: 1.67138791084\n",
      "Val loss: 3.08459210396\n",
      "Iteration 35750, loss_train: 2.27790880203, norm: 1.83185112476\n",
      "Val loss: 3.36414313316\n",
      "Iteration 36000, loss_train: 2.18070578575, norm: 1.5469019413\n",
      "Val loss: 3.28562641144\n",
      "Iteration 36250, loss_train: 2.28264212608, norm: 2.65683341026\n",
      "Val loss: 2.78441309929\n",
      "Iteration 36500, loss_train: 2.22454380989, norm: 2.44508957863\n",
      "Val loss: 2.74293279648\n",
      "Iteration 36750, loss_train: 2.18320560455, norm: 1.42663824558\n",
      "Val loss: 3.29199743271\n",
      "Iteration 37000, loss_train: 2.08183217049, norm: 1.48411858082\n",
      "Val loss: 3.37101602554\n",
      "Iteration 37250, loss_train: 2.18929171562, norm: 1.91454613209\n",
      "Val loss: 3.25327014923\n",
      "Iteration 37500, loss_train: 2.17179465294, norm: 1.62707161903\n",
      "Val loss: 3.26849555969\n",
      "Iteration 37750, loss_train: 2.32241725922, norm: 2.21832537651\n",
      "Val loss: 3.24645090103\n",
      "Iteration 38000, loss_train: 2.38130378723, norm: 2.511272192\n",
      "Val loss: 3.25499391556\n",
      "Iteration 38250, loss_train: 2.38462209702, norm: 1.8518422842\n",
      "Val loss: 2.96556282043\n",
      "Iteration 38500, loss_train: 2.29678058624, norm: 2.32529950142\n",
      "Val loss: 3.21163320541\n",
      "Iteration 38750, loss_train: 2.33990764618, norm: 1.64882946014\n",
      "Val loss: 3.26324224472\n",
      "Iteration 39000, loss_train: 2.16012883186, norm: 2.01866531372\n",
      "Val loss: 2.99935317039\n",
      "Iteration 39250, loss_train: 2.19214081764, norm: 1.50246572495\n",
      "Val loss: 3.02210831642\n",
      "Iteration 39500, loss_train: 2.03836464882, norm: 1.74600124359\n",
      "Val loss: 3.35616540909\n",
      "Iteration 39750, loss_train: 2.13740968704, norm: 2.25697398186\n",
      "Val loss: 3.03540420532\n",
      "Iteration 40000, loss_train: 2.11623430252, norm: 1.74643087387\n",
      "Val loss: 3.37091255188\n",
      "Iteration 40250, loss_train: 2.17783236504, norm: 1.92991447449\n",
      "Val loss: 3.07344150543\n",
      "Iteration 40500, loss_train: 1.98417627811, norm: 1.8976111412\n",
      "Val loss: 3.28471827507\n",
      "Iteration 40750, loss_train: 2.21478796005, norm: 2.36013031006\n",
      "Val loss: 3.23557806015\n",
      "Iteration 41000, loss_train: 1.99776363373, norm: 1.98192477226\n",
      "Val loss: 3.3233089447\n",
      "Iteration 41250, loss_train: 2.03873181343, norm: 1.49507665634\n",
      "Val loss: 3.15738582611\n",
      "Iteration 41500, loss_train: 2.07688355446, norm: 1.787561059\n",
      "Val loss: 3.11713528633\n",
      "Iteration 41750, loss_train: 2.07014775276, norm: 1.68254375458\n",
      "Val loss: 3.04662847519\n",
      "Iteration 42000, loss_train: 2.10597038269, norm: 1.53074228764\n",
      "Val loss: 2.99784111977\n",
      "Iteration 42250, loss_train: 2.21040678024, norm: 1.78325557709\n",
      "Val loss: 3.53566002846\n",
      "Iteration 42500, loss_train: 2.02136182785, norm: 1.6358538866\n",
      "Val loss: 3.00176429749\n",
      "Iteration 42750, loss_train: 2.07125759125, norm: 1.99417984486\n",
      "Val loss: 3.09033060074\n",
      "Iteration 43000, loss_train: 2.07640862465, norm: 2.16836571693\n",
      "Val loss: 3.08052682877\n",
      "Iteration 43250, loss_train: 1.98435950279, norm: 1.92699193954\n",
      "Val loss: 3.18352603912\n",
      "Iteration 43500, loss_train: 2.0091176033, norm: 1.68627631664\n",
      "Val loss: 3.30915474892\n",
      "Iteration 43750, loss_train: 2.14878845215, norm: 1.69002151489\n",
      "Val loss: 3.34338140488\n",
      "Iteration 44000, loss_train: 2.21687364578, norm: 2.01821327209\n",
      "Val loss: 3.3733716011\n",
      "Iteration 44250, loss_train: 2.06673645973, norm: 2.03675627708\n",
      "Val loss: 3.03330564499\n",
      "Iteration 44500, loss_train: 2.11824297905, norm: 2.18637990952\n",
      "Val loss: 3.53454995155\n",
      "Iteration 44750, loss_train: 2.10644459724, norm: 1.76542234421\n",
      "Val loss: 3.28316235542\n",
      "Iteration 45000, loss_train: 1.88530921936, norm: 2.02620816231\n",
      "Val loss: 2.81810450554\n",
      "Iteration 45250, loss_train: 1.94296038151, norm: 2.09249687195\n",
      "Val loss: 2.88254261017\n",
      "Iteration 45500, loss_train: 2.06008934975, norm: 2.1076374054\n",
      "Val loss: 3.19388222694\n",
      "Iteration 45750, loss_train: 2.01088356972, norm: 1.9350618124\n",
      "Val loss: 3.0126543045\n",
      "Iteration 46000, loss_train: 2.08356618881, norm: 2.13290476799\n",
      "Val loss: 3.39181208611\n",
      "Iteration 46250, loss_train: 2.01924204826, norm: 2.25182032585\n",
      "Val loss: 2.99174785614\n",
      "Iteration 46500, loss_train: 2.01302814484, norm: 2.34751629829\n",
      "Val loss: 3.102820158\n",
      "Iteration 46750, loss_train: 1.82636070251, norm: 2.01642680168\n",
      "Val loss: 3.00598239899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47000, loss_train: 1.95230472088, norm: 2.50723147392\n",
      "Val loss: 3.10752248764\n",
      "Iteration 47250, loss_train: 2.05961847305, norm: 1.7025103569\n",
      "Val loss: 3.47669529915\n",
      "Iteration 47500, loss_train: 1.96740674973, norm: 1.65140354633\n",
      "Val loss: 3.04024887085\n",
      "Iteration 47750, loss_train: 2.09920716286, norm: 2.18202257156\n",
      "Val loss: 3.06809091568\n",
      "Iteration 48000, loss_train: 1.887134552, norm: 2.40102481842\n",
      "Val loss: 3.03737330437\n",
      "Iteration 48250, loss_train: 2.05409431458, norm: 1.6500825882\n",
      "Val loss: 2.86211395264\n",
      "Iteration 48500, loss_train: 1.86089587212, norm: 1.94330954552\n",
      "Val loss: 2.95741629601\n",
      "Iteration 48750, loss_train: 1.88908326626, norm: 2.01504135132\n",
      "Val loss: 3.22826695442\n",
      "Iteration 49000, loss_train: 1.9935516119, norm: 2.64654374123\n",
      "Val loss: 2.94873213768\n",
      "Iteration 49250, loss_train: 2.0845348835, norm: 1.60294640064\n",
      "Val loss: 3.42038226128\n",
      "Iteration 49500, loss_train: 1.94390499592, norm: 1.70454990864\n",
      "Val loss: 2.91448545456\n",
      "Iteration 49750, loss_train: 1.93013370037, norm: 1.95513248444\n",
      "Val loss: 2.83886408806\n",
      "Iteration 50000, loss_train: 1.77381801605, norm: 1.5434564352\n",
      "Val loss: 3.19366931915\n",
      "Iteration 50250, loss_train: 1.86080527306, norm: 2.34034562111\n",
      "Val loss: 2.870043993\n",
      "Iteration 50500, loss_train: 1.89520382881, norm: 1.78934085369\n",
      "Val loss: 2.97505831718\n",
      "Iteration 50750, loss_train: 1.98648679256, norm: 2.1618013382\n",
      "Val loss: 3.03173804283\n",
      "Iteration 51000, loss_train: 1.82605242729, norm: 2.57187581062\n",
      "Val loss: 3.26953315735\n",
      "Iteration 51250, loss_train: 1.94294285774, norm: 1.55900669098\n",
      "Val loss: 2.95500016212\n",
      "Iteration 51500, loss_train: 1.9459605217, norm: 1.73919188976\n",
      "Val loss: 2.88024973869\n",
      "Iteration 51750, loss_train: 1.99980056286, norm: 2.05872869492\n",
      "Val loss: 3.22115087509\n",
      "Iteration 52000, loss_train: 1.87860405445, norm: 1.71348011494\n",
      "Val loss: 3.38966965675\n",
      "Iteration 52250, loss_train: 1.87752950191, norm: 2.04228138924\n",
      "Val loss: 2.95911812782\n",
      "Iteration 52500, loss_train: 1.81714141369, norm: 1.78160154819\n",
      "Val loss: 3.48606753349\n",
      "Iteration 52750, loss_train: 1.90737044811, norm: 1.66087257862\n",
      "Val loss: 3.29061484337\n",
      "Iteration 53000, loss_train: 1.87264239788, norm: 1.80989003181\n",
      "Val loss: 3.41285943985\n",
      "Iteration 53250, loss_train: 1.77095544338, norm: 1.99647402763\n",
      "Val loss: 3.05030107498\n",
      "Iteration 53500, loss_train: 1.96719264984, norm: 1.70013308525\n",
      "Val loss: 3.04655337334\n",
      "Iteration 53750, loss_train: 1.66361045837, norm: 1.7189463377\n",
      "Val loss: 3.19177913666\n",
      "Iteration 54000, loss_train: 1.86805009842, norm: 1.92271769047\n",
      "Val loss: 3.34984254837\n",
      "Iteration 54250, loss_train: 1.71216273308, norm: 2.46144628525\n",
      "Val loss: 3.79251337051\n",
      "Iteration 54500, loss_train: 1.8234962225, norm: 1.8168592453\n",
      "Val loss: 3.32856988907\n",
      "Iteration 54750, loss_train: 1.83959782124, norm: 2.24933743477\n",
      "Val loss: 3.12061452866\n",
      "Iteration 55000, loss_train: 1.90760374069, norm: 1.8102722168\n",
      "Val loss: 3.27036380768\n",
      "Iteration 55250, loss_train: 1.86549663544, norm: 2.20719408989\n",
      "Val loss: 3.54495692253\n",
      "Iteration 55500, loss_train: 1.7259747982, norm: 1.9939930439\n",
      "Val loss: 3.20811319351\n",
      "Iteration 55750, loss_train: 1.86804461479, norm: 2.25227713585\n",
      "Val loss: 3.67087340355\n",
      "Iteration 56000, loss_train: 1.75720369816, norm: 1.60279548168\n",
      "Val loss: 3.1096098423\n",
      "Iteration 56250, loss_train: 1.80274617672, norm: 2.02207946777\n",
      "Val loss: 2.9596619606\n",
      "Iteration 56500, loss_train: 1.87421536446, norm: 2.1130297184\n",
      "Val loss: 3.39623761177\n",
      "Iteration 56750, loss_train: 1.81616222858, norm: 2.35692667961\n",
      "Val loss: 3.36714839935\n",
      "Iteration 57000, loss_train: 1.80367219448, norm: 2.17102503777\n",
      "Val loss: 3.35934996605\n",
      "Iteration 57250, loss_train: 1.81987369061, norm: 2.57246351242\n",
      "Val loss: 3.8507745266\n",
      "Iteration 57500, loss_train: 1.59481906891, norm: 1.87682855129\n",
      "Val loss: 3.0203473568\n",
      "Iteration 57750, loss_train: 1.78863346577, norm: 1.92542159557\n",
      "Val loss: 3.50583410263\n",
      "Iteration 58000, loss_train: 1.85627830029, norm: 2.32787680626\n",
      "Val loss: 3.33336091042\n",
      "Iteration 58250, loss_train: 1.67749738693, norm: 2.47206258774\n",
      "Val loss: 2.97302508354\n",
      "Iteration 58500, loss_train: 1.8418712616, norm: 1.9061511755\n",
      "Val loss: 3.18245434761\n",
      "Iteration 58750, loss_train: 1.89425051212, norm: 2.45487689972\n",
      "Val loss: 3.04868650436\n",
      "Iteration 59000, loss_train: 1.84368503094, norm: 2.2615442276\n",
      "Val loss: 2.98715543747\n",
      "Iteration 59250, loss_train: 1.72755765915, norm: 1.76612997055\n",
      "Val loss: 3.22475266457\n",
      "Iteration 59500, loss_train: 1.85737645626, norm: 2.02647042274\n",
      "Val loss: 3.00014638901\n",
      "Iteration 59750, loss_train: 1.70086050034, norm: 1.90099751949\n",
      "Val loss: 2.9516556263\n",
      "Iteration 60000, loss_train: 1.84412312508, norm: 2.02032279968\n",
      "Val loss: 2.80247330666\n",
      "Iteration 60250, loss_train: 1.81958937645, norm: 2.15074753761\n",
      "Val loss: 3.42089629173\n",
      "Iteration 60500, loss_train: 1.77796494961, norm: 2.34466576576\n",
      "Val loss: 3.21557092667\n",
      "Iteration 60750, loss_train: 1.71969592571, norm: 1.93280792236\n",
      "Val loss: 3.31062817574\n",
      "Iteration 61000, loss_train: 1.54582798481, norm: 1.73407793045\n",
      "Val loss: 3.32901930809\n",
      "Iteration 61250, loss_train: 1.62263643742, norm: 1.89801740646\n",
      "Val loss: 3.50196075439\n",
      "Iteration 61500, loss_train: 1.81801724434, norm: 2.17261314392\n",
      "Val loss: 2.85390591621\n",
      "Iteration 61750, loss_train: 1.55970072746, norm: 1.94327366352\n",
      "Val loss: 3.291482687\n",
      "Iteration 62000, loss_train: 2.01899790764, norm: 2.42037463188\n",
      "Val loss: 2.99821949005\n",
      "Iteration 62250, loss_train: 1.79753649235, norm: 2.10518240929\n",
      "Val loss: 3.33488702774\n",
      "Iteration 62500, loss_train: 1.76478374004, norm: 2.10992598534\n",
      "Val loss: 3.26437592506\n",
      "Iteration 62750, loss_train: 1.69594717026, norm: 1.91082859039\n",
      "Val loss: 3.43174815178\n",
      "Iteration 63000, loss_train: 1.67578613758, norm: 1.89013576508\n",
      "Val loss: 3.21786904335\n",
      "Iteration 63250, loss_train: 1.56498765945, norm: 1.76983308792\n",
      "Val loss: 3.02072834969\n",
      "Iteration 63500, loss_train: 1.8406355381, norm: 2.19425940514\n",
      "Val loss: 3.3980782032\n",
      "Iteration 63750, loss_train: 1.72614324093, norm: 2.01011586189\n",
      "Val loss: 3.52756094933\n",
      "Iteration 64000, loss_train: 1.5455994606, norm: 2.15314340591\n",
      "Val loss: 3.21683311462\n",
      "Iteration 64250, loss_train: 1.64385211468, norm: 1.66936087608\n",
      "Val loss: 3.1041302681\n",
      "Iteration 64500, loss_train: 1.50918912888, norm: 2.24573254585\n",
      "Val loss: 3.21438336372\n",
      "Iteration 64750, loss_train: 1.58412528038, norm: 1.37482953072\n",
      "Val loss: 3.11197519302\n",
      "Iteration 65000, loss_train: 1.75838816166, norm: 1.99314844608\n",
      "Val loss: 3.43387722969\n",
      "Iteration 65250, loss_train: 1.6317743063, norm: 1.94387221336\n",
      "Val loss: 2.74989175797\n",
      "Iteration 65500, loss_train: 1.76220953465, norm: 1.72800421715\n",
      "Val loss: 3.66557025909\n",
      "Iteration 65750, loss_train: 1.63982641697, norm: 2.43705177307\n",
      "Val loss: 3.21395277977\n",
      "Iteration 66000, loss_train: 1.69978404045, norm: 1.70468425751\n",
      "Val loss: 2.98707699776\n",
      "Iteration 66250, loss_train: 1.78577327728, norm: 1.7314312458\n",
      "Val loss: 3.04137182236\n",
      "Iteration 66500, loss_train: 1.71057665348, norm: 1.89698755741\n",
      "Val loss: 3.10922765732\n",
      "Iteration 66750, loss_train: 1.67621481419, norm: 2.05082416534\n",
      "Val loss: 3.27740836143\n",
      "Iteration 67000, loss_train: 1.53501200676, norm: 1.926872015\n",
      "Val loss: 3.31468892097\n",
      "Iteration 67250, loss_train: 1.67340254784, norm: 2.21813011169\n",
      "Val loss: 2.97907423973\n",
      "Iteration 67500, loss_train: 1.76086878777, norm: 2.28987216949\n",
      "Val loss: 3.1293592453\n",
      "Iteration 67750, loss_train: 1.55902183056, norm: 2.29069280624\n",
      "Val loss: 3.20170354843\n",
      "Iteration 68000, loss_train: 1.7034676075, norm: 1.92488336563\n",
      "Val loss: 3.43078184128\n",
      "Iteration 68250, loss_train: 1.56821656227, norm: 1.72334218025\n",
      "Val loss: 3.24134016037\n",
      "Iteration 68500, loss_train: 1.70434176922, norm: 1.92564213276\n",
      "Val loss: 3.01419496536\n",
      "Iteration 68750, loss_train: 1.79139554501, norm: 1.98147082329\n",
      "Val loss: 3.55515909195\n",
      "Iteration 69000, loss_train: 1.63675045967, norm: 2.04705810547\n",
      "Val loss: 3.26754951477\n",
      "Iteration 69250, loss_train: 1.6036645174, norm: 1.49748492241\n",
      "Val loss: 2.82844400406\n",
      "Iteration 69500, loss_train: 1.59421491623, norm: 2.24450540543\n",
      "Val loss: 3.17307853699\n",
      "Iteration 69750, loss_train: 1.56716179848, norm: 2.12969779968\n",
      "Val loss: 3.53979277611\n",
      "Iteration 70000, loss_train: 1.70625519753, norm: 1.86056184769\n",
      "Val loss: 3.35622119904\n",
      "Iteration 70250, loss_train: 1.38078629971, norm: 2.50107741356\n",
      "Val loss: 4.09555101395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70500, loss_train: 1.70494925976, norm: 2.18061685562\n",
      "Val loss: 3.61847186089\n",
      "Iteration 70750, loss_train: 1.54713523388, norm: 2.78348588943\n",
      "Val loss: 3.77800893784\n",
      "Iteration 71000, loss_train: 1.62787818909, norm: 1.95576369762\n",
      "Val loss: 2.87939214706\n",
      "Iteration 71250, loss_train: 1.42294609547, norm: 1.94410145283\n",
      "Val loss: 3.64659714699\n",
      "Iteration 71500, loss_train: 1.58204543591, norm: 1.97062647343\n",
      "Val loss: 3.25859189034\n",
      "Iteration 71750, loss_train: 1.61160755157, norm: 2.36363840103\n",
      "Val loss: 3.2046983242\n",
      "Iteration 72000, loss_train: 1.63012170792, norm: 2.0988676548\n",
      "Val loss: 3.27167439461\n",
      "Iteration 72250, loss_train: 1.66281986237, norm: 2.47841620445\n",
      "Val loss: 3.08662438393\n",
      "Iteration 72500, loss_train: 1.46624183655, norm: 1.70794594288\n",
      "Val loss: 3.29423093796\n",
      "Iteration 72750, loss_train: 1.58077764511, norm: 1.96272230148\n",
      "Val loss: 3.04057359695\n",
      "Iteration 73000, loss_train: 1.83151066303, norm: 2.10879015923\n",
      "Val loss: 3.26036787033\n",
      "Iteration 73250, loss_train: 1.554428339, norm: 2.46324443817\n",
      "Val loss: 3.5107884407\n",
      "Iteration 73500, loss_train: 1.47752940655, norm: 2.04153037071\n",
      "Val loss: 3.08673930168\n",
      "Iteration 73750, loss_train: 1.57329118252, norm: 1.92533040047\n",
      "Val loss: 3.40520405769\n",
      "Iteration 74000, loss_train: 1.562079072, norm: 2.08314538002\n",
      "Val loss: 3.31431984901\n",
      "Iteration 74250, loss_train: 1.49565660954, norm: 2.4920091629\n",
      "Val loss: 4.01286268234\n",
      "Iteration 74500, loss_train: 1.57861363888, norm: 2.98334670067\n",
      "Val loss: 3.49923563004\n",
      "Iteration 74750, loss_train: 1.53383076191, norm: 2.08015322685\n",
      "Val loss: 3.14396476746\n",
      "Iteration 75000, loss_train: 1.3993139267, norm: 1.92305612564\n",
      "Val loss: 3.17312121391\n",
      "Iteration 75250, loss_train: 1.52653229237, norm: 2.05847811699\n",
      "Val loss: 3.19405913353\n",
      "Iteration 75500, loss_train: 1.56828844547, norm: 2.31437563896\n",
      "Val loss: 3.81883621216\n",
      "Iteration 75750, loss_train: 1.88946175575, norm: 2.46349716187\n",
      "Val loss: 3.56894803047\n",
      "Iteration 76000, loss_train: 1.5521171093, norm: 2.08268976212\n",
      "Val loss: 2.79546833038\n",
      "Iteration 76250, loss_train: 1.42217934132, norm: 1.69955646992\n",
      "Val loss: 3.33042669296\n",
      "Iteration 76500, loss_train: 1.58408093452, norm: 1.88371407986\n",
      "Val loss: 3.19497179985\n",
      "Iteration 76750, loss_train: 1.58701980114, norm: 2.11123347282\n",
      "Val loss: 3.18325686455\n",
      "Iteration 77000, loss_train: 1.6012417078, norm: 1.92009890079\n",
      "Val loss: 3.47388195992\n",
      "Iteration 77250, loss_train: 1.48434340954, norm: 2.10056686401\n",
      "Val loss: 3.05280017853\n",
      "Iteration 77500, loss_train: 1.59498441219, norm: 1.71915817261\n",
      "Val loss: 3.37550067902\n",
      "Iteration 77750, loss_train: 1.3209605217, norm: 1.9179315567\n",
      "Val loss: 3.21112513542\n",
      "Iteration 78000, loss_train: 1.64181554317, norm: 2.43336105347\n",
      "Val loss: 3.06559872627\n",
      "Iteration 78250, loss_train: 1.53812432289, norm: 2.0903468132\n",
      "Val loss: 3.69816398621\n",
      "Iteration 78500, loss_train: 1.48048734665, norm: 2.16574335098\n",
      "Val loss: 3.51571083069\n",
      "Iteration 78750, loss_train: 1.56996536255, norm: 1.85612130165\n",
      "Val loss: 3.16305732727\n",
      "Iteration 79000, loss_train: 1.46231472492, norm: 1.95289683342\n",
      "Val loss: 3.90728020668\n",
      "Iteration 79250, loss_train: 1.4252910614, norm: 1.77313494682\n",
      "Val loss: 3.23657059669\n",
      "Iteration 79500, loss_train: 1.51685106754, norm: 1.94634091854\n",
      "Val loss: 3.05714011192\n",
      "Iteration 79750, loss_train: 1.49582076073, norm: 2.16496610641\n",
      "Val loss: 3.5162987709\n",
      "Iteration 80000, loss_train: 1.37273132801, norm: 2.99906015396\n",
      "Val loss: 4.03646278381\n",
      "Iteration 80250, loss_train: 1.49052727222, norm: 1.80663859844\n",
      "Val loss: 3.52784848213\n",
      "Iteration 80500, loss_train: 1.44352734089, norm: 1.90749847889\n",
      "Val loss: 3.60491681099\n",
      "Iteration 80750, loss_train: 1.50362360477, norm: 2.16709923744\n",
      "Val loss: 3.83591365814\n",
      "Iteration 81000, loss_train: 1.21227586269, norm: 2.09679746628\n",
      "Val loss: 3.30113101006\n",
      "Iteration 81250, loss_train: 1.32940530777, norm: 2.21682286263\n",
      "Val loss: 3.45187830925\n",
      "Iteration 81500, loss_train: 1.30634891987, norm: 1.95217967033\n",
      "Val loss: 3.38174700737\n",
      "Iteration 81750, loss_train: 1.44672882557, norm: 1.84153866768\n",
      "Val loss: 3.65451574326\n",
      "Iteration 82000, loss_train: 1.3581430912, norm: 2.08316445351\n",
      "Val loss: 3.38255906105\n",
      "Iteration 82250, loss_train: 1.38638472557, norm: 1.69341039658\n",
      "Val loss: 2.99352931976\n",
      "Iteration 82500, loss_train: 1.27963209152, norm: 2.35955142975\n",
      "Val loss: 3.39798688889\n",
      "Iteration 82750, loss_train: 1.26283538342, norm: 1.73864614964\n",
      "Val loss: 3.07709646225\n",
      "Iteration 83000, loss_train: 1.40456163883, norm: 2.0178091526\n",
      "Val loss: 3.56335735321\n",
      "Iteration 83250, loss_train: 1.36541640759, norm: 1.45546483994\n",
      "Val loss: 2.88372445107\n",
      "Iteration 83500, loss_train: 1.50018632412, norm: 1.91408979893\n",
      "Val loss: 3.36842632294\n",
      "Iteration 83750, loss_train: 1.41107034683, norm: 2.02417397499\n",
      "Val loss: 3.06486749649\n",
      "Iteration 84000, loss_train: 1.29578244686, norm: 1.85136139393\n",
      "Val loss: 3.7617752552\n",
      "Iteration 84250, loss_train: 1.49338030815, norm: 1.93234312534\n",
      "Val loss: 3.32467269897\n",
      "Iteration 84500, loss_train: 1.4149235487, norm: 2.80236077309\n",
      "Val loss: 2.9775056839\n",
      "Iteration 84750, loss_train: 1.38165557384, norm: 2.00248456001\n",
      "Val loss: 3.41363596916\n",
      "Iteration 85000, loss_train: 1.39182841778, norm: 2.32478642464\n",
      "Val loss: 3.33445835114\n",
      "Iteration 85250, loss_train: 1.33072090149, norm: 2.17130494118\n",
      "Val loss: 3.35425686836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7eb1cf172218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_batch_for_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_data_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration {}, loss_train: {}, norm: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/medic02/users/ag6516/miniconda/envs/TheanoLasagne/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(100000):\n",
    "    x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(get_data_batch(augmented_images, BATCH_SIZE))\n",
    "    loss_train, norm = f_train(x_cnn, x_sentence, mask, y_sentence)\n",
    "    if not iteration % 250:\n",
    "        print('Iteration {}, loss_train: {}, norm: {}'.format(iteration, loss_train, norm))\n",
    "        try:\n",
    "            batch = get_data_batch(augmented_images, BATCH_SIZE, split='val')\n",
    "            x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(batch)\n",
    "            loss_val = f_val(x_cnn, x_sentence, mask, y_sentence)\n",
    "            print('Val loss: {}'.format(loss_val))\n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.22634744644\n"
     ]
    }
   ],
   "source": [
    "batch = get_data_batch(augmented_images, BATCH_SIZE, split='val')\n",
    "x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(batch)\n",
    "loss_val = f_val(x_cnn, x_sentence, mask, y_sentence)\n",
    "print('Val loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_values = lasagne.layers.get_all_param_values(l_out)\n",
    "d = {'param values': param_values,\n",
    "     'vocab': vocab,\n",
    "     'word_to_index': word_to_index,\n",
    "     'index_to_word': index_to_word,\n",
    "    }\n",
    "pickle.dump(d, open('aug_baseline_trained_v_167.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TheanoLasagne]",
   "language": "python",
   "name": "conda-env-TheanoLasagne-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
