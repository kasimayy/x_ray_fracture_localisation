{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/PyTorch27/lib/python2.7/site-packages')\n",
    "# sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/PyTorch27/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg')\n",
    "# sys.path.insert(0,'/vol/medic02/users/ag6516/miniconda/envs/PyTorch27/lib/python2.7/site-packages/IPython/extensions')\n",
    "# sys.path\n",
    "import csv\n",
    "import pandas as pd\n",
    "from read_data import read_csv_into_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = ['Accession', 'Clinical history', 'Comment', 'Report text']\n",
    "# dir = ('/vol/medic02/users/ag6516/x_ray_fracture_localisation/')\n",
    "dir = ('/Users/Aydan/PhD/x_ray_fracture_localisation/')\n",
    "\n",
    "with open(dir + 'data/Reports/XKNEB_Jan_Dec_2015_anon.csv', 'rU') as csvfile:\n",
    "    data1 = csv.reader(csvfile, delimiter=',')\n",
    "    headers1 = data1.next()\n",
    "\n",
    "with open(dir + 'data/Reports/XKNEB_Jan_Dec_2016_anon.csv', 'rU') as csvfile:\n",
    "    data2 = csv.reader(csvfile, delimiter=',')\n",
    "    headers2 = data2.next()\n",
    "\n",
    "# with open(dir + 'data/Reports/XKNEB_Jan_Jul_2017_anon.csv', 'rU') as csvfile:\n",
    "#     data3 = csv.reader(csvfile, delimiter=',')\n",
    "#     headers3 = data3.next()\n",
    "\n",
    "cols1 = [headers1.index(item) for item in fields]\n",
    "cols2 = [headers2.index(item) for item in fields]\n",
    "# cols3 = [headers3.index(item) for item in fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = read_csv_into_df(dir + 'data/Reports/XKNEB_Jan_Dec_2015_anon.csv', fields, cols1)\n",
    "df2 = read_csv_into_df(dir + 'data/Reports/XKNEB_Jan_Dec_2016_anon.csv', fields, cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accession           3576\n",
      "Clinical history    2232\n",
      "Comment             2402\n",
      "Report text         3576\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df1,df2])\n",
    "print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for missing information\n",
    "df_nans = df[df.isnull().any(axis=1)]\n",
    "df_nans.to_csv('missing_info.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_reports.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  2158\n",
      "Avg no. of appearances:  41\n",
      "STD:  349.019096469\n"
     ]
    }
   ],
   "source": [
    "# Basic filtering and stats\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "reports = []\n",
    "labels = []\n",
    "words = []\n",
    "for i, row in df.iterrows():\n",
    "    report = row['Report text'].decode('utf-8').lower()\n",
    "    \n",
    "    # Replace unnecessary punctuation\n",
    "    \n",
    "    for char in ['~', '\"']:\n",
    "        report = report.replace(char, '')\n",
    "        \n",
    "    for char in ['!', '?', ';', ':', '.']:\n",
    "        report = report.replace(char, ' . ')\n",
    "    \n",
    "    for char in ['(', ')', ',']:\n",
    "        report = report.replace(char, ' , ')\n",
    "        \n",
    "    # Tokenize\n",
    "    report_tok = word_tokenize(report)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_report = [word for word in report_tok if word not in stopwords.words('english')]\n",
    "    \n",
    "    reports.append(filtered_report)\n",
    "    labels.append(row['Accession'])\n",
    "    [words.append(word) for word in filtered_report]\n",
    "\n",
    "print 'Vocab length: ', len(sorted(set(words)))\n",
    "print 'Avg no. of appearances: ', len(words)/len(sorted(set(words)))\n",
    "fdist_all = nltk.FreqDist(words)\n",
    "freqs = [freq for _, freq in fdist_all.most_common()]\n",
    "print 'STD: ', np.std(freqs)\n",
    "#print fdist_all.hapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train doc2vec model\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "class DocIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield LabeledSentence(words=doc, tags=[self.labels_list[idx]])\n",
    "            \n",
    "it = DocIterator(reports, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = gensim.models.Doc2Vec(size=300, min_count=0, alpha=0.025, min_alpha=0.025)\n",
    "model.build_vocab(it)\n",
    "#training of model\n",
    "for epoch in range(10):\n",
    "    print 'iteration ' +str(epoch+1)\n",
    "    model.train(it, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "    model.train(it, total_examples=model.corpus_count, epochs=model.iter)\n",
    "#saving the created model\n",
    "model.save('doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349    XR Knee Both : \\n\\nLeft knee: There is severe ...\n",
      "Most similar documents:\n",
      "2538    XR Knee Both : Both knees have severe patellof...\n",
      "Name: Report text, dtype: object\n",
      "3176    Left knee: TKR in situ, good alignment. No adv...\n",
      "Name: Report text, dtype: object\n",
      "2691    XR Knee Both : In the right knee there is seve...\n",
      "Name: Report text, dtype: object\n",
      "1208    XR Knee Both : \\nBilateral OA changes in both ...\n",
      "Name: Report text, dtype: object\n",
      "208    XR Knee Both : minor OA changes both patello f...\n",
      "Name: Report text, dtype: object\n",
      "3149    Left knee: There is severe OA change affecting...\n",
      "Name: Report text, dtype: object\n",
      "76    XR Knee Both : Early OA changes Patello femora...\n",
      "Name: Report text, dtype: object\n",
      "3121    XR Knee Both : There is moderate OA change to ...\n",
      "Name: Report text, dtype: object\n",
      "2365    XR Knee Both : No significant bone or joint ab...\n",
      "Name: Report text, dtype: object\n",
      "2853    XR Knee Both : \\n\\nLeft knee: There is moderat...\n",
      "Name: Report text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "label_sample = random.choice(labels)\n",
    "report_sample = df.loc[df['Accession'] == label_sample]['Report text']\n",
    "print report_sample.to_string()\n",
    "\n",
    "most_similar = model.docvecs.most_similar(label_sample)\n",
    "print \"Most similar documents:\"\n",
    "\n",
    "for label, _ in most_similar:\n",
    "    print df.loc[df['Accession'] == label]['Report text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
